{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60e23471",
   "metadata": {},
   "source": [
    "### HyDE (Hypothetical Document Embeddings)\n",
    "\n",
    "#### üß† ¬øQu√© es HyDE?\n",
    "\n",
    "**HyDE (Hypothetical Document Embeddings)** es una t√©cnica de recuperaci√≥n donde, en lugar de hacer embedding de la consulta del usuario directamente, primero generas una **respuesta hipot√©tica** (documento) a la consulta usando un LLM ‚Äî y luego haces embedding de ese documento hipot√©tico para buscar en tu vector store.\n",
    "\n",
    "#### üéØ ¬øC√≥mo funciona HyDE?\n",
    "\n",
    "**Flujo tradicional (sin HyDE):**\n",
    "```\n",
    "Consulta Usuario ‚Üí Embedding ‚Üí B√∫squeda Vector Store ‚Üí Documentos Relevantes\n",
    "```\n",
    "\n",
    "**Flujo con HyDE:**\n",
    "```\n",
    "Consulta Usuario ‚Üí LLM genera respuesta hipot√©tica ‚Üí Embedding de la respuesta ‚Üí B√∫squeda Vector Store ‚Üí Documentos Relevantes\n",
    "```\n",
    "\n",
    "#### ‚úÖ ¬øCu√°ndo usar HyDE?\n",
    "\n",
    "HyDE cierra la brecha entre la intenci√≥n del usuario y el contenido relevante, especialmente cuando:\n",
    "\n",
    "1. **Las consultas son cortas** - \"Steve Jobs despedido\"\n",
    "2. **Hay desajuste de lenguaje** entre consulta y documentos (pregunta vs respuesta)\n",
    "3. **Quieres recuperar basado en contenido de respuesta**, no en palabras de pregunta\n",
    "4. **Vocabulario diferente** - Usuario pregunta con t√©rminos simples, documentos usan terminolog√≠a t√©cnica\n",
    "\n",
    "#### üîç Ejemplo:\n",
    "\n",
    "**Consulta:** \"¬øCu√°ndo despidieron a Steve Jobs de Apple?\"\n",
    "\n",
    "**Sin HyDE:** Se busca directamente el embedding de la pregunta\n",
    "- Puede no encontrar documentos que contengan la respuesta pero no la pregunta exacta\n",
    "\n",
    "**Con HyDE:** \n",
    "1. LLM genera respuesta hipot√©tica: \"Steve Jobs fue despedido de Apple en septiembre de 1985 debido a conflictos con la junta directiva...\"\n",
    "2. Se hace embedding de esta respuesta hipot√©tica\n",
    "3. Se buscan documentos similares a esta respuesta\n",
    "4. Mayor probabilidad de encontrar documentos relevantes que contengan informaci√≥n similar\n",
    "\n",
    "#### üí° Ventajas:\n",
    "- ‚úÖ Mejora recuperaci√≥n cuando hay desajuste sem√°ntico\n",
    "- ‚úÖ Funciona bien con consultas cortas\n",
    "- ‚úÖ Captura el formato y estilo de las respuestas esperadas\n",
    "- ‚úÖ Reduce el problema de \"vocabulary mismatch\"\n",
    "\n",
    "#### ‚ö†Ô∏è Consideraciones:\n",
    "- Requiere llamada adicional al LLM (m√°s latencia y costo)\n",
    "- La calidad depende de la respuesta hipot√©tica generada\n",
    "- Puede no ser necesario si los documentos y consultas ya est√°n bien alineados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38efa93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaci√≥n de librer√≠as necesarias para implementar HyDE\n",
    "\n",
    "# WikipediaLoader: Para cargar art√≠culos de Wikipedia autom√°ticamente\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "# RecursiveCharacterTextSplitter: Para dividir texto en fragmentos (chunks) manejables\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# HuggingFaceEmbeddings: Para generar embeddings vectoriales usando modelos de HuggingFace\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Chroma: Base de datos vectorial para almacenar y buscar embeddings\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9795806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Cargar y fragmentar el dataset desde Wikipedia\n",
    "\n",
    "# Configurar par√°metros de fragmentaci√≥n\n",
    "# chunk_size=300: Cada fragmento tendr√° m√°ximo 300 caracteres\n",
    "chunk_size = 300\n",
    "\n",
    "# chunk_overlap=100: Habr√° una superposici√≥n de 100 caracteres entre fragmentos\n",
    "# Una superposici√≥n mayor (100 vs 50) ayuda a mantener m√°s contexto\n",
    "chunk_overlap = 100\n",
    "\n",
    "# Cargar datos de Wikipedia sobre Steve Jobs\n",
    "# query=\"Steve Jobs\": Busca art√≠culos relacionados con Steve Jobs\n",
    "# load_max_docs=5: Carga m√°ximo 5 documentos de Wikipedia\n",
    "loader = WikipediaLoader(query=\"Steve Jobs\", load_max_docs=5)\n",
    "\n",
    "# load() descarga y devuelve los art√≠culos de Wikipedia\n",
    "documents = loader.load()\n",
    "\n",
    "# Crear un splitter para dividir el texto en fragmentos\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, \n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "# Dividir los documentos de Wikipedia en fragmentos m√°s peque√±os\n",
    "docs = text_splitter.split_documents(documents=documents)\n",
    "\n",
    "# Mostrar los fragmentos generados\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea67d371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 2: Construir el vector store con embeddings est√°ndar\n",
    "\n",
    "# Importar FAISS para b√∫squeda de similitud eficiente\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Inicializar el modelo de embeddings de HuggingFace\n",
    "# all-MiniLM-L6-v2 es un modelo compacto y eficiente (384 dimensiones)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Crear el vector store FAISS a partir de los documentos fragmentados\n",
    "# FAISS convierte cada fragmento en un vector para b√∫squeda r√°pida\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784942fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3: Configurar el LLM que generar√° las respuestas hipot√©ticas\n",
    "\n",
    "# Importar m√≥dulos para manejo de variables de entorno\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Cargar las variables de entorno desde el archivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Importar funci√≥n para inicializar modelos de chat\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Establecer la API key de Groq desde las variables de entorno\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Inicializar el LLM usando Gemma2-9B-IT de Groq\n",
    "# Este modelo generar√° las respuestas hipot√©ticas para HyDE\n",
    "llm = init_chat_model(\"groq:gemma2-9b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2397d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 4: Crear vector store persistente con ChromaDB\n",
    "\n",
    "# Importar Chroma para almacenamiento vectorial persistente\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Crear vector store ChromaDB a partir de los documentos\n",
    "# documents=docs: Fragmentos de Wikipedia sobre Steve Jobs\n",
    "# embedding=embeddings: Modelo de embeddings HuggingFace\n",
    "# persist_directory: Directorio donde se guardar√°n los embeddings de forma persistente\n",
    "#                    Esto permite reutilizar los embeddings sin recalcularlos\n",
    "db = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"output/steve_jobs_for_hyde.db\"\n",
    ")\n",
    "\n",
    "# Crear el recuperador base (sin HyDE a√∫n)\n",
    "# search_kwargs={\"k\": 5}: Recuperar los 5 documentos m√°s similares\n",
    "base_retriever = db.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07abc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 5: Implementar funci√≥n para generar documentos hipot√©ticos (HyDE manual)\n",
    "\n",
    "# Importar parser para convertir salida del LLM a string\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Importar templates para crear prompts estructurados\n",
    "from langchain.prompts.chat import SystemMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "def get_hyde_doc(query):\n",
    "    \"\"\"\n",
    "    Genera un documento hipot√©tico (respuesta imaginaria) para una consulta.\n",
    "    \n",
    "    Este es el n√∫cleo de la t√©cnica HyDE:\n",
    "    1. Toma la consulta del usuario\n",
    "    2. Pide al LLM que imagine una respuesta detallada\n",
    "    3. Devuelve esa respuesta hipot√©tica\n",
    "    \n",
    "    Args:\n",
    "        query (str): La consulta del usuario\n",
    "    \n",
    "    Returns:\n",
    "        str: Documento hipot√©tico generado por el LLM\n",
    "    \"\"\"\n",
    "    \n",
    "    # Plantilla que instruye al LLM a generar una respuesta hipot√©tica\n",
    "    # El LLM debe imaginar que es un experto escribiendo una explicaci√≥n detallada\n",
    "    template = \"\"\"Imagina que eres un experto escribiendo una explicaci√≥n detallada sobre el tema: '{query}'\n",
    "    Crea una respuesta hipot√©tica para el tema\"\"\"\n",
    "\n",
    "    # Crear mensaje del sistema con la plantilla\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(template=template)\n",
    "    \n",
    "    # Crear prompt de chat completo\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt])\n",
    "    \n",
    "    # Formatear el prompt con la consulta espec√≠fica\n",
    "    messages = chat_prompt.format_prompt(query=query).to_messages()\n",
    "    \n",
    "    # Mostrar los mensajes que se enviar√°n al LLM (para debug)\n",
    "    print(messages)\n",
    "    \n",
    "    # Invocar el LLM para generar la respuesta hipot√©tica\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Extraer el contenido de texto de la respuesta\n",
    "    hypo_doc = response.content\n",
    "    \n",
    "    # Devolver el documento hipot√©tico generado\n",
    "    return hypo_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979a89e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 6: Probar la generaci√≥n de documento hipot√©tico\n",
    "\n",
    "# Definir una consulta sobre cu√°ndo despidieron a Steve Jobs\n",
    "query = '¬øCu√°ndo despidieron a Steve Jobs de Apple?'\n",
    "\n",
    "# Generar y mostrar el documento hipot√©tico\n",
    "# Observa c√≥mo el LLM crea una respuesta imaginaria detallada\n",
    "# Esta respuesta hipot√©tica se usar√° para buscar documentos similares\n",
    "print(\"üìÑ Documento Hipot√©tico Generado:\")\n",
    "print(\"=\"*80)\n",
    "print(get_hyde_doc(query=query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fcc5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 7: Usar HyDE para recuperar documentos relevantes\n",
    "\n",
    "# Flujo completo de HyDE:\n",
    "# 1. get_hyde_doc(query) genera una respuesta hipot√©tica\n",
    "# 2. base_retriever.invoke() busca documentos similares a esa respuesta hipot√©tica\n",
    "# 3. Los documentos recuperados son probablemente m√°s relevantes que si busc√°ramos la consulta directamente\n",
    "\n",
    "matched_doc = base_retriever.invoke(get_hyde_doc(query))\n",
    "\n",
    "# Mostrar los documentos recuperados usando HyDE\n",
    "print(\"üîç Documentos Recuperados con HyDE:\")\n",
    "print(\"=\"*80)\n",
    "print(matched_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fab6e72",
   "metadata": {},
   "source": [
    "### üîß Implementaci√≥n con LangChain: HypotheticalDocumentEmbedder\n",
    "\n",
    "LangChain proporciona una clase integrada llamada `HypotheticalDocumentEmbedder` que implementa HyDE de forma m√°s elegante y autom√°tica.\n",
    "\n",
    "**Ventajas del HypotheticalDocumentEmbedder:**\n",
    "- ‚úÖ Integraci√≥n directa en el pipeline de embeddings\n",
    "- ‚úÖ Prompts pre-configurados para diferentes tipos de b√∫squeda\n",
    "- ‚úÖ M√°s f√°cil de usar y mantener\n",
    "- ‚úÖ Autom√°ticamente genera documento hipot√©tico antes de hacer embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a417f3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 8: Implementar HyDE usando la clase de LangChain\n",
    "\n",
    "# Importar el HypotheticalDocumentEmbedder de LangChain\n",
    "from langchain.chains.hyde.base import HypotheticalDocumentEmbedder\n",
    "\n",
    "# Importar librer√≠as necesarias\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# Cargar el dataset sobre LangChain y CrewAI\n",
    "loader = TextLoader(\"langchain_crewai_dataset.txt\")\n",
    "\n",
    "# load() devuelve una lista de documentos\n",
    "docs = loader.load()\n",
    "\n",
    "# Crear splitter para dividir en fragmentos\n",
    "# chunk_size=300: Fragmentos de m√°ximo 300 caracteres\n",
    "# chunk_overlap=50: Superposici√≥n de 50 caracteres entre fragmentos\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "\n",
    "# Dividir los documentos en chunks\n",
    "chunks = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d58f9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 9: Configurar embeddings base\n",
    "\n",
    "# Inicializar embeddings HuggingFace que se usar√°n como base\n",
    "# Estos embeddings se aplicar√°n al documento hipot√©tico generado por HyDE\n",
    "base_embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326cf690",
   "metadata": {},
   "source": [
    "### üìã Tipos de Prompts Disponibles en HyDE\n",
    "\n",
    "Seg√∫n la documentaci√≥n oficial de LangChain y el c√≥digo fuente (PROMPT_MAP), las opciones de `prompt_key` predeterminadas son:\n",
    "\n",
    "1. **`web_search`** - B√∫squeda web general (recomendado para la mayor√≠a de casos)\n",
    "2. **`sci_fact`** - Hechos cient√≠ficos\n",
    "3. **`arguana`** - Argumentaci√≥n y debate\n",
    "4. **`trec_covid`** - Informaci√≥n m√©dica/COVID\n",
    "5. **`fiqa`** - Finanzas y preguntas financieras\n",
    "6. **`dbpedia_entity`** - Entidades de DBpedia\n",
    "7. **`trec_news`** - Noticias y art√≠culos period√≠sticos\n",
    "8. **`mr_tydi`** - B√∫squeda multiling√ºe\n",
    "\n",
    "**¬øC√≥mo elegir el prompt_key correcto?**\n",
    "- **web_search**: Uso general, preguntas variadas (DEFAULT)\n",
    "- **sci_fact**: Preguntas cient√≠ficas o t√©cnicas\n",
    "- **fiqa**: Preguntas sobre finanzas, inversiones, econom√≠a\n",
    "- **trec_news**: B√∫squeda en noticias o art√≠culos\n",
    "\n",
    "Cada `prompt_key` tiene un prompt optimizado para generar documentos hipot√©ticos en ese dominio espec√≠fico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee63ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 10: Crear HyDE Embedder usando prompt_key predeterminado\n",
    "\n",
    "# HypotheticalDocumentEmbedder.from_llm() crea un embedder que:\n",
    "# 1. Toma una consulta\n",
    "# 2. Usa el LLM para generar un documento hipot√©tico\n",
    "# 3. Genera embeddings del documento hipot√©tico (no de la consulta original)\n",
    "\n",
    "hyde_embedding_function = HypotheticalDocumentEmbedder.from_llm(\n",
    "    llm=llm,  # LLM que generar√° los documentos hipot√©ticos\n",
    "    base_embeddings=base_embeddings,  # Embeddings que se aplicar√°n al documento hipot√©tico\n",
    "    prompt_key=\"web_search\"  # Tipo de prompt optimizado para b√∫squeda web general\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c402d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 11: Crear vector store con HyDE embeddings\n",
    "\n",
    "# Crear ChromaDB usando el HypotheticalDocumentEmbedder\n",
    "# IMPORTANTE: Ahora los embeddings se generan usando HyDE:\n",
    "# - Durante la indexaci√≥n: Se almacenan embeddings normales de los documentos\n",
    "# - Durante la b√∫squeda: La consulta se convierte en documento hipot√©tico antes de hacer embedding\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,  # Fragmentos del dataset\n",
    "    embedding=hyde_embedding_function,  # Funci√≥n de embedding con HyDE integrado\n",
    "    persist_directory=\"output/langchain\"  # Directorio de persistencia\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28c284e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 12: Crear prompt para generaci√≥n de respuestas RAG\n",
    "\n",
    "# Esta plantilla define c√≥mo el LLM usar√° el contexto recuperado para responder\n",
    "rag_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Usa el contexto a continuaci√≥n para responder la pregunta.\n",
    "\n",
    "Contexto:\n",
    "{context}\n",
    "\n",
    "Pregunta: {input}\n",
    "\"\"\")\n",
    "\n",
    "# Crear la cadena de documentos que combina LLM con el prompt\n",
    "# Esta cadena toma documentos recuperados y genera una respuesta\n",
    "rag_chain = create_stuff_documents_chain(llm=llm, prompt=rag_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0af24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 13: Construir el pipeline RAG completo con HyDE\n",
    "\n",
    "def hyde_rag_pipeline(query):\n",
    "    \"\"\"\n",
    "    Pipeline RAG completo que usa HyDE para mejorar la recuperaci√≥n.\n",
    "    \n",
    "    Flujo del pipeline:\n",
    "    1. La consulta se convierte autom√°ticamente en documento hipot√©tico (por HyDE)\n",
    "    2. Se hace embedding del documento hipot√©tico\n",
    "    3. Se buscan documentos similares al documento hipot√©tico\n",
    "    4. Se genera respuesta usando los documentos recuperados\n",
    "    \n",
    "    Args:\n",
    "        query (str): La consulta del usuario\n",
    "    \n",
    "    Returns:\n",
    "        str: Respuesta generada por el LLM basada en documentos recuperados con HyDE\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Buscar documentos similares usando HyDE\n",
    "    # similarity_search() autom√°ticamente:\n",
    "    #    a. Genera documento hipot√©tico de la consulta (usando el LLM)\n",
    "    #    b. Hace embedding del documento hipot√©tico\n",
    "    #    c. Busca los k documentos m√°s similares\n",
    "    matched_docs = vectorstore.similarity_search(query, k=4)\n",
    "    \n",
    "    # Mostrar los documentos recuperados (para debug)\n",
    "    print(\"üîç Documentos recuperados con HyDE:\")\n",
    "    print(matched_docs)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # 2. Generar respuesta usando los documentos recuperados como contexto\n",
    "    response = rag_chain.invoke({\n",
    "        \"input\": query,  # Consulta original del usuario\n",
    "        \"context\": matched_docs  # Documentos recuperados con HyDE\n",
    "    })\n",
    "    \n",
    "    # 3. Devolver la respuesta generada\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be5b493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 14: Ejecutar consulta de ejemplo con HyDE\n",
    "\n",
    "# Definir una consulta sobre m√≥dulos de memoria en LangChain\n",
    "query = \"¬øQu√© m√≥dulos de memoria proporciona LangChain?\"\n",
    "\n",
    "# Invocar el pipeline RAG con HyDE\n",
    "# HyDE autom√°ticamente:\n",
    "# 1. Generar√° una respuesta hipot√©tica a esta pregunta\n",
    "# 2. Buscar√° documentos similares a esa respuesta hipot√©tica\n",
    "# 3. Usar√° esos documentos para generar la respuesta final\n",
    "answer = hyde_rag_pipeline(query)\n",
    "\n",
    "# Mostrar la respuesta final\n",
    "print(\"‚úÖ Respuesta Final:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c0623f",
   "metadata": {},
   "source": [
    "### üé® Prompt Personalizado para HyDE\n",
    "\n",
    "Adem√°s de usar los `prompt_key` predeterminados, puedes crear tu propio prompt personalizado usando el par√°metro `custom_prompt`.\n",
    "\n",
    "**¬øCu√°ndo usar un prompt personalizado?**\n",
    "- Cuando ninguno de los prompt_key predeterminados se ajusta a tu dominio\n",
    "- Cuando quieres control total sobre c√≥mo se genera el documento hipot√©tico\n",
    "- Para experimentar con diferentes estilos de respuestas hipot√©ticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8182e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 15: Crear HyDE Embedder con prompt personalizado\n",
    "\n",
    "# Importar PromptTemplate para crear plantillas personalizadas\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Definir un prompt personalizado para generar documentos hipot√©ticos\n",
    "# Este prompt es m√°s conciso que los predeterminados\n",
    "custom = PromptTemplate.from_template(\n",
    "    \"Genera una respuesta hipot√©tica concisa para este tema: {query}\"\n",
    ")\n",
    "\n",
    "# Crear HyDE Embedder usando el prompt personalizado\n",
    "hyde_embedding_function = HypotheticalDocumentEmbedder.from_llm(\n",
    "    llm=llm,  # LLM para generar documentos hipot√©ticos\n",
    "    base_embeddings=base_embeddings,  # Embeddings base\n",
    "    custom_prompt=custom  # Usar prompt personalizado en lugar de prompt_key\n",
    ")\n",
    "\n",
    "# NOTA: Puedes usar este hyde_embedding_function de la misma forma que antes\n",
    "# La √∫nica diferencia es que usar√° tu prompt personalizado para generar\n",
    "# documentos hipot√©ticos m√°s concisos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "### üìä Resumen: HyDE (Hypothetical Document Embeddings)\n",
    "\n",
    "#### üîÑ Comparaci√≥n: B√∫squeda Tradicional vs HyDE\n",
    "\n",
    "**B√∫squeda Tradicional:**\n",
    "```\n",
    "Consulta: \"¬øCu√°ndo despidieron a Steve Jobs?\"\n",
    "    ‚Üì (embedding directo)\n",
    "Vector de la pregunta\n",
    "    ‚Üì (b√∫squeda)\n",
    "Documentos que contienen preguntas similares o palabras clave\n",
    "```\n",
    "\n",
    "**B√∫squeda con HyDE:**\n",
    "```\n",
    "Consulta: \"¬øCu√°ndo despidieron a Steve Jobs?\"\n",
    "    ‚Üì (LLM genera respuesta hipot√©tica)\n",
    "\"Steve Jobs fue despedido de Apple en septiembre de 1985...\"\n",
    "    ‚Üì (embedding de la respuesta)\n",
    "Vector de la respuesta hipot√©tica\n",
    "    ‚Üì (b√∫squeda)\n",
    "Documentos que contienen respuestas similares\n",
    "```\n",
    "\n",
    "#### ‚úÖ Ventajas de HyDE\n",
    "\n",
    "1. **Resuelve el problema de vocabulary mismatch**\n",
    "   - Usuario: \"¬øCu√°ndo echaron a Steve Jobs?\"\n",
    "   - Documento: \"Jobs fue despedido en 1985\"\n",
    "   - HyDE genera: \"despedido en 1985\" ‚Üí mejor match\n",
    "\n",
    "2. **Mejora recuperaci√≥n con consultas cortas**\n",
    "   - Consulta corta: \"memoria LangChain\"\n",
    "   - HyDE expande: \"LangChain ofrece ConversationBufferMemory y ConversationSummaryMemory...\"\n",
    "   - M√°s contexto = mejor recuperaci√≥n\n",
    "\n",
    "3. **Alinea formato pregunta-respuesta**\n",
    "   - Busca respuestas similares a una respuesta hipot√©tica\n",
    "   - M√°s natural que buscar documentos similares a una pregunta\n",
    "\n",
    "#### ‚ö†Ô∏è Desventajas de HyDE\n",
    "\n",
    "1. **Latencia aumentada**\n",
    "   - Requiere llamada adicional al LLM antes de la b√∫squeda\n",
    "   - ~500ms-2s de overhead dependiendo del LLM\n",
    "\n",
    "2. **Costo adicional**\n",
    "   - Cada b√∫squeda consume tokens del LLM\n",
    "   - Para APIs de pago (OpenAI, etc.), aumenta el costo\n",
    "\n",
    "3. **Dependencia de la calidad del LLM**\n",
    "   - Si el documento hipot√©tico es incorrecto, la b√∫squeda ser√° peor\n",
    "   - Requiere un LLM razonablemente bueno\n",
    "\n",
    "4. **No siempre necesario**\n",
    "   - Si documentos y consultas ya est√°n alineados, HyDE puede no ayudar\n",
    "   - Puede ser overkill para bases de conocimiento bien estructuradas\n",
    "\n",
    "#### üéØ Casos de Uso Ideales\n",
    "\n",
    "**‚úÖ Cu√°ndo USAR HyDE:**\n",
    "- Consultas en lenguaje natural vs documentos t√©cnicos\n",
    "- Preguntas cortas que necesitan expansi√≥n sem√°ntica\n",
    "- Bases de conocimiento con vocabulario diferente al usuario\n",
    "- FAQs donde buscas respuestas, no preguntas\n",
    "- Dominios especializados (medicina, leyes, finanzas)\n",
    "\n",
    "**‚ùå Cu√°ndo NO usar HyDE:**\n",
    "- B√∫squeda de palabras clave exactas\n",
    "- Aplicaciones de latencia cr√≠tica\n",
    "- Presupuesto muy limitado\n",
    "- Documentos y consultas ya bien alineados\n",
    "- B√∫squeda de c√≥digo (donde sintaxis exacta importa)\n",
    "\n",
    "#### üîß Opciones de Implementaci√≥n\n",
    "\n",
    "**1. Manual (m√°s control):**\n",
    "```python\n",
    "hyde_doc = llm.invoke(\"Genera respuesta para: {query}\")\n",
    "results = vectorstore.similarity_search(hyde_doc)\n",
    "```\n",
    "\n",
    "**2. LangChain HypotheticalDocumentEmbedder (recomendado):**\n",
    "```python\n",
    "hyde_embedder = HypotheticalDocumentEmbedder.from_llm(\n",
    "    llm=llm,\n",
    "    base_embeddings=embeddings,\n",
    "    prompt_key=\"web_search\"  # o custom_prompt=...\n",
    ")\n",
    "```\n",
    "\n",
    "#### üìà Comparaci√≥n con Otras T√©cnicas\n",
    "\n",
    "| T√©cnica | Latencia | Costo | Precisi√≥n | Uso Ideal |\n",
    "|---------|----------|-------|-----------|----------|\n",
    "| **B√∫squeda Simple** | Baja | Bajo | ‚≠ê‚≠ê‚≠ê | Documentos y consultas alineados |\n",
    "| **Query Expansion** | Media | Medio | ‚≠ê‚≠ê‚≠ê‚≠ê | Agregar sin√≥nimos |\n",
    "| **HyDE** | Alta | Alto | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Vocabulary mismatch |\n",
    "| **Query Decomposition** | Alta | Alto | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Consultas complejas |\n",
    "\n",
    "#### üí° Consejos Pr√°cticos\n",
    "\n",
    "1. **Experimenta con diferentes prompt_keys** - Cada dominio puede funcionar mejor con diferentes prompts\n",
    "2. **Combina con otras t√©cnicas** - HyDE + Reranking puede dar excelentes resultados\n",
    "3. **Cachea documentos hipot√©ticos** - Si las consultas se repiten, guarda el documento hipot√©tico\n",
    "4. **Monitorea la calidad** - Revisa qu√© documentos hipot√©ticos se generan\n",
    "5. **A/B testing** - Compara resultados con y sin HyDE en tu caso de uso espec√≠fico"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
