{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8e7d68c",
   "metadata": {},
   "source": [
    "### üß† ¬øQu√© es la Descomposici√≥n de Consultas?\n",
    "\n",
    "La **descomposici√≥n de consultas** es el proceso de tomar una pregunta compleja de m√∫ltiples partes y dividirla en sub-preguntas m√°s simples y at√≥micas que pueden ser recuperadas y respondidas individualmente.\n",
    "\n",
    "#### ‚úÖ ¬øPor qu√© usar Descomposici√≥n de Consultas?\n",
    "\n",
    "**Problemas que resuelve:**\n",
    "- Las consultas complejas a menudo involucran m√∫ltiples conceptos\n",
    "- Los LLMs o recuperadores pueden perder partes de la pregunta original\n",
    "- Permite razonamiento multi-hop (responder en pasos)\n",
    "- Posibilita paralelismo (especialmente en frameworks multi-agente)\n",
    "\n",
    "**Ejemplo:**\n",
    "- **Consulta compleja:** \"¬øC√≥mo usa LangChain la memoria y los agentes comparado con CrewAI?\"\n",
    "- **Descomposici√≥n:**\n",
    "  1. ¬øQu√© mecanismos de memoria ofrece LangChain?\n",
    "  2. ¬øC√≥mo funcionan los agentes en LangChain?\n",
    "  3. ¬øQu√© mecanismos de memoria ofrece CrewAI?\n",
    "  4. ¬øC√≥mo funcionan los agentes en CrewAI?\n",
    "  5. ¬øCu√°les son las diferencias clave entre ambos?\n",
    "\n",
    "**Ventajas:**\n",
    "- ‚úÖ Mejora la precisi√≥n en respuestas complejas\n",
    "- ‚úÖ Permite recuperaci√≥n m√°s enfocada para cada aspecto\n",
    "- ‚úÖ Facilita el razonamiento paso a paso\n",
    "- ‚úÖ Reduce sobrecarga de contexto en cada consulta\n",
    "- ‚úÖ Permite procesamiento paralelo de sub-preguntas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b23a442b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Udemy\\RAGBootcamp\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importaci√≥n de librer√≠as necesarias para el pipeline RAG con descomposici√≥n de consultas\n",
    "\n",
    "# init_chat_model: Para inicializar modelos de chat de diferentes proveedores (OpenAI, Groq, etc.)\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# PromptTemplate: Para crear plantillas de prompts con variables din√°micas\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# TextLoader: Para cargar archivos de texto plano\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# RecursiveCharacterTextSplitter: Para dividir texto en fragmentos (chunks) manejables\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# HuggingFaceEmbeddings: Para generar embeddings vectoriales usando modelos de HuggingFace\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# FAISS: Biblioteca de Facebook para b√∫squeda de similitud vectorial eficiente\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# StrOutputParser: Para convertir la salida del LLM a string\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# create_stuff_documents_chain: Crea una cadena que inserta documentos en un prompt\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# RunnableSequence: Permite encadenar operaciones secuencialmente\n",
    "from langchain_core.runnables import RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76de0145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Cargar y crear el almac√©n vectorial del documento\n",
    "\n",
    "# Cargar el archivo de texto que contiene informaci√≥n sobre LangChain y CrewAI\n",
    "loader = TextLoader(\"langchain_crewai_dataset.txt\")\n",
    "\n",
    "# load() devuelve una lista de documentos con todo el contenido del archivo\n",
    "docs = loader.load()\n",
    "\n",
    "# Crear un splitter para dividir el texto en fragmentos m√°s peque√±os\n",
    "# chunk_size=300: Cada fragmento tendr√° m√°ximo 300 caracteres\n",
    "# chunk_overlap=50: Habr√° una superposici√≥n de 50 caracteres entre fragmentos consecutivos\n",
    "# La superposici√≥n ayuda a mantener el contexto entre fragmentos adyacentes\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "\n",
    "# Dividir los documentos en fragmentos (chunks) manejables\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# Inicializar el modelo de embeddings de HuggingFace\n",
    "# all-MiniLM-L6-v2 es un modelo compacto y eficiente que genera vectores de 384 dimensiones\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Crear el almac√©n vectorial FAISS a partir de los fragmentos de documentos\n",
    "# FAISS convierte cada fragmento en un vector num√©rico para b√∫squeda de similitud eficiente\n",
    "vectorstore = FAISS.from_documents(chunks, embedding)\n",
    "\n",
    "# Crear el recuperador con MMR (Maximal Marginal Relevance)\n",
    "# search_type=\"mmr\": Usa MMR para balancear relevancia y diversidad\n",
    "# k=4: Recuperar los 4 documentos m√°s relevantes\n",
    "# lambda_mult=0.7: Factor que controla el balance entre relevancia (1.0) y diversidad (0.0)\n",
    "#                  0.7 significa 70% relevancia, 30% diversidad\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\", \n",
    "    search_kwargs={\"k\": 4, \"lambda_mult\": 0.7}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f43149d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002A7A20EB0B0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002A7A3978800>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paso 2: Configurar el LLM (Large Language Model) y variables de entorno\n",
    "\n",
    "# Importar m√≥dulos para manejo de variables de entorno\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Cargar las variables de entorno desde el archivo .env\n",
    "# Este archivo contiene las API keys de forma segura\n",
    "load_dotenv()\n",
    "\n",
    "# Establecer la API key de Groq desde las variables de entorno\n",
    "# Groq ofrece inferencia de LLM ultra-r√°pida\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Inicializar el LLM usando el modelo Gemma2-9B-IT de Groq\n",
    "# Gemma2 es un modelo eficiente de Google, optimizado para tareas de razonamiento\n",
    "# \"it\" significa \"instruction-tuned\" (ajustado para seguir instrucciones)\n",
    "llm = init_chat_model(model=\"groq:llama-3.1-8b-instant\")\n",
    "\n",
    "# Mostrar la configuraci√≥n del LLM\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0af1982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3: Crear la cadena de descomposici√≥n de consultas\n",
    "\n",
    "# Esta plantilla de prompt instruye al LLM para descomponer consultas complejas\n",
    "# El objetivo es dividir una pregunta compleja en 2-4 sub-preguntas m√°s simples\n",
    "# Cada sub-pregunta debe ser at√≥mica (enfocada en un solo aspecto)\n",
    "decomposition_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Eres un asistente de IA. Descomp√≥n la siguiente pregunta compleja en 2 a 4 sub-preguntas m√°s simples para mejorar la recuperaci√≥n de documentos.\n",
    "\n",
    "Pregunta: \"{question}\"\n",
    "\n",
    "Sub-preguntas:\n",
    "\"\"\")\n",
    "\n",
    "# Crear la cadena de descomposici√≥n de consultas\n",
    "# Esta cadena conecta: prompt ‚Üí LLM ‚Üí parser de salida\n",
    "# El operador | (pipe) encadena los componentes secuencialmente\n",
    "decomposition_chain = decomposition_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c9797ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 4: Probar la descomposici√≥n de una consulta compleja\n",
    "\n",
    "# Definir una consulta compleja que involucra m√∫ltiples conceptos:\n",
    "# 1. Memoria en LangChain\n",
    "# 2. Agentes en LangChain\n",
    "# 3. Memoria en CrewAI\n",
    "# 4. Agentes en CrewAI\n",
    "# 5. Comparaci√≥n entre ambos\n",
    "query = \"¬øC√≥mo usa LangChain la memoria y los agentes comparado con CrewAI?\"\n",
    "\n",
    "# Invocar la cadena de descomposici√≥n para dividir la consulta compleja\n",
    "# El LLM generar√° 2-4 sub-preguntas m√°s espec√≠ficas y enfocadas\n",
    "decomposition_question = decomposition_chain.invoke({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b4819b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Sub-preguntas Generadas:\n",
      "Para mejorar la recuperaci√≥n de documentos, puedo descomponer la pregunta original en 4 sub-preguntas m√°s simples. Aqu√≠ te presento la descomposici√≥n:\n",
      "\n",
      "1. **¬øQu√© es LangChain y qu√© tipo de memoria utiliza?**\n",
      " - Esta sub-pregunta busca entender la arquitectura fundamental de LangChain y c√≥mo se relaciona con la memoria.\n",
      "\n",
      "2. **¬øC√≥mo funcionan los agentes en LangChain?**\n",
      " - Esta sub-pregunta se enfoca en la funcionalidad de los agentes en LangChain y c√≥mo interact√∫an con la memoria.\n",
      "\n",
      "3. **¬øQu√© es CrewAI y qu√© tipo de memoria utiliza?**\n",
      " - Esta sub-pregunta busca entender la arquitectura de CrewAI y c√≥mo se relaciona con la memoria, para poder compararla con LangChain.\n",
      "\n",
      "4. **¬øC√≥mo comparar la implementaci√≥n de memoria y agentes en LangChain y CrewAI?**\n",
      " - Esta sub-pregunta busca identificar los aspectos clave para comparar la implementaci√≥n de memoria y agentes en ambos frameworks.\n",
      "\n",
      "Al responder estas sub-preguntas, podr√≠amos obtener una comprensi√≥n m√°s detallada de c√≥mo LangChain y CrewAI utilizan la memoria y los agentes, y qu√© caracter√≠sticas son m√°s relevantes para comparar entre s√≠.\n"
     ]
    }
   ],
   "source": [
    "# Mostrar las sub-preguntas generadas por el LLM\n",
    "# Observa c√≥mo el LLM divide la consulta compleja en preguntas at√≥micas\n",
    "# Cada sub-pregunta se enfoca en un aspecto espec√≠fico\n",
    "print(\"üîç Sub-preguntas Generadas:\")\n",
    "print(decomposition_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5be04719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 5: Crear la cadena de Q&A para cada sub-pregunta\n",
    "\n",
    "# Esta plantilla define c√≥mo el LLM usar√° el contexto recuperado para responder\n",
    "# Se aplicar√° individualmente a cada sub-pregunta\n",
    "qa_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Usa el contexto a continuaci√≥n para responder la pregunta.\n",
    "\n",
    "Contexto:\n",
    "{context}\n",
    "\n",
    "Pregunta: {input}\n",
    "\"\"\")\n",
    "\n",
    "# Crear la cadena de documentos que combina el LLM con el prompt de Q&A\n",
    "# Esta cadena toma documentos recuperados y los inserta en el prompt\n",
    "# \"stuff\" significa que todos los documentos se insertan directamente en el prompt\n",
    "qa_chain = create_stuff_documents_chain(llm=llm, prompt=qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26c735b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 6: Construir el pipeline RAG completo con descomposici√≥n de consultas\n",
    "\n",
    "def full_query_decomposition_rag_pipeline(user_query):\n",
    "    \"\"\"\n",
    "    Pipeline RAG completo que implementa descomposici√≥n de consultas.\n",
    "    \n",
    "    Flujo del pipeline:\n",
    "    1. Descompone la consulta compleja en sub-preguntas\n",
    "    2. Para cada sub-pregunta:\n",
    "       a. Recupera documentos relevantes del vector store\n",
    "       b. Genera una respuesta usando los documentos como contexto\n",
    "    3. Combina todas las respuestas en un resultado final\n",
    "    \n",
    "    Args:\n",
    "        user_query (str): La consulta compleja del usuario\n",
    "    \n",
    "    Returns:\n",
    "        str: Respuestas combinadas para todas las sub-preguntas\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Descomponer la consulta compleja en sub-preguntas\n",
    "    # El LLM genera un texto con m√∫ltiples sub-preguntas separadas por saltos de l√≠nea\n",
    "    sub_qs_text = decomposition_chain.invoke({\"question\": user_query})\n",
    "    \n",
    "    # 2. Parsear el texto de sub-preguntas en una lista limpia\n",
    "    # - split(\"\\n\"): Divide el texto por saltos de l√≠nea\n",
    "    # - strip(\"-‚Ä¢1234567890. \"): Elimina caracteres de numeraci√≥n y vi√±etas\n",
    "    # - if q.strip(): Filtra l√≠neas vac√≠as\n",
    "    sub_questions = [\n",
    "        q.strip(\"-‚Ä¢1234567890. \").strip() \n",
    "        for q in sub_qs_text.split(\"\\n\") \n",
    "        if q.strip()\n",
    "    ]\n",
    "    \n",
    "    # 3. Lista para almacenar los resultados de cada sub-pregunta\n",
    "    results = []\n",
    "    \n",
    "    # 4. Procesar cada sub-pregunta individualmente\n",
    "    for subq in sub_questions:\n",
    "        # a. Recuperar documentos relevantes para esta sub-pregunta espec√≠fica\n",
    "        #    El recuperador usa MMR para obtener documentos relevantes y diversos\n",
    "        docs = retriever.invoke(subq)\n",
    "        \n",
    "        # b. Generar una respuesta usando los documentos recuperados como contexto\n",
    "        #    La cadena qa_chain inserta los documentos en el prompt y llama al LLM\n",
    "        result = qa_chain.invoke({\"input\": subq, \"context\": docs})\n",
    "        \n",
    "        # c. Formatear y almacenar el resultado con la pregunta y respuesta\n",
    "        results.append(f\"Q: {subq}\\nA: {result}\")\n",
    "    \n",
    "    # 5. Combinar todas las respuestas en un solo texto\n",
    "    # Separar cada par Q&A con dos saltos de l√≠nea para mejor legibilidad\n",
    "    return \"\\n\\n\".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac50f32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Respuesta Final:\n",
      "================================================================================\n",
      "Q: Excelente pregunta. Para descomponerla en sub-preguntas m√°s simples, te propongo las siguientes:\n",
      "A: No hay una pregunta espec√≠fica en el contexto proporcionado. Parece que est√°s presentando informaci√≥n sobre LangChain y CrewAI, que son herramientas relacionadas con la inteligencia artificial y los lenguajes de marcado natural (LLM). Si te gustar√≠a formular una pregunta sobre este tema, estar√© encantado de ayudarte a descomponerla en sub-preguntas m√°s simples. ¬øCu√°l es tu pregunta?\n",
      "\n",
      "Q: **¬øQu√© es LangChain y qu√© tipo de memoria utiliza?**\n",
      "A: LangChain es una plataforma para desarrollar aplicaciones con Inteligencia Artificial de Lenguaje (LLM) escalables y f√°ciles de mantener. \n",
      "\n",
      "LangChain utiliza dos tipos de memoria: \n",
      "\n",
      "- **ConversationBufferMemory**: que permite que el LLM mantenga la informaci√≥n de los turnos de conversaci√≥n anteriores.\n",
      "- **ConversationSummaryMemory**: que resume interacciones largas para ajustarse a los l√≠mites de tokens.\n",
      "\n",
      "Q: * Esta sub-pregunta busca comprender el concepto fundamental de LangChain y su enfoque en la memoria, lo que nos ayudar√° a comparar con CrewAI\n",
      "A: Basado en el contexto proporcionado, parece que el concepto fundamental de LangChain es la creaci√≥n de cadenas, que son secuencias de llamadas a LLM (Modelos de Lenguaje de Larga Memoria) y otras herramientas. Estas cadenas pueden ser simples o complejas, y LangChain facilita la composici√≥n y reutilizaci√≥n de estas cadenas.\n",
      "\n",
      "Una pregunta clave es: ¬øQu√© papel juega la memoria en el enfoque de LangChain?\n",
      "\n",
      "La respuesta es que LangChain utiliza memoria contextual en todos los pasos, lo que significa que cada paso de la cadena tiene acceso a la informaci√≥n anterior y puede utilizarla para tomar decisiones. Esto permite a LangChain utilizar la memoria para tomar decisiones din√°micas, razonar de manera condicional y realizar otras tareas complejas.\n",
      "\n",
      "En resumen, la memoria es un componentes clave de la arquitectura de LangChain, lo que permite a los modelos de lenguaje y herramientas interactuar de manera m√°s inteligente y flexible. Esto contrasta con el enfoque de CrewAI, que se centra en la colaboraci√≥n basada en roles y el uso de memoria contextual en todos los pasos.\n",
      "\n",
      "Q: **¬øC√≥mo funcionan los agentes en LangChain?**\n",
      "A: Seg√∫n el contexto proporcionado, los agentes en LangChain funcionan mediante un modelo planificador-ejecutor. Esto significa que los agentes planifican una secuencia de invocaciones de herramientas para lograr un objetivo, lo que puede incluir:\n",
      "\n",
      "- Toma de decisiones din√°mica\n",
      "- L√≥gica de ramificaci√≥n\n",
      "- Uso de memoria contextual en todos los pasos\n",
      "\n",
      "Adem√°s, los desarrolladores pueden definir un equipo (agente) mediante una configuraci√≥n similar a YAML o JSON, especificando los roles, objetivos, memoria y herramientas de los agentes. CrewAI orquesta el bucle de agentes y gestiona los turnos y la toma de decisiones de forma aut√≥noma.\n",
      "\n",
      "Q: * Aqu√≠ buscamos entender el papel de los agentes en el marco de LangChain y c√≥mo interact√∫an con la memoria\n",
      "A: En el marco de LangChain, los agentes desempe√±an un papel fundamental en la toma de decisiones y la resoluci√≥n de problemas. Cada agente opera mediante un modelo planificador-ejecutor, que se encarga de planificar una secuencia de invocaciones de herramientas para lograr un objetivo espec√≠fico.\n",
      "\n",
      "La relaci√≥n entre los agentes y la memoria en LangChain es clave para entender c√≥mo funcionan estos modelos. Los agentes pueden acceder a diferentes tipos de memoria, como la memoria de conversaci√≥n (ConversationBufferMemory y ConversationSummaryMemory), que les permite mantener la informaci√≥n de los turnos de conversaci√≥n anteriores o resumir interacciones largas.\n",
      "\n",
      "La memoria contextual es utilizada en todos los pasos del proceso de toma de decisiones, lo que significa que los agentes pueden recordar informaci√≥n de conversaciones anteriores y utilizarla para tomar decisiones informadas en el presente.\n",
      "\n",
      "Aqu√≠ hay un resumen de c√≥mo interact√∫an los agentes con la memoria en LangChain:\n",
      "\n",
      "1. **Acceso a la memoria**: Los agentes pueden acceder a diferentes tipos de memoria, como la memoria de conversaci√≥n, para recordar informaci√≥n de conversaciones anteriores.\n",
      "2. **Uso de la memoria contextual**: Los agentes pueden utilizar la memoria contextual para tomar decisiones informadas en el presente, considerando la informaci√≥n de conversaciones anteriores.\n",
      "3. **Planificaci√≥n y ejecuci√≥n**: Los agentes planifican una secuencia de invocaciones de herramientas para lograr un objetivo espec√≠fico, utilizando la informaci√≥n disponible en la memoria.\n",
      "4. **Toma de decisiones din√°mica**: Los agentes pueden tomar decisiones din√°micas en funci√≥n de la informaci√≥n disponible en la memoria y del contexto de la conversaci√≥n actual.\n",
      "\n",
      "En resumen, los agentes en LangChain interact√∫an con la memoria de manera integral, utilizando la informaci√≥n disponible para tomar decisiones informadas y lograr objetivos espec√≠ficos.\n",
      "\n",
      "Q: **¬øQu√© tipo de memoria utiliza CrewAI?**\n",
      "A: Seg√∫n el texto proporcionado, CrewAI utiliza memoria contextual en todos los pasos (v10).\n",
      "\n",
      "Q: * Al comparar la memoria utilizada por CrewAI, podemos comprender mejor sus fortalezas y debilidades en relaci√≥n con LangChain\n",
      "A: Al comparar la memoria utilizada por CrewAI y LangChain, podemos destacar las siguientes caracter√≠sticas y diferencias:\n",
      "\n",
      "**Similaridades:**\n",
      "\n",
      "* Ambas plataformas utilizan memoria contextual para almacenar informaci√≥n sobre la conversaci√≥n o interacci√≥n en curso.\n",
      "* Tanto CrewAI como LangChain permiten la recuperaci√≥n y el acceso a informaci√≥n previa para ajustarse a los l√≠mites de tokens.\n",
      "\n",
      "**Diferencias:**\n",
      "\n",
      "* **Nivel de abstracci√≥n**: LangChain ofrece m√≥dulos de memoria como ConversationBufferMemory y ConversationSummaryMemory, que son m√°s espec√≠ficos y se enfocan en la gesti√≥n de la conversaci√≥n. Por otro lado, CrewAI utiliza memoria contextual a todos los pasos, lo que sugiere una mayor flexibilidad y capacidad para adaptarse a diferentes escenarios.\n",
      "* **Integraci√≥n con herramientas**: LangChain se enfoca en la composici√≥n y reutilizaci√≥n de cadenas, que son secuencias de llamadas a LLM y otras herramientas. CrewAI, por otro lado, se integra con agentes y herramientas de LangChain, lo que permite una mayor flexibilidad en la composici√≥n de sistemas h√≠bridos.\n",
      "* **Enfoque en la colaboraci√≥n**: CrewAI se enfoca en la colaboraci√≥n basada en roles, lo que sugiere una mayor capacidad para manejar interacciones complejas con m√∫ltiples actores. LangChain, por otro lado, se enfoca en la gesti√≥n de la conversaci√≥n y la recuperaci√≥n de informaci√≥n.\n",
      "\n",
      "En resumen, la memoria utilizada por CrewAI y LangChain comparte algunas caracter√≠sticas similares, pero tambi√©n hay diferencias significativas en cuanto a nivel de abstracci√≥n, integraci√≥n con herramientas y enfoque en la colaboraci√≥n. Al comprender estas diferencias, podemos tener una mejor comprensi√≥n de las fortalezas y debilidades de cada plataforma en relaci√≥n con la memoria utilizada.\n",
      "\n",
      "Q: **¬øC√≥mo se utilizan los agentes en CrewAI comparado con LangChain?**\n",
      "A: Seg√∫n el contexto proporcionado, los agentes se utilizan de manera diferente en CrewAI y LangChain.\n",
      "\n",
      "En CrewAI, los agentes se utilizan en un contexto de colaboraci√≥n basada en roles, donde cada agente tiene un rol definido (como investigador, planificador o ejecutor) y opera de forma semiindependiente en un contexto colaborativo. Los desarrolladores pueden definir un equipo mediante una configuraci√≥n similar a YAML o JSON, especificando los roles, objetivos, memoria y herramientas de los agentes. CrewAI orquesta el bucle de agentes y gestiona los turnos y la toma de decisiones de forma aut√≥noma.\n",
      "\n",
      "En LangChain, los agentes no se mencionan expl√≠citamente en el contexto proporcionado. Sin embargo, se menciona que LangChain facilita la composici√≥n y reutilizaci√≥n de cadenas utilizando patrones est√°ndar como Stuff, Map-Reduce y Refine. Esto sugiere que LangChain se centra m√°s en la composici√≥n y reutilizaci√≥n de cadenas de texto, en lugar de utilizar agentes con roles definidos como se hace en CrewAI.\n",
      "\n",
      "En resumen, los agentes en CrewAI se utilizan en un contexto de colaboraci√≥n basada en roles, mientras que LangChain se centra en la composici√≥n y reutilizaci√≥n de cadenas de texto.\n",
      "\n",
      "Q: * Esta sub-pregunta busca identificar las similitudes y diferencias en la implementaci√≥n de los agentes en ambos marcos, lo que nos ayudar√° a comprender mejor su comparativa\n",
      "A: Bas√°ndome en el contexto proporcionado, puedo identificar algunas similitudes y diferencias en la implementaci√≥n de los agentes en CrewAI y LangChain.\n",
      "\n",
      "**Similitudes:**\n",
      "\n",
      "1. Ambos marcos se basan en el concepto de agentes aut√≥nomos, lo que significa que cada agente puede operar de forma semiindependiente en un contexto colaborativo.\n",
      "2. Los agentes en ambos marcos pueden utilizar LLM (Language Models) para razonar y tomar decisiones.\n",
      "3. Los agentes en ambos marcos pueden integrarse con herramientas y fuentes de conocimiento externas, como bases de datos y APIs.\n",
      "\n",
      "**Diferencias:**\n",
      "\n",
      "1. **Rol de los agentes**: En CrewAI, los agentes tienen roles definidos como investigador, planificador o ejecutor, lo que sugiere una estructura m√°s r√≠gida. En LangChain, los agentes pueden ejecutar tareas de varios pasos y integrarse con diferentes herramientas y fuentes de conocimiento.\n",
      "2. **Grado de autonom√≠a**: CrewAI orquesta el bucle de agentes y gestiona los turnos y la toma de decisiones de forma aut√≥noma, lo que implica una mayor influencia del marco sobre la ejecuci√≥n de los agentes. En LangChain, los agentes tienen m√°s libertad para tomar decisiones y ejecutar tareas de forma aut√≥noma.\n",
      "3. **Integraci√≥n con herramientas**: LangChain ofrece una mayor flexibilidad para integrar herramientas y fuentes de conocimiento externas a trav√©s de canales de extremo a extremo, lo que permite a los desarrolladores crear flujos de trabajo m√°s complejos y personalizados. CrewAI, por otro lado, se centra en la orquestaci√≥n de agentes y la gesti√≥n de la memoria y la recuperaci√≥n.\n",
      "\n",
      "En resumen, aunque ambos marcos comparten algunas similitudes en la implementaci√≥n de los agentes, existen diferencias significativas en la estructura, grado de autonom√≠a y integraci√≥n con herramientas.\n",
      "\n",
      "Q: Al responder estas sub-preguntas, podremos recuperar informaci√≥n m√°s precisa y relevante para mejorar la comparativa entre LangChain y CrewAI\n",
      "A: ¬°Excelente elecci√≥n de contexto! Para responder a la pregunta de c√≥mo mejorar la comparativa entre LangChain y CrewAI, te propongo las siguientes sub-preguntas:\n",
      "\n",
      "1. **¬øC√≥mo se pueden combinar los m√≥dulos de LangChain con la tecnolog√≠a de orquestaci√≥n de CrewAI para crear agentes colaborativos escalables y f√°ciles de mantener?**\n",
      "\n",
      "Esta sub-pregunta busca entender c√≥mo se pueden integrar los componentes de LangChain con la arquitectura de CrewAI para aprovechar las ventajas de ambos frameworks.\n",
      "\n",
      "2. **¬øC√≥mo se puede utilizar la tecnolog√≠a de Recuperaci√≥n de Conocimiento Externo (RAG) de CrewAI para mejorar la precisi√≥n y reducir la alucinaci√≥n en los modelos LLM de LangChain?**\n",
      "\n",
      "Esta sub-pregunta busca entender c√≥mo se puede utilizar RAG para mejorar la precisi√≥n de los modelos LLM de LangChain y reducir la alucinaci√≥n.\n",
      "\n",
      "3. **¬øC√≥mo se pueden utilizar los m√≥dulos de memoria de LangChain, como ConversationBufferMemory y ConversationSummaryMemory, para almacenar y recuperar informaci√≥n de conversaciones largas en CrewAI?**\n",
      "\n",
      "Esta sub-pregunta busca entender c√≥mo se pueden utilizar los m√≥dulos de memoria de LangChain para almacenar y recuperar informaci√≥n de conversaciones largas en la arquitectura de CrewAI.\n",
      "\n",
      "4. **¬øC√≥mo se pueden utilizar los agentes colaborativos de CrewAI para estructurar tareas y dividir responsabilidades en proyectos que involucran LangChain?**\n",
      "\n",
      "Esta sub-pregunta busca entender c√≥mo se pueden utilizar los agentes colaborativos de CrewAI para estructurar tareas y dividir responsabilidades en proyectos que involucran LangChain.\n",
      "\n",
      "5. **¬øC√≥mo se pueden comparar los beneficios de escalabilidad y mantenimiento de LangChain con la capacidad de orquestaci√≥n de CrewAI para crear agentes colaborativos?**\n",
      "\n",
      "Esta sub-pregunta busca entender c√≥mo se pueden comparar los beneficios de escalabilidad y mantenimiento de LangChain con la capacidad de orquestaci√≥n de CrewAI para crear agentes colaborativos.\n",
      "\n",
      "Respuestas a estas sub-preguntas pueden ayudarte a mejorar la comparativa entre LangChain y CrewAI.\n"
     ]
    }
   ],
   "source": [
    "# Paso 7: Ejecutar el pipeline completo con una consulta compleja\n",
    "\n",
    "# Definir la consulta compleja que involucra comparaci√≥n entre dos frameworks\n",
    "query = \"¬øC√≥mo usa LangChain la memoria y los agentes comparado con CrewAI?\"\n",
    "\n",
    "# Invocar el pipeline RAG completo con descomposici√≥n de consultas\n",
    "# El pipeline autom√°ticamente:\n",
    "# 1. Descompondr√° la consulta en sub-preguntas\n",
    "# 2. Recuperar√° documentos para cada sub-pregunta\n",
    "# 3. Generar√° respuestas individuales\n",
    "# 4. Combinar√° todas las respuestas\n",
    "final_answer = full_query_decomposition_rag_pipeline(query)\n",
    "\n",
    "# Mostrar el resultado final con todas las sub-preguntas y sus respuestas\n",
    "print(\"‚úÖ Respuesta Final:\")\n",
    "print(\"=\"*80)\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "### üìä Resumen: Beneficios de la Descomposici√≥n de Consultas\n",
    "\n",
    "**Flujo de Descomposici√≥n:**\n",
    "```\n",
    "Consulta Compleja\n",
    "       ‚Üì\n",
    "Descomposici√≥n (LLM)\n",
    "       ‚Üì\n",
    "Sub-pregunta 1 ‚Üí Recuperaci√≥n ‚Üí Respuesta 1\n",
    "Sub-pregunta 2 ‚Üí Recuperaci√≥n ‚Üí Respuesta 2\n",
    "Sub-pregunta 3 ‚Üí Recuperaci√≥n ‚Üí Respuesta 3\n",
    "Sub-pregunta 4 ‚Üí Recuperaci√≥n ‚Üí Respuesta 4\n",
    "       ‚Üì\n",
    "Respuesta Final Combinada\n",
    "```\n",
    "\n",
    "**Ventajas clave:**\n",
    "- ‚úÖ **Precisi√≥n mejorada**: Cada sub-pregunta obtiene contexto espec√≠fico\n",
    "- ‚úÖ **Razonamiento multi-hop**: Permite responder preguntas que requieren m√∫ltiples pasos\n",
    "- ‚úÖ **Mejor cobertura**: No se pierden aspectos de la pregunta original\n",
    "- ‚úÖ **Recuperaci√≥n enfocada**: Cada b√∫squeda es m√°s espec√≠fica y relevante\n",
    "- ‚úÖ **Respuestas estructuradas**: El resultado final est√° organizado por aspectos\n",
    "\n",
    "**Casos de uso ideales:**\n",
    "- ‚úÖ Preguntas comparativas (\"A vs B\")\n",
    "- ‚úÖ Consultas multi-aspecto (memoria + agentes + herramientas)\n",
    "- ‚úÖ Preguntas que requieren razonamiento en pasos\n",
    "- ‚úÖ An√°lisis complejos que involucran m√∫ltiples entidades\n",
    "- ‚úÖ Investigaci√≥n exploratoria de temas amplios\n",
    "\n",
    "**Consideraciones:**\n",
    "- ‚ö†Ô∏è **Latencia aumentada**: M√∫ltiples llamadas al LLM (1 descomposici√≥n + N respuestas)\n",
    "- ‚ö†Ô∏è **Costo mayor**: M√°s tokens consumidos por las llamadas adicionales\n",
    "- ‚ö†Ô∏è **Complejidad**: Requiere parseo y manejo de m√∫ltiples sub-preguntas\n",
    "- ‚ö†Ô∏è **No siempre necesario**: Para consultas simples, puede ser overkill\n",
    "\n",
    "**Comparaci√≥n con Query Expansion:**\n",
    "\n",
    "| Aspecto | Query Expansion | Query Decomposition |\n",
    "|---------|----------------|---------------------|\n",
    "| **Prop√≥sito** | Enriquecer consulta con sin√≥nimos | Dividir consulta en partes |\n",
    "| **Llamadas LLM** | 1 expansi√≥n + 1 respuesta | 1 descomposici√≥n + N respuestas |\n",
    "| **Mejor para** | Consultas vagas o t√©cnicas | Consultas complejas multi-aspecto |\n",
    "| **Latencia** | Baja-Media | Media-Alta |\n",
    "| **Precisi√≥n** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "\n",
    "**¬øCu√°ndo usar cada t√©cnica?**\n",
    "- **Query Expansion**: \"memoria en LangChain\" ‚Üí agregar sin√≥nimos t√©cnicos\n",
    "- **Query Decomposition**: \"¬øC√≥mo se compara LangChain con CrewAI en memoria y agentes?\" ‚Üí dividir en sub-preguntas espec√≠ficas\n",
    "- **Ambas**: Para m√°xima precisi√≥n, primero descomponer y luego expandir cada sub-pregunta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
