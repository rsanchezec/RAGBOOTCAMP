{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f25489ee",
   "metadata": {},
   "source": [
    "### Mejora de Consultas – Técnicas de Expansión de Consultas\n",
    "\n",
    "En un pipeline RAG, la calidad de la consulta enviada al recuperador determina qué tan bueno es el contexto recuperado — y por lo tanto, qué tan precisa será la respuesta final del LLM.\n",
    "\n",
    "Ahí es donde entra la **Expansión / Mejora de Consultas**.\n",
    "\n",
    "#### 🎯 ¿Qué es la Mejora de Consultas?\n",
    "La mejora de consultas se refiere a técnicas utilizadas para mejorar o reformular la consulta del usuario para recuperar documentos mejores y más relevantes de la base de conocimiento.\n",
    "\n",
    "Es especialmente útil cuando:\n",
    "\n",
    "- La consulta original es corta, ambigua o está poco especificada\n",
    "- Quieres ampliar el alcance para capturar sinónimos, frases relacionadas o variantes ortográficas\n",
    "- Necesitas agregar contexto técnico o terminología específica del dominio\n",
    "\n",
    "**Ventajas de la Expansión de Consultas:**\n",
    "- ✅ Mejora la recuperación de documentos relevantes\n",
    "- ✅ Captura variaciones semánticas de la consulta original\n",
    "- ✅ Reduce falsos negativos (documentos relevantes no encontrados)\n",
    "- ✅ Enriquece consultas vagas con contexto adicional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ff6ab72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Udemy\\RAGBootcamp\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importación de librerías necesarias para el pipeline RAG con expansión de consultas\n",
    "\n",
    "# TextLoader: Para cargar archivos de texto plano\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# RecursiveCharacterTextSplitter: Para dividir texto en fragmentos (chunks) manejables\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# HuggingFaceEmbeddings: Para generar embeddings vectoriales usando modelos de HuggingFace\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# FAISS: Biblioteca de Facebook para búsqueda de similitud vectorial eficiente\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# init_chat_model: Para inicializar modelos de chat de diferentes proveedores\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# PromptTemplate: Para crear plantillas de prompts con variables dinámicas\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# create_stuff_documents_chain: Crea una cadena que inserta documentos en un prompt\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# create_retrieval_chain: Combina recuperación de documentos con generación de respuestas\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "# StrOutputParser: Para convertir la salida del LLM a string\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# RunnableMap: Permite ejecutar múltiples operaciones en paralelo y combinar resultados\n",
    "from langchain_core.runnables import RunnableMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "410d3b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Cargar y dividir el dataset\n",
    "\n",
    "# Cargar el archivo de texto que contiene información sobre LangChain y CrewAI\n",
    "loader = TextLoader(\"langchain_crewai_dataset.txt\")\n",
    "\n",
    "# load() devuelve una lista de documentos con todo el contenido del archivo\n",
    "raw_docs = loader.load()\n",
    "\n",
    "# Crear un splitter para dividir el texto en fragmentos más pequeños\n",
    "# chunk_size=300: Cada fragmento tendrá máximo 300 caracteres\n",
    "# chunk_overlap=50: Habrá una superposición de 50 caracteres entre fragmentos consecutivos\n",
    "# La superposición ayuda a mantener el contexto entre fragmentos\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "\n",
    "# Dividir los documentos crudos en fragmentos (chunks) manejables\n",
    "chunks = splitter.split_documents(raw_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "791ba3d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='LangChain es un framework de cÃ³digo abierto diseÃ±ado para desarrollar aplicaciones basadas en grandes modelos de lenguaje (LLM). Simplifica el proceso de creaciÃ³n, gestiÃ³n y escalado de cadenas de pensamiento complejas al abstraer la gestiÃ³n de indicaciones, la recuperaciÃ³n, la memoria y la'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='indicaciones, la recuperaciÃ³n, la memoria y la orquestaciÃ³n de agentes. Los desarrolladores pueden usar LangChain para crear canales de extremo a extremo que conectan los LLM con herramientas, API, bases de datos vectoriales y otras fuentes de conocimiento. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='La base de LangChain es el concepto de cadenas, que son secuencias de llamadas a LLM y otras herramientas. Las cadenas pueden ser simples, como una sola indicaciÃ³n enviada a un LLM, o complejas, con mÃºltiples pasos ejecutados condicionalmente. LangChain facilita la composiciÃ³n y reutilizaciÃ³n'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='facilita la composiciÃ³n y reutilizaciÃ³n de cadenas utilizando patrones estÃ¡ndar como Stuff, Map-Reduce y Refine. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='LangChain se integra a la perfecciÃ³n con bases de datos vectoriales como FAISS, Chroma, Pinecone y Weaviate, lo que permite la bÃºsqueda semÃ¡ntica en grandes corpus de documentos. Esta capacidad es especialmente importante en la GeneraciÃ³n Aumentada por RecuperaciÃ³n (RAG), donde se obtiene'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='por RecuperaciÃ³n (RAG), donde se obtiene conocimiento externo y se inyecta en el prompt LLM para mejorar la precisiÃ³n y reducir la alucinaciÃ³n. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='LangChain tambiÃ©n admite la recuperaciÃ³n hÃ\\xadbrida, que combina mÃ©todos de recuperaciÃ³n basados â€‹â€‹en palabras clave (dispersos), como BM25, con la recuperaciÃ³n basada en incrustaciÃ³n (densos). Esto garantiza una mejor recuperaciÃ³n al detectar tanto coincidencias exactas de tÃ©rminos como'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='tanto coincidencias exactas de tÃ©rminos como contenido semÃ¡nticamente similar. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='Una de las caracterÃ\\xadsticas destacadas de LangChain es su compatibilidad con agentes. Los agentes utilizan LLM para razonar sobre quÃ© herramienta llamar, quÃ© entrada proporcionar y cÃ³mo procesar la salida. Los agentes de LangChain pueden ejecutar tareas de varios pasos, integrÃ¡ndose con'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='tareas de varios pasos, integrÃ¡ndose con herramientas como bÃºsquedas web, calculadoras, entornos de ejecuciÃ³n de cÃ³digo y API personalizadas. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='Los agentes de LangChain operan mediante un modelo planificador-ejecutor, donde el agente planifica una secuencia de invocaciones de herramientas para lograr un objetivo. Esto puede incluir la toma de decisiones dinÃ¡mica, la lÃ³gica de ramificaciÃ³n y el uso de memoria contextual en todos los'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='y el uso de memoria contextual en todos los pasos. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='LangChain ofrece mÃ³dulos de memoria como ConversationBufferMemory y ConversationSummaryMemory. Estos permiten que el LLM mantenga la informaciÃ³n de los turnos de conversaciÃ³n anteriores o resuma interacciones largas para ajustarse a los lÃ\\xadmites de tokens. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='La ingenierÃ\\xada de indicaciones es fundamental para el diseÃ±o de LangChain. Proporciona funciones de creaciÃ³n de plantillas, variables de entrada, opciones de formato y encadenamiento de indicaciones. Los desarrolladores pueden reutilizar las plantillas de indicaciones en diferentes cadenas e'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='de indicaciones en diferentes cadenas e incluso anidarlas. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='LangChain es compatible con mÃºltiples proveedores de LLM, como OpenAI, Anthropic, Cohere, Hugging Face y otros. Esta flexibilidad garantiza que los desarrolladores puedan cambiar entre modelos sin reescribir la lÃ³gica central. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='Los flujos de trabajo de LangChain son modulares y componibles. Componentes como recuperadores, memorias, agentes y cadenas se pueden combinar y reutilizar fÃ¡cilmente. Esto lo hace ideal para crear aplicaciones LLM escalables y fÃ¡ciles de mantener. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='CrewAI es un marco de orquestaciÃ³n multiagente diseÃ±ado para crear agentes colaborativos con tecnologÃ\\xada LLM. Permite a los desarrolladores estructurar agentes en equipos organizados que trabajan juntos para completar tareas dividiendo responsabilidades, compartiendo contexto y comunicÃ¡ndose'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='compartiendo contexto y comunicÃ¡ndose dinÃ¡micamente entre sÃ\\xad. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='CrewAI se basa en el concepto de agentes autÃ³nomos, pero lo mejora al permitirles formar flujos de trabajo estructurados. Cada agente de un equipo tiene un rol definido, como investigador, planificador o ejecutor, y opera de forma semiindependiente en un contexto colaborativo. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='Los agentes de CrewAI se definen con un propÃ³sito, un objetivo y un conjunto de herramientas que pueden utilizar. El marco garantiza que cada agente se mantenga concentrado en su tarea y contribuya significativamente al objetivo general del equipo. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='Una de las principales innovaciones de CrewAI es el uso del intercambio de contexto entre agentes, donde los agentes se intercambian datos intermedios de forma estructurada. Esto genera comportamientos emergentes como la delegaciÃ³n, la consulta y la revisiÃ³n entre agentes. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='CrewAI es especialmente Ãºtil en flujos de trabajo de varios pasos, como estudios de mercado, anÃ¡lisis de documentos legales, desarrollo de productos y asistentes de codificaciÃ³n, donde las tareas complejas se benefician de la especializaciÃ³n y la colaboraciÃ³n. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='El framework admite la trazabilidad completa de las decisiones e interacciones de los agentes, lo que facilita la depuraciÃ³n y la transparencia en comparaciÃ³n con las configuraciones de agentes independientes. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='CrewAI es compatible con los agentes y herramientas de LangChain, lo que permite sistemas hÃ\\xadbridos donde LangChain gestiona la recuperaciÃ³n y el encapsulado de herramientas, mientras que CrewAI gestiona la colaboraciÃ³n basada en roles. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='Los desarrolladores pueden definir un equipo mediante una configuraciÃ³n similar a YAML o JSON, especificando los roles, objetivos, memoria y herramientas de los agentes. CrewAI orquesta el bucle de agentes y gestiona los turnos y la toma de decisiones de forma autÃ³noma. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='CrewAI admite mÃºltiples backends LLM e incluye compatibilidad con streaming, ejecuciÃ³n paralela e invocaciÃ³n asÃ\\xadncrona de herramientas, lo que lo hace adecuado tanto para sistemas de prototipado rÃ¡pido como para sistemas listos para producciÃ³n. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='Al permitir la colaboraciÃ³n estructurada entre agentes, CrewAI permite a los equipos construir sistemas inteligentes que escalan tanto horizontalmente (mÃ¡s agentes) como verticalmente (Mayor profundidad de razonamiento). (v9)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='LangChain es un framework de cÃ³digo abierto diseÃ±ado para desarrollar aplicaciones basadas en grandes modelos de lenguaje (LLM). Simplifica el proceso de creaciÃ³n, gestiÃ³n y escalado de cadenas de pensamiento complejas al abstraer la gestiÃ³n de indicaciones, la recuperaciÃ³n, la memoria y la'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='indicaciones, la recuperaciÃ³n, la memoria y la orquestaciÃ³n de agentes. Los desarrolladores pueden usar LangChain para crear canales de extremo a extremo que conectan los LLM con herramientas, API, bases de datos vectoriales y otras fuentes de conocimiento. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='En el corazÃ³n de LangChain se encuentra el concepto de cadenas, que son secuencias de llamadas a los LLM y otras herramientas. Las cadenas pueden ser simples, como una sola indicaciÃ³n enviada a un LLM, o complejas, con mÃºltiples pasos ejecutados condicionalmente. LangChain facilita la'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='condicionalmente. LangChain facilita la composiciÃ³n y reutilizaciÃ³n de cadenas utilizando patrones estÃ¡ndar como Stuff, Map-Reduce y Refine. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='LangChain se integra perfectamente con bases de datos vectoriales como FAISS, Chroma, Pinecone y Weaviate, lo que permite la bÃºsqueda semÃ¡ntica en grandes corpus de documentos. Esta capacidad es especialmente importante en la GeneraciÃ³n Aumentada por RecuperaciÃ³n (RAG), donde se obtiene'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='por RecuperaciÃ³n (RAG), donde se obtiene conocimiento externo y se inyecta en el prompt LLM para mejorar la precisiÃ³n y reducir la alucinaciÃ³n. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='LangChain tambiÃ©n admite la recuperaciÃ³n hÃ\\xadbrida, que combina mÃ©todos de recuperaciÃ³n basados â€‹â€‹en palabras clave (dispersos), como BM25, con la recuperaciÃ³n basada en incrustaciÃ³n (densos). Esto garantiza una mejor recuperaciÃ³n al detectar tanto coincidencias exactas de tÃ©rminos como'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='tanto coincidencias exactas de tÃ©rminos como contenido semÃ¡nticamente similar. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='Una de las caracterÃ\\xadsticas destacadas de LangChain es su compatibilidad con agentes. Los agentes utilizan LLM para razonar sobre quÃ© herramienta llamar, quÃ© entrada proporcionar y cÃ³mo procesar la salida. Los agentes de LangChain pueden ejecutar tareas de varios pasos, integrÃ¡ndose con'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='tareas de varios pasos, integrÃ¡ndose con herramientas como bÃºsquedas web, calculadoras, entornos de ejecuciÃ³n de cÃ³digo y API personalizadas. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='Los agentes de LangChain operan mediante un modelo planificador-ejecutor, donde el agente planifica una secuencia de invocaciones de herramientas para lograr un objetivo. Esto puede incluir la toma de decisiones dinÃ¡mica, la lÃ³gica de ramificaciÃ³n y el uso de memoria contextual en todos los'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='y el uso de memoria contextual en todos los pasos. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='LangChain ofrece mÃ³dulos de memoria como ConversationBufferMemory y ConversationSummaryMemory. Estos permiten que el LLM mantenga la informaciÃ³n de los turnos de conversaciÃ³n anteriores o resuma interacciones largas para ajustarse a los lÃ\\xadmites de tokens. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='La ingenierÃ\\xada de indicaciones es fundamental para el diseÃ±o de LangChain. Proporciona funciones de creaciÃ³n de plantillas, variables de entrada, opciones de formato y encadenamiento de indicaciones. Los desarrolladores pueden reutilizar las plantillas de indicaciones en diferentes cadenas e'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='de indicaciones en diferentes cadenas e incluso anidarlas. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='LangChain es compatible con mÃºltiples proveedores de LLM, como OpenAI, Anthropic, Cohere, Hugging Face y otros. Esta flexibilidad garantiza que los desarrolladores puedan cambiar entre modelos sin reescribir la lÃ³gica central. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='Los flujos de trabajo de LangChain son modulares y componibles. Componentes como recuperadores, memorias, agentes y cadenas se pueden combinar y reutilizar fÃ¡cilmente. Esto lo hace ideal para crear aplicaciones LLM escalables y fÃ¡ciles de mantener. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='CrewAI es un marco de orquestaciÃ³n multiagente diseÃ±ado para crear agentes colaborativos con tecnologÃ\\xada LLM. Permite a los desarrolladores estructurar agentes en equipos organizados que trabajan juntos para completar tareas dividiendo responsabilidades, compartiendo contexto y comunicÃ¡ndose'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='compartiendo contexto y comunicÃ¡ndose dinÃ¡micamente entre sÃ\\xad. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='CrewAI se basa en el concepto de agentes autÃ³nomos, pero lo mejora al permitirles formar flujos de trabajo estructurados. Cada agente de un equipo tiene un rol definido, como investigador, planificador o ejecutor, y opera de forma semiindependiente en un contexto colaborativo. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='Los agentes de CrewAI se definen con un propÃ³sito, un objetivo y un conjunto de herramientas que pueden utilizar. El marco garantiza que cada agente se mantenga concentrado en su tarea y contribuya significativamente al objetivo general del equipo. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='Una de las principales innovaciones de CrewAI es el uso del intercambio de contexto entre agentes, donde los agentes se intercambian datos intermedios de forma estructurada. Esto genera comportamientos emergentes como la delegaciÃ³n, la consulta y la revisiÃ³n entre agentes. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='CrewAI es especialmente Ãºtil en flujos de trabajo de varios pasos, como estudios de mercado, anÃ¡lisis de documentos legales, desarrollo de productos y asistentes de codificaciÃ³n, donde las tareas complejas se benefician de la especializaciÃ³n y la colaboraciÃ³n. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='El framework admite la trazabilidad completa de las decisiones e interacciones de los agentes, lo que facilita la depuraciÃ³n y la transparencia en comparaciÃ³n con las configuraciones de agentes independientes. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='CrewAI es compatible con los agentes y herramientas de LangChain, lo que permite sistemas hÃ\\xadbridos donde LangChain gestiona la recuperaciÃ³n y el encapsulado de herramientas, mientras que CrewAI gestiona la colaboraciÃ³n basada en roles. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='Los desarrolladores pueden definir un equipo mediante una configuraciÃ³n similar a YAML o JSON, especificando los roles, objetivos, memoria y herramientas de los agentes. CrewAI orquesta el bucle de agentes y gestiona los turnos y la toma de decisiones de forma autÃ³noma. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='CrewAI admite mÃºltiples backends LLM e incluye compatibilidad con streaming, ejecuciÃ³n paralela e invocaciÃ³n asÃ\\xadncrona de herramientas, lo que lo hace adecuado tanto para prototipado rÃ¡pido como para sistemas listos para producciÃ³n. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='Al permitir la colaboraciÃ³n estructurada entre agentes, CrewAI permite a los equipos construir sistemas inteligentes que escalan tanto')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostrar los fragmentos generados\n",
    "# Esto nos permite ver cómo se dividió el documento\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f001ff8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000021169E9F440>, search_type='mmr', search_kwargs={'k': 5})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paso 2: Crear el almacén vectorial (Vector Store)\n",
    "\n",
    "# Inicializar el modelo de embeddings de HuggingFace\n",
    "# all-MiniLM-L6-v2 es un modelo compacto y eficiente que genera vectores de 384 dimensiones\n",
    "# Este modelo es ideal para búsqueda semántica en español e inglés\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Crear el almacén vectorial FAISS a partir de los fragmentos de documentos\n",
    "# FAISS convierte cada fragmento en un vector numérico para búsqueda de similitud eficiente\n",
    "vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
    "\n",
    "# Paso 3: Crear el recuperador con MMR (Maximal Marginal Relevance)\n",
    "# MMR balancea relevancia y diversidad para evitar documentos redundantes\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",  # Usar MMR en lugar de similitud simple\n",
    "    search_kwargs={\"k\": 5}  # Recuperar los 5 documentos más relevantes y diversos\n",
    ")\n",
    "\n",
    "# Mostrar la configuración del recuperador\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14e8f2fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000002116CF1E900>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002116D2361B0>, root_client=<openai.OpenAI object at 0x00000211676BA000>, root_async_client=<openai.AsyncOpenAI object at 0x000002116BCBAA50>, model_name='o4-mini', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paso 4: Configurar el LLM (Large Language Model) y variables de entorno\n",
    "\n",
    "# Importar módulos para manejo de variables de entorno\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Cargar las variables de entorno desde el archivo .env\n",
    "# Este archivo contiene las API keys de forma segura\n",
    "load_dotenv()\n",
    "\n",
    "# Establecer la API key de OpenAI desde las variables de entorno\n",
    "# Esta clave es necesaria para usar los modelos de OpenAI\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Inicializar el LLM usando el modelo o4-mini de OpenAI\n",
    "# Este modelo es eficiente y económico para tareas de expansión de consultas\n",
    "llm = init_chat_model(\"openai:o4-mini\")\n",
    "\n",
    "# Mostrar la configuración del LLM\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "116e2cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='\\nEres un asistente útil. Expande la siguiente consulta para mejorar la recuperación de documentos agregando sinónimos relevantes, términos técnicos y contexto útil.\\n\\nConsulta original: \"{query}\"\\n\\nConsulta expandida:\\n')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000002116CF1E900>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002116D2361B0>, root_client=<openai.OpenAI object at 0x00000211676BA000>, root_async_client=<openai.AsyncOpenAI object at 0x000002116BCBAA50>, model_name='o4-mini', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paso 5: Crear la cadena de expansión de consultas\n",
    "\n",
    "# Esta plantilla de prompt instruye al LLM para expandir la consulta del usuario\n",
    "# El objetivo es agregar sinónimos, términos técnicos y contexto útil\n",
    "query_expansion_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Eres un asistente útil. Expande la siguiente consulta para mejorar la recuperación de documentos agregando sinónimos relevantes, términos técnicos y contexto útil.\n",
    "\n",
    "Consulta original: \"{query}\"\n",
    "\n",
    "Consulta expandida:\n",
    "\"\"\")\n",
    "\n",
    "# Crear la cadena de expansión de consultas\n",
    "# Esta cadena conecta: prompt → LLM → parser de salida\n",
    "# El operador | (pipe) encadena los componentes secuencialmente\n",
    "query_expansion_chain = query_expansion_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Mostrar la estructura de la cadena de expansión\n",
    "query_expansion_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d629dd9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aquí tienes una posible versión de la consulta enriquecida, que incorpora sinónimos, términos técnicos y contexto para abarcar diversos aspectos de “LangChain memory”:\\n\\n(\"LangChain memory\"  \\n OR \"LangChain memory management\"  \\n OR \"LangChain memory module\"  \\n OR \"LangChain converse memory\"  \\n OR \"LangChain session memory\"  \\n OR \"LangChain persistent memory\"  \\n OR \"LangChain state management\"  \\n OR \"LangChain context storage\"  \\n OR \"LangChain cache\"  \\n OR \"LangChain long-term memory\"  \\n OR \"LangChain short-term memory\"  \\n OR \"LangChain external memory\"  \\n OR \"LangChain vector store memory\"  \\n OR \"LangChain retrieval-augmented memory\"  \\n OR \"LangChain RAG memory\"  \\n OR \"LangChain embedding cache\"  \\n OR \"stateful chatbots\"  \\n OR \"conversational AI memory\"  \\n OR \"context window management\"  \\n OR \"contextual retrieval\"  \\n OR \"memory classes in LangChain\"  \\n OR \"gestión de memoria LangChain\"  \\n OR \"módulo de memoria LangChain\"  \\n OR \"almacenamiento de contexto\"  \\n OR \"memoria conversacional\"  \\n OR \"almacén de vectores\")  \\nAND  \\n(\"tutorial\" OR \"guide\" OR \"API reference\" OR \"best practices\" OR \"use cases\" OR \"benchmark\" OR \"performance\" OR \"Python\" OR \"JavaScript\" OR \"Node.js\")  \\n\\nExplicación de las secciones:  \\n- Sinónimos y variantes sintácticas de “LangChain memory”  \\n- Términos técnicos (session vs. persistent vs. long-term vs. short-term memory)  \\n- Conceptos relacionados (state management, RAG, embeddings, vector store, cache)  \\n- Contexto de aplicación (chatbots, stateful AI, documentación, performance, lenguajes de implementación)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Probar la cadena de expansión de consultas\n",
    "# Enviamos una consulta simple sobre \"Langchain memory\" para ver cómo la expande el LLM\n",
    "# El LLM agregará términos técnicos, sinónimos y variaciones útiles\n",
    "query_expansion_chain.invoke({\"query\": \"Langchain memory\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec6e3ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 6: Crear el prompt para generar respuestas\n",
    "\n",
    "# Esta plantilla define cómo el LLM usará el contexto recuperado para responder\n",
    "answer_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Responde la pregunta basándote en el contexto a continuación.\n",
    "\n",
    "Contexto:\n",
    "{context}\n",
    "\n",
    "Pregunta: {input}\n",
    "\"\"\")\n",
    "\n",
    "# Crear la cadena de documentos que combina el LLM con el prompt de respuesta\n",
    "# Esta cadena toma documentos recuperados y los inserta en el prompt\n",
    "# \"stuff\" significa que todos los documentos se insertan directamente en el prompt\n",
    "document_chain = create_stuff_documents_chain(llm=llm, prompt=answer_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c57e726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 7: Construir el pipeline RAG completo con expansión de consultas\n",
    "\n",
    "# Este pipeline integra la expansión de consultas en el flujo RAG:\n",
    "# 1. Recibe la consulta original del usuario\n",
    "# 2. Expande la consulta usando el LLM (agregando sinónimos y contexto)\n",
    "# 3. Usa la consulta expandida para recuperar documentos relevantes\n",
    "# 4. Genera la respuesta final usando los documentos recuperados\n",
    "\n",
    "rag_pipeline = (\n",
    "    # RunnableMap ejecuta múltiples operaciones en paralelo\n",
    "    RunnableMap({\n",
    "        # Mantener la consulta original para la respuesta final\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \n",
    "        # Flujo de recuperación con expansión de consultas:\n",
    "        # 1. Toma la consulta original (x[\"input\"])\n",
    "        # 2. La expande usando query_expansion_chain\n",
    "        # 3. Usa la consulta expandida para invocar el retriever\n",
    "        # 4. Devuelve los documentos recuperados como contexto\n",
    "        \"context\": lambda x: retriever.invoke(\n",
    "            query_expansion_chain.invoke({\"query\": x[\"input\"]})\n",
    "        )\n",
    "    })\n",
    "    # El resultado del RunnableMap se pasa a document_chain para generar la respuesta\n",
    "    | document_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddebe80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Consulta Expandida:\n",
      "Consulta ampliada:\n",
      "\n",
      "“¿Cuáles son los distintos mecanismos y objetos de ‘Memory’ que soporta el framework LangChain para gestionar el contexto y el estado de las conversaciones con modelos de lenguaje? En concreto, me interesa conocer:\n",
      "\n",
      "1. Tipos de memoria integrados en LangChain  \n",
      "   - ConversationBufferMemory (caché de diálogo completo)  \n",
      "   - ConversationSummaryMemory (resumen incremental)  \n",
      "   - ConversationEntityMemory (extracción y seguimiento de entidades)  \n",
      "   - VectorStoreRetrieverMemory o VectorStoreMemory (memoria basada en vectores)  \n",
      "   - CombinedMemory (combinación de varios back-ends)  \n",
      "   - LongTermMemory (memoria a largo plazo)\n",
      "\n",
      "2. Sinónimos y términos relacionados  \n",
      "   - State management, session state, chat history  \n",
      "   - Context persistence, almacenamiento de contexto, retención de estado  \n",
      "   - Embedding store, vector database, base de datos de vectores\n",
      "\n",
      "3. Opciones de persistencia y back-ends compatibles  \n",
      "   - Redis, FAISS, ChromaDB, Milvus  \n",
      "   - SQLite, PostgreSQL, MongoDB (para memoria en disco)  \n",
      "   - Pinecone, Weaviate (servicios gestionados)\n",
      "\n",
      "4. Casos de uso y diferencias técnicas  \n",
      "   - ¿Cuándo conviene usar buffer vs. summary vs. entity memory?  \n",
      "   - Ventajas de la indexación vectorial para recuperación de contexto  \n",
      "   - Impacto en latencia y consumo de tokens\n",
      "\n",
      "5. Parámetros de configuración clave  \n",
      "   - Tamaño del buffer, estrategia de resumen (max_tokens, chunk_size)  \n",
      "   - Parámetros de embeddings (modelo, dimensionalidad, métricas de similitud)  \n",
      "   - Persistencia (auto_save, persist_directory, connection_args)\n",
      "\n",
      "Con esta descripción más completa, pretendo optimizar la búsqueda de documentación y ejemplos de implementación de cada tipo de memoria en LangChain.”\n",
      "\n",
      "================================================================================\n",
      "\n",
      "✅ Respuesta:\n",
      "LangChain incluye, al menos, dos módulos de memoria incorporados:\n",
      "\n",
      "1. ConversationBufferMemory  \n",
      "   • Guarda literalmente todos los turnos de conversación anteriores.  \n",
      "   • Útil cuando quieres disponer del historial completo para cada llamada al LLM.\n",
      "\n",
      "2. ConversationSummaryMemory  \n",
      "   • Resume las interacciones largas en un texto compacto.  \n",
      "   • Permite mantener el contexto esencial sin pasarse de límites de tokens.\n"
     ]
    }
   ],
   "source": [
    "# Paso 8: Ejecutar una consulta de ejemplo sobre tipos de memoria en LangChain\n",
    "\n",
    "# Definir la consulta del usuario\n",
    "query = {\"input\": \"¿Qué tipos de memoria soporta LangChain?\"}\n",
    "\n",
    "# Primero, mostrar cómo se expande la consulta\n",
    "# Esto nos permite ver qué términos adicionales agregó el LLM\n",
    "print(\"🔍 Consulta Expandida:\")\n",
    "print(query_expansion_chain.invoke({\"query\": query[\"input\"]}))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Invocar el pipeline RAG completo con la consulta\n",
    "# El pipeline automáticamente:\n",
    "# 1. Expandirá la consulta\n",
    "# 2. Recuperará documentos relevantes\n",
    "# 3. Generará una respuesta basada en el contexto\n",
    "response = rag_pipeline.invoke(query)\n",
    "\n",
    "# Mostrar la respuesta final del sistema RAG\n",
    "print(\"✅ Respuesta:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efd86621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Consulta Expandida:\n",
      "Consulta expandida:\n",
      "\n",
      "(“CrewAI” OR “Crew AI” OR “Crew-AI” OR “plataforma CrewAI” OR “framework CrewAI”)  \n",
      "AND  \n",
      "(“agente inteligente” OR “agente conversacional” OR “agente autónomo” OR “bot de IA” OR “agente de inteligencia artificial”)  \n",
      "AND  \n",
      "(“inteligencia artificial” OR “IA” OR “machine learning” OR “aprendizaje automático” OR “NLP” OR “procesamiento de lenguaje natural” OR “LLM” OR “modelos de lenguaje” OR “transformers”)  \n",
      "AND  \n",
      "(“documentación técnica” OR “guía de uso” OR “guía de implementación” OR “SDK” OR “API” OR “librería” OR “versión” OR “release notes”)  \n",
      "AND  \n",
      "(“arquitectura” OR “orquestación de agentes” OR “diseño multiagente” OR “integración” OR “casos de uso” OR “patrones de diseño” OR “benchmark” OR “rendimiento” OR “escalabilidad” OR “deployment” OR “CI/CD”)  \n",
      "AND  \n",
      "(“automatización de tareas” OR “asistentes virtuales” OR “sistemas de diálogo” OR “chatbots” OR “workflow inteligente” OR “pipeline de IA”)  \n",
      "\n",
      "Esta consulta combina sinónimos, términos técnicos y contexto (arquitectura, integración, casos de uso, rendimiento) para maximizar la cobertura en la recuperación de documentación sobre los agentes de CrewAI.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "✅ Respuesta:\n",
      "En CrewAI, los “agentes” son componentes autónomos y especializados que, en lugar de funcionar de manera aislada, se organizan en equipos para cubrir distintos roles dentro de un flujo de trabajo estructurado. Sus características clave son:\n",
      "\n",
      "1. Roles definidos  \n",
      "   - Investigador, planificador, ejecutor, revisor, etc.  \n",
      "   - Cada agente se centra en un subobjetivo concreto.\n",
      "\n",
      "2. Colaboración y orquestación  \n",
      "   - Intercambio de contexto estructurado (datos intermedios).  \n",
      "   - Delegación, consulta mutua y revisiones cruzadas entre agentes.\n",
      "\n",
      "3. Paralelismo y asincronía  \n",
      "   - Invocación paralela de herramientas y APIs.  \n",
      "   - Soporte para llamadas asíncronas y streaming de resultados.\n",
      "\n",
      "4. Flexibilidad de backends  \n",
      "   - Compatibilidad con múltiples LLM (GPT, PaLM, LLaMA, etc.).  \n",
      "   - Se puede integrar en prototipos rápidos o sistemas de producción.\n",
      "\n",
      "5. Integración con LangChain  \n",
      "   - Canales de extremo a extremo que conectan al LLM con bases de datos vectoriales, APIs, sistemas de recuperación de información y otros.\n",
      "\n",
      "6. Casos de uso típicos  \n",
      "   - Estudios de mercado de varios pasos.  \n",
      "   - Análisis de documentos legales.  \n",
      "   - Desarrollo de productos.  \n",
      "   - Asistentes de codificación colaborativos.\n",
      "\n",
      "En conjunto, los agentes de CrewAI permiten descomponer tareas complejas en subprocesos especializados, intercambiar información de manera ordenada y alcanzar objetivos globales con mayor eficiencia y transparencia.\n"
     ]
    }
   ],
   "source": [
    "# Paso 9: Ejecutar una segunda consulta sobre agentes de CrewAI\n",
    "\n",
    "# Definir una consulta más corta y menos específica\n",
    "# Esta consulta se beneficiará especialmente de la expansión\n",
    "query = {\"input\": \"¿Agentes CrewAI?\"}\n",
    "\n",
    "# Mostrar la consulta expandida para ver el enriquecimiento\n",
    "print(\"🔍 Consulta Expandida:\")\n",
    "print(query_expansion_chain.invoke({\"query\": query[\"input\"]}))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Invocar el pipeline RAG completo\n",
    "# Observa cómo una consulta muy corta (\"¿Agentes CrewAI?\") se convierte en\n",
    "# una búsqueda enriquecida con contexto técnico y sinónimos\n",
    "response = rag_pipeline.invoke(query)\n",
    "\n",
    "# Mostrar la respuesta generada\n",
    "print(\"✅ Respuesta:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd03d32d",
   "metadata": {},
   "source": [
    "### 📊 Resumen: Beneficios de la Expansión de Consultas\n",
    "\n",
    "**Antes de la expansión:**\n",
    "- Consulta: \"Langchain memory\"\n",
    "- Problema: Muy específica, puede perder documentos relevantes que usen terminología diferente\n",
    "\n",
    "**Después de la expansión:**\n",
    "- Consulta expandida incluye: \"ConversationBufferMemory\", \"ConversationSummaryMemory\", \"memory management\", \"session context\", etc.\n",
    "- Resultado: Mejor cobertura de documentos relevantes\n",
    "\n",
    "**Casos de uso ideales:**\n",
    "- ✅ Consultas cortas o vagas\n",
    "- ✅ Búsquedas técnicas que requieren terminología específica\n",
    "- ✅ Cuando el usuario no conoce los términos exactos\n",
    "- ✅ Mejorar recall (recuperación) sin sacrificar precisión\n",
    "\n",
    "**Consideraciones:**\n",
    "- ⚠️ Agrega latencia (~500ms-1s por la llamada al LLM)\n",
    "- ⚠️ Puede sobre-expandir consultas ya muy específicas\n",
    "- ⚠️ Requiere una API key y tiene costo asociado"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
