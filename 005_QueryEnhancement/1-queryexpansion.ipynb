{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f25489ee",
   "metadata": {},
   "source": [
    "### Mejora de Consultas â€“ TÃ©cnicas de ExpansiÃ³n de Consultas\n",
    "\n",
    "En un pipeline RAG, la calidad de la consulta enviada al recuperador determina quÃ© tan bueno es el contexto recuperado â€” y por lo tanto, quÃ© tan precisa serÃ¡ la respuesta final del LLM.\n",
    "\n",
    "AhÃ­ es donde entra la **ExpansiÃ³n / Mejora de Consultas**.\n",
    "\n",
    "#### ğŸ¯ Â¿QuÃ© es la Mejora de Consultas?\n",
    "La mejora de consultas se refiere a tÃ©cnicas utilizadas para mejorar o reformular la consulta del usuario para recuperar documentos mejores y mÃ¡s relevantes de la base de conocimiento.\n",
    "\n",
    "Es especialmente Ãºtil cuando:\n",
    "\n",
    "- La consulta original es corta, ambigua o estÃ¡ poco especificada\n",
    "- Quieres ampliar el alcance para capturar sinÃ³nimos, frases relacionadas o variantes ortogrÃ¡ficas\n",
    "- Necesitas agregar contexto tÃ©cnico o terminologÃ­a especÃ­fica del dominio\n",
    "\n",
    "**Ventajas de la ExpansiÃ³n de Consultas:**\n",
    "- âœ… Mejora la recuperaciÃ³n de documentos relevantes\n",
    "- âœ… Captura variaciones semÃ¡nticas de la consulta original\n",
    "- âœ… Reduce falsos negativos (documentos relevantes no encontrados)\n",
    "- âœ… Enriquece consultas vagas con contexto adicional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ff6ab72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Udemy\\RAGBootcamp\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ImportaciÃ³n de librerÃ­as necesarias para el pipeline RAG con expansiÃ³n de consultas\n",
    "\n",
    "# TextLoader: Para cargar archivos de texto plano\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# RecursiveCharacterTextSplitter: Para dividir texto en fragmentos (chunks) manejables\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# HuggingFaceEmbeddings: Para generar embeddings vectoriales usando modelos de HuggingFace\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# FAISS: Biblioteca de Facebook para bÃºsqueda de similitud vectorial eficiente\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# init_chat_model: Para inicializar modelos de chat de diferentes proveedores\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# PromptTemplate: Para crear plantillas de prompts con variables dinÃ¡micas\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# create_stuff_documents_chain: Crea una cadena que inserta documentos en un prompt\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# create_retrieval_chain: Combina recuperaciÃ³n de documentos con generaciÃ³n de respuestas\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "# StrOutputParser: Para convertir la salida del LLM a string\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# RunnableMap: Permite ejecutar mÃºltiples operaciones en paralelo y combinar resultados\n",
    "from langchain_core.runnables import RunnableMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "410d3b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Cargar y dividir el dataset\n",
    "\n",
    "# Cargar el archivo de texto que contiene informaciÃ³n sobre LangChain y CrewAI\n",
    "loader = TextLoader(\"langchain_crewai_dataset.txt\")\n",
    "\n",
    "# load() devuelve una lista de documentos con todo el contenido del archivo\n",
    "raw_docs = loader.load()\n",
    "\n",
    "# Crear un splitter para dividir el texto en fragmentos mÃ¡s pequeÃ±os\n",
    "# chunk_size=300: Cada fragmento tendrÃ¡ mÃ¡ximo 300 caracteres\n",
    "# chunk_overlap=50: HabrÃ¡ una superposiciÃ³n de 50 caracteres entre fragmentos consecutivos\n",
    "# La superposiciÃ³n ayuda a mantener el contexto entre fragmentos\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "\n",
    "# Dividir los documentos crudos en fragmentos (chunks) manejables\n",
    "chunks = splitter.split_documents(raw_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "791ba3d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='LangChain es un framework de cÃƒÂ³digo abierto diseÃƒÂ±ado para desarrollar aplicaciones basadas en grandes modelos de lenguaje (LLM). Simplifica el proceso de creaciÃƒÂ³n, gestiÃƒÂ³n y escalado de cadenas de pensamiento complejas al abstraer la gestiÃƒÂ³n de indicaciones, la recuperaciÃƒÂ³n, la memoria y la'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='indicaciones, la recuperaciÃƒÂ³n, la memoria y la orquestaciÃƒÂ³n de agentes. Los desarrolladores pueden usar LangChain para crear canales de extremo a extremo que conectan los LLM con herramientas, API, bases de datos vectoriales y otras fuentes de conocimiento. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='La base de LangChain es el concepto de cadenas, que son secuencias de llamadas a LLM y otras herramientas. Las cadenas pueden ser simples, como una sola indicaciÃƒÂ³n enviada a un LLM, o complejas, con mÃƒÂºltiples pasos ejecutados condicionalmente. LangChain facilita la composiciÃƒÂ³n y reutilizaciÃƒÂ³n'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='facilita la composiciÃƒÂ³n y reutilizaciÃƒÂ³n de cadenas utilizando patrones estÃƒÂ¡ndar como Stuff, Map-Reduce y Refine. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='LangChain se integra a la perfecciÃƒÂ³n con bases de datos vectoriales como FAISS, Chroma, Pinecone y Weaviate, lo que permite la bÃƒÂºsqueda semÃƒÂ¡ntica en grandes corpus de documentos. Esta capacidad es especialmente importante en la GeneraciÃƒÂ³n Aumentada por RecuperaciÃƒÂ³n (RAG), donde se obtiene'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='por RecuperaciÃƒÂ³n (RAG), donde se obtiene conocimiento externo y se inyecta en el prompt LLM para mejorar la precisiÃƒÂ³n y reducir la alucinaciÃƒÂ³n. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='LangChain tambiÃƒÂ©n admite la recuperaciÃƒÂ³n hÃƒ\\xadbrida, que combina mÃƒÂ©todos de recuperaciÃƒÂ³n basados Ã¢â‚¬â€¹Ã¢â‚¬â€¹en palabras clave (dispersos), como BM25, con la recuperaciÃƒÂ³n basada en incrustaciÃƒÂ³n (densos). Esto garantiza una mejor recuperaciÃƒÂ³n al detectar tanto coincidencias exactas de tÃƒÂ©rminos como'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='tanto coincidencias exactas de tÃƒÂ©rminos como contenido semÃƒÂ¡nticamente similar. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='Una de las caracterÃƒ\\xadsticas destacadas de LangChain es su compatibilidad con agentes. Los agentes utilizan LLM para razonar sobre quÃƒÂ© herramienta llamar, quÃƒÂ© entrada proporcionar y cÃƒÂ³mo procesar la salida. Los agentes de LangChain pueden ejecutar tareas de varios pasos, integrÃƒÂ¡ndose con'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='tareas de varios pasos, integrÃƒÂ¡ndose con herramientas como bÃƒÂºsquedas web, calculadoras, entornos de ejecuciÃƒÂ³n de cÃƒÂ³digo y API personalizadas. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='Los agentes de LangChain operan mediante un modelo planificador-ejecutor, donde el agente planifica una secuencia de invocaciones de herramientas para lograr un objetivo. Esto puede incluir la toma de decisiones dinÃƒÂ¡mica, la lÃƒÂ³gica de ramificaciÃƒÂ³n y el uso de memoria contextual en todos los'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='y el uso de memoria contextual en todos los pasos. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='LangChain ofrece mÃƒÂ³dulos de memoria como ConversationBufferMemory y ConversationSummaryMemory. Estos permiten que el LLM mantenga la informaciÃƒÂ³n de los turnos de conversaciÃƒÂ³n anteriores o resuma interacciones largas para ajustarse a los lÃƒ\\xadmites de tokens. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='La ingenierÃƒ\\xada de indicaciones es fundamental para el diseÃƒÂ±o de LangChain. Proporciona funciones de creaciÃƒÂ³n de plantillas, variables de entrada, opciones de formato y encadenamiento de indicaciones. Los desarrolladores pueden reutilizar las plantillas de indicaciones en diferentes cadenas e'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='de indicaciones en diferentes cadenas e incluso anidarlas. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='LangChain es compatible con mÃƒÂºltiples proveedores de LLM, como OpenAI, Anthropic, Cohere, Hugging Face y otros. Esta flexibilidad garantiza que los desarrolladores puedan cambiar entre modelos sin reescribir la lÃƒÂ³gica central. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='Los flujos de trabajo de LangChain son modulares y componibles. Componentes como recuperadores, memorias, agentes y cadenas se pueden combinar y reutilizar fÃƒÂ¡cilmente. Esto lo hace ideal para crear aplicaciones LLM escalables y fÃƒÂ¡ciles de mantener. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='CrewAI es un marco de orquestaciÃƒÂ³n multiagente diseÃƒÂ±ado para crear agentes colaborativos con tecnologÃƒ\\xada LLM. Permite a los desarrolladores estructurar agentes en equipos organizados que trabajan juntos para completar tareas dividiendo responsabilidades, compartiendo contexto y comunicÃƒÂ¡ndose'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='compartiendo contexto y comunicÃƒÂ¡ndose dinÃƒÂ¡micamente entre sÃƒ\\xad. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='CrewAI se basa en el concepto de agentes autÃƒÂ³nomos, pero lo mejora al permitirles formar flujos de trabajo estructurados. Cada agente de un equipo tiene un rol definido, como investigador, planificador o ejecutor, y opera de forma semiindependiente en un contexto colaborativo. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='Los agentes de CrewAI se definen con un propÃƒÂ³sito, un objetivo y un conjunto de herramientas que pueden utilizar. El marco garantiza que cada agente se mantenga concentrado en su tarea y contribuya significativamente al objetivo general del equipo. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='Una de las principales innovaciones de CrewAI es el uso del intercambio de contexto entre agentes, donde los agentes se intercambian datos intermedios de forma estructurada. Esto genera comportamientos emergentes como la delegaciÃƒÂ³n, la consulta y la revisiÃƒÂ³n entre agentes. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='CrewAI es especialmente ÃƒÂºtil en flujos de trabajo de varios pasos, como estudios de mercado, anÃƒÂ¡lisis de documentos legales, desarrollo de productos y asistentes de codificaciÃƒÂ³n, donde las tareas complejas se benefician de la especializaciÃƒÂ³n y la colaboraciÃƒÂ³n. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='El framework admite la trazabilidad completa de las decisiones e interacciones de los agentes, lo que facilita la depuraciÃƒÂ³n y la transparencia en comparaciÃƒÂ³n con las configuraciones de agentes independientes. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='CrewAI es compatible con los agentes y herramientas de LangChain, lo que permite sistemas hÃƒ\\xadbridos donde LangChain gestiona la recuperaciÃƒÂ³n y el encapsulado de herramientas, mientras que CrewAI gestiona la colaboraciÃƒÂ³n basada en roles. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='Los desarrolladores pueden definir un equipo mediante una configuraciÃƒÂ³n similar a YAML o JSON, especificando los roles, objetivos, memoria y herramientas de los agentes. CrewAI orquesta el bucle de agentes y gestiona los turnos y la toma de decisiones de forma autÃƒÂ³noma. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='CrewAI admite mÃƒÂºltiples backends LLM e incluye compatibilidad con streaming, ejecuciÃƒÂ³n paralela e invocaciÃƒÂ³n asÃƒ\\xadncrona de herramientas, lo que lo hace adecuado tanto para sistemas de prototipado rÃƒÂ¡pido como para sistemas listos para producciÃƒÂ³n. (v1)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='Al permitir la colaboraciÃƒÂ³n estructurada entre agentes, CrewAI permite a los equipos construir sistemas inteligentes que escalan tanto horizontalmente (mÃƒÂ¡s agentes) como verticalmente (Mayor profundidad de razonamiento). (v9)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='LangChain es un framework de cÃƒÂ³digo abierto diseÃƒÂ±ado para desarrollar aplicaciones basadas en grandes modelos de lenguaje (LLM). Simplifica el proceso de creaciÃƒÂ³n, gestiÃƒÂ³n y escalado de cadenas de pensamiento complejas al abstraer la gestiÃƒÂ³n de indicaciones, la recuperaciÃƒÂ³n, la memoria y la'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='indicaciones, la recuperaciÃƒÂ³n, la memoria y la orquestaciÃƒÂ³n de agentes. Los desarrolladores pueden usar LangChain para crear canales de extremo a extremo que conectan los LLM con herramientas, API, bases de datos vectoriales y otras fuentes de conocimiento. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='En el corazÃƒÂ³n de LangChain se encuentra el concepto de cadenas, que son secuencias de llamadas a los LLM y otras herramientas. Las cadenas pueden ser simples, como una sola indicaciÃƒÂ³n enviada a un LLM, o complejas, con mÃƒÂºltiples pasos ejecutados condicionalmente. LangChain facilita la'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='condicionalmente. LangChain facilita la composiciÃƒÂ³n y reutilizaciÃƒÂ³n de cadenas utilizando patrones estÃƒÂ¡ndar como Stuff, Map-Reduce y Refine. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='LangChain se integra perfectamente con bases de datos vectoriales como FAISS, Chroma, Pinecone y Weaviate, lo que permite la bÃƒÂºsqueda semÃƒÂ¡ntica en grandes corpus de documentos. Esta capacidad es especialmente importante en la GeneraciÃƒÂ³n Aumentada por RecuperaciÃƒÂ³n (RAG), donde se obtiene'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='por RecuperaciÃƒÂ³n (RAG), donde se obtiene conocimiento externo y se inyecta en el prompt LLM para mejorar la precisiÃƒÂ³n y reducir la alucinaciÃƒÂ³n. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='LangChain tambiÃƒÂ©n admite la recuperaciÃƒÂ³n hÃƒ\\xadbrida, que combina mÃƒÂ©todos de recuperaciÃƒÂ³n basados Ã¢â‚¬â€¹Ã¢â‚¬â€¹en palabras clave (dispersos), como BM25, con la recuperaciÃƒÂ³n basada en incrustaciÃƒÂ³n (densos). Esto garantiza una mejor recuperaciÃƒÂ³n al detectar tanto coincidencias exactas de tÃƒÂ©rminos como'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='tanto coincidencias exactas de tÃƒÂ©rminos como contenido semÃƒÂ¡nticamente similar. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='Una de las caracterÃƒ\\xadsticas destacadas de LangChain es su compatibilidad con agentes. Los agentes utilizan LLM para razonar sobre quÃƒÂ© herramienta llamar, quÃƒÂ© entrada proporcionar y cÃƒÂ³mo procesar la salida. Los agentes de LangChain pueden ejecutar tareas de varios pasos, integrÃƒÂ¡ndose con'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='tareas de varios pasos, integrÃƒÂ¡ndose con herramientas como bÃƒÂºsquedas web, calculadoras, entornos de ejecuciÃƒÂ³n de cÃƒÂ³digo y API personalizadas. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='Los agentes de LangChain operan mediante un modelo planificador-ejecutor, donde el agente planifica una secuencia de invocaciones de herramientas para lograr un objetivo. Esto puede incluir la toma de decisiones dinÃƒÂ¡mica, la lÃƒÂ³gica de ramificaciÃƒÂ³n y el uso de memoria contextual en todos los'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='y el uso de memoria contextual en todos los pasos. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='LangChain ofrece mÃƒÂ³dulos de memoria como ConversationBufferMemory y ConversationSummaryMemory. Estos permiten que el LLM mantenga la informaciÃƒÂ³n de los turnos de conversaciÃƒÂ³n anteriores o resuma interacciones largas para ajustarse a los lÃƒ\\xadmites de tokens. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='La ingenierÃƒ\\xada de indicaciones es fundamental para el diseÃƒÂ±o de LangChain. Proporciona funciones de creaciÃƒÂ³n de plantillas, variables de entrada, opciones de formato y encadenamiento de indicaciones. Los desarrolladores pueden reutilizar las plantillas de indicaciones en diferentes cadenas e'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='de indicaciones en diferentes cadenas e incluso anidarlas. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='LangChain es compatible con mÃƒÂºltiples proveedores de LLM, como OpenAI, Anthropic, Cohere, Hugging Face y otros. Esta flexibilidad garantiza que los desarrolladores puedan cambiar entre modelos sin reescribir la lÃƒÂ³gica central. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='Los flujos de trabajo de LangChain son modulares y componibles. Componentes como recuperadores, memorias, agentes y cadenas se pueden combinar y reutilizar fÃƒÂ¡cilmente. Esto lo hace ideal para crear aplicaciones LLM escalables y fÃƒÂ¡ciles de mantener. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='CrewAI es un marco de orquestaciÃƒÂ³n multiagente diseÃƒÂ±ado para crear agentes colaborativos con tecnologÃƒ\\xada LLM. Permite a los desarrolladores estructurar agentes en equipos organizados que trabajan juntos para completar tareas dividiendo responsabilidades, compartiendo contexto y comunicÃƒÂ¡ndose'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='compartiendo contexto y comunicÃƒÂ¡ndose dinÃƒÂ¡micamente entre sÃƒ\\xad. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='CrewAI se basa en el concepto de agentes autÃƒÂ³nomos, pero lo mejora al permitirles formar flujos de trabajo estructurados. Cada agente de un equipo tiene un rol definido, como investigador, planificador o ejecutor, y opera de forma semiindependiente en un contexto colaborativo. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='Los agentes de CrewAI se definen con un propÃƒÂ³sito, un objetivo y un conjunto de herramientas que pueden utilizar. El marco garantiza que cada agente se mantenga concentrado en su tarea y contribuya significativamente al objetivo general del equipo. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='Una de las principales innovaciones de CrewAI es el uso del intercambio de contexto entre agentes, donde los agentes se intercambian datos intermedios de forma estructurada. Esto genera comportamientos emergentes como la delegaciÃƒÂ³n, la consulta y la revisiÃƒÂ³n entre agentes. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='CrewAI es especialmente ÃƒÂºtil en flujos de trabajo de varios pasos, como estudios de mercado, anÃƒÂ¡lisis de documentos legales, desarrollo de productos y asistentes de codificaciÃƒÂ³n, donde las tareas complejas se benefician de la especializaciÃƒÂ³n y la colaboraciÃƒÂ³n. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='El framework admite la trazabilidad completa de las decisiones e interacciones de los agentes, lo que facilita la depuraciÃƒÂ³n y la transparencia en comparaciÃƒÂ³n con las configuraciones de agentes independientes. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='CrewAI es compatible con los agentes y herramientas de LangChain, lo que permite sistemas hÃƒ\\xadbridos donde LangChain gestiona la recuperaciÃƒÂ³n y el encapsulado de herramientas, mientras que CrewAI gestiona la colaboraciÃƒÂ³n basada en roles. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='Los desarrolladores pueden definir un equipo mediante una configuraciÃƒÂ³n similar a YAML o JSON, especificando los roles, objetivos, memoria y herramientas de los agentes. CrewAI orquesta el bucle de agentes y gestiona los turnos y la toma de decisiones de forma autÃƒÂ³noma. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='CrewAI admite mÃƒÂºltiples backends LLM e incluye compatibilidad con streaming, ejecuciÃƒÂ³n paralela e invocaciÃƒÂ³n asÃƒ\\xadncrona de herramientas, lo que lo hace adecuado tanto para prototipado rÃƒÂ¡pido como para sistemas listos para producciÃƒÂ³n. (v10)'),\n",
       " Document(metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='Al permitir la colaboraciÃƒÂ³n estructurada entre agentes, CrewAI permite a los equipos construir sistemas inteligentes que escalan tanto')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostrar los fragmentos generados\n",
    "# Esto nos permite ver cÃ³mo se dividiÃ³ el documento\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f001ff8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000021169E9F440>, search_type='mmr', search_kwargs={'k': 5})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paso 2: Crear el almacÃ©n vectorial (Vector Store)\n",
    "\n",
    "# Inicializar el modelo de embeddings de HuggingFace\n",
    "# all-MiniLM-L6-v2 es un modelo compacto y eficiente que genera vectores de 384 dimensiones\n",
    "# Este modelo es ideal para bÃºsqueda semÃ¡ntica en espaÃ±ol e inglÃ©s\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Crear el almacÃ©n vectorial FAISS a partir de los fragmentos de documentos\n",
    "# FAISS convierte cada fragmento en un vector numÃ©rico para bÃºsqueda de similitud eficiente\n",
    "vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
    "\n",
    "# Paso 3: Crear el recuperador con MMR (Maximal Marginal Relevance)\n",
    "# MMR balancea relevancia y diversidad para evitar documentos redundantes\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",  # Usar MMR en lugar de similitud simple\n",
    "    search_kwargs={\"k\": 5}  # Recuperar los 5 documentos mÃ¡s relevantes y diversos\n",
    ")\n",
    "\n",
    "# Mostrar la configuraciÃ³n del recuperador\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14e8f2fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000002116CF1E900>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002116D2361B0>, root_client=<openai.OpenAI object at 0x00000211676BA000>, root_async_client=<openai.AsyncOpenAI object at 0x000002116BCBAA50>, model_name='o4-mini', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paso 4: Configurar el LLM (Large Language Model) y variables de entorno\n",
    "\n",
    "# Importar mÃ³dulos para manejo de variables de entorno\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Cargar las variables de entorno desde el archivo .env\n",
    "# Este archivo contiene las API keys de forma segura\n",
    "load_dotenv()\n",
    "\n",
    "# Establecer la API key de OpenAI desde las variables de entorno\n",
    "# Esta clave es necesaria para usar los modelos de OpenAI\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Inicializar el LLM usando el modelo o4-mini de OpenAI\n",
    "# Este modelo es eficiente y econÃ³mico para tareas de expansiÃ³n de consultas\n",
    "llm = init_chat_model(\"openai:o4-mini\")\n",
    "\n",
    "# Mostrar la configuraciÃ³n del LLM\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "116e2cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='\\nEres un asistente Ãºtil. Expande la siguiente consulta para mejorar la recuperaciÃ³n de documentos agregando sinÃ³nimos relevantes, tÃ©rminos tÃ©cnicos y contexto Ãºtil.\\n\\nConsulta original: \"{query}\"\\n\\nConsulta expandida:\\n')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000002116CF1E900>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002116D2361B0>, root_client=<openai.OpenAI object at 0x00000211676BA000>, root_async_client=<openai.AsyncOpenAI object at 0x000002116BCBAA50>, model_name='o4-mini', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paso 5: Crear la cadena de expansiÃ³n de consultas\n",
    "\n",
    "# Esta plantilla de prompt instruye al LLM para expandir la consulta del usuario\n",
    "# El objetivo es agregar sinÃ³nimos, tÃ©rminos tÃ©cnicos y contexto Ãºtil\n",
    "query_expansion_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Eres un asistente Ãºtil. Expande la siguiente consulta para mejorar la recuperaciÃ³n de documentos agregando sinÃ³nimos relevantes, tÃ©rminos tÃ©cnicos y contexto Ãºtil.\n",
    "\n",
    "Consulta original: \"{query}\"\n",
    "\n",
    "Consulta expandida:\n",
    "\"\"\")\n",
    "\n",
    "# Crear la cadena de expansiÃ³n de consultas\n",
    "# Esta cadena conecta: prompt â†’ LLM â†’ parser de salida\n",
    "# El operador | (pipe) encadena los componentes secuencialmente\n",
    "query_expansion_chain = query_expansion_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Mostrar la estructura de la cadena de expansiÃ³n\n",
    "query_expansion_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d629dd9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AquÃ­ tienes una posible versiÃ³n de la consulta enriquecida, que incorpora sinÃ³nimos, tÃ©rminos tÃ©cnicos y contexto para abarcar diversos aspectos de â€œLangChain memoryâ€:\\n\\n(\"LangChain memory\"  \\n OR \"LangChain memory management\"  \\n OR \"LangChain memory module\"  \\n OR \"LangChain converse memory\"  \\n OR \"LangChain session memory\"  \\n OR \"LangChain persistent memory\"  \\n OR \"LangChain state management\"  \\n OR \"LangChain context storage\"  \\n OR \"LangChain cache\"  \\n OR \"LangChain long-term memory\"  \\n OR \"LangChain short-term memory\"  \\n OR \"LangChain external memory\"  \\n OR \"LangChain vector store memory\"  \\n OR \"LangChain retrieval-augmented memory\"  \\n OR \"LangChain RAG memory\"  \\n OR \"LangChain embedding cache\"  \\n OR \"stateful chatbots\"  \\n OR \"conversational AI memory\"  \\n OR \"context window management\"  \\n OR \"contextual retrieval\"  \\n OR \"memory classes in LangChain\"  \\n OR \"gestiÃ³n de memoria LangChain\"  \\n OR \"mÃ³dulo de memoria LangChain\"  \\n OR \"almacenamiento de contexto\"  \\n OR \"memoria conversacional\"  \\n OR \"almacÃ©n de vectores\")  \\nAND  \\n(\"tutorial\" OR \"guide\" OR \"API reference\" OR \"best practices\" OR \"use cases\" OR \"benchmark\" OR \"performance\" OR \"Python\" OR \"JavaScript\" OR \"Node.js\")  \\n\\nExplicaciÃ³n de las secciones:  \\n- SinÃ³nimos y variantes sintÃ¡cticas de â€œLangChain memoryâ€  \\n- TÃ©rminos tÃ©cnicos (session vs. persistent vs. long-term vs. short-term memory)  \\n- Conceptos relacionados (state management, RAG, embeddings, vector store, cache)  \\n- Contexto de aplicaciÃ³n (chatbots, stateful AI, documentaciÃ³n, performance, lenguajes de implementaciÃ³n)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Probar la cadena de expansiÃ³n de consultas\n",
    "# Enviamos una consulta simple sobre \"Langchain memory\" para ver cÃ³mo la expande el LLM\n",
    "# El LLM agregarÃ¡ tÃ©rminos tÃ©cnicos, sinÃ³nimos y variaciones Ãºtiles\n",
    "query_expansion_chain.invoke({\"query\": \"Langchain memory\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec6e3ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 6: Crear el prompt para generar respuestas\n",
    "\n",
    "# Esta plantilla define cÃ³mo el LLM usarÃ¡ el contexto recuperado para responder\n",
    "answer_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Responde la pregunta basÃ¡ndote en el contexto a continuaciÃ³n.\n",
    "\n",
    "Contexto:\n",
    "{context}\n",
    "\n",
    "Pregunta: {input}\n",
    "\"\"\")\n",
    "\n",
    "# Crear la cadena de documentos que combina el LLM con el prompt de respuesta\n",
    "# Esta cadena toma documentos recuperados y los inserta en el prompt\n",
    "# \"stuff\" significa que todos los documentos se insertan directamente en el prompt\n",
    "document_chain = create_stuff_documents_chain(llm=llm, prompt=answer_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c57e726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 7: Construir el pipeline RAG completo con expansiÃ³n de consultas\n",
    "\n",
    "# Este pipeline integra la expansiÃ³n de consultas en el flujo RAG:\n",
    "# 1. Recibe la consulta original del usuario\n",
    "# 2. Expande la consulta usando el LLM (agregando sinÃ³nimos y contexto)\n",
    "# 3. Usa la consulta expandida para recuperar documentos relevantes\n",
    "# 4. Genera la respuesta final usando los documentos recuperados\n",
    "\n",
    "rag_pipeline = (\n",
    "    # RunnableMap ejecuta mÃºltiples operaciones en paralelo\n",
    "    RunnableMap({\n",
    "        # Mantener la consulta original para la respuesta final\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \n",
    "        # Flujo de recuperaciÃ³n con expansiÃ³n de consultas:\n",
    "        # 1. Toma la consulta original (x[\"input\"])\n",
    "        # 2. La expande usando query_expansion_chain\n",
    "        # 3. Usa la consulta expandida para invocar el retriever\n",
    "        # 4. Devuelve los documentos recuperados como contexto\n",
    "        \"context\": lambda x: retriever.invoke(\n",
    "            query_expansion_chain.invoke({\"query\": x[\"input\"]})\n",
    "        )\n",
    "    })\n",
    "    # El resultado del RunnableMap se pasa a document_chain para generar la respuesta\n",
    "    | document_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddebe80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Consulta Expandida:\n",
      "Consulta ampliada:\n",
      "\n",
      "â€œÂ¿CuÃ¡les son los distintos mecanismos y objetos de â€˜Memoryâ€™ que soporta el framework LangChain para gestionar el contexto y el estado de las conversaciones con modelos de lenguaje? En concreto, me interesa conocer:\n",
      "\n",
      "1. Tipos de memoria integrados en LangChain  \n",
      "   - ConversationBufferMemory (cachÃ© de diÃ¡logo completo)  \n",
      "   - ConversationSummaryMemory (resumen incremental)  \n",
      "   - ConversationEntityMemory (extracciÃ³n y seguimiento de entidades)  \n",
      "   - VectorStoreRetrieverMemory o VectorStoreMemory (memoria basada en vectores)  \n",
      "   - CombinedMemory (combinaciÃ³n de varios back-ends)  \n",
      "   - LongTermMemory (memoria a largo plazo)\n",
      "\n",
      "2. SinÃ³nimos y tÃ©rminos relacionados  \n",
      "   - State management, session state, chat history  \n",
      "   - Context persistence, almacenamiento de contexto, retenciÃ³n de estado  \n",
      "   - Embedding store, vector database, base de datos de vectores\n",
      "\n",
      "3. Opciones de persistencia y back-ends compatibles  \n",
      "   - Redis, FAISS, ChromaDB, Milvus  \n",
      "   - SQLite, PostgreSQL, MongoDB (para memoria en disco)  \n",
      "   - Pinecone, Weaviate (servicios gestionados)\n",
      "\n",
      "4. Casos de uso y diferencias tÃ©cnicas  \n",
      "   - Â¿CuÃ¡ndo conviene usar buffer vs. summary vs. entity memory?  \n",
      "   - Ventajas de la indexaciÃ³n vectorial para recuperaciÃ³n de contexto  \n",
      "   - Impacto en latencia y consumo de tokens\n",
      "\n",
      "5. ParÃ¡metros de configuraciÃ³n clave  \n",
      "   - TamaÃ±o del buffer, estrategia de resumen (max_tokens, chunk_size)  \n",
      "   - ParÃ¡metros de embeddings (modelo, dimensionalidad, mÃ©tricas de similitud)  \n",
      "   - Persistencia (auto_save, persist_directory, connection_args)\n",
      "\n",
      "Con esta descripciÃ³n mÃ¡s completa, pretendo optimizar la bÃºsqueda de documentaciÃ³n y ejemplos de implementaciÃ³n de cada tipo de memoria en LangChain.â€\n",
      "\n",
      "================================================================================\n",
      "\n",
      "âœ… Respuesta:\n",
      "LangChain incluye, al menos, dos mÃ³dulos de memoria incorporados:\n",
      "\n",
      "1. ConversationBufferMemory  \n",
      "   â€¢ Guarda literalmente todos los turnos de conversaciÃ³n anteriores.  \n",
      "   â€¢ Ãštil cuando quieres disponer del historial completo para cada llamada al LLM.\n",
      "\n",
      "2. ConversationSummaryMemory  \n",
      "   â€¢ Resume las interacciones largas en un texto compacto.  \n",
      "   â€¢ Permite mantener el contexto esencial sin pasarse de lÃ­mites de tokens.\n"
     ]
    }
   ],
   "source": [
    "# Paso 8: Ejecutar una consulta de ejemplo sobre tipos de memoria en LangChain\n",
    "\n",
    "# Definir la consulta del usuario\n",
    "query = {\"input\": \"Â¿QuÃ© tipos de memoria soporta LangChain?\"}\n",
    "\n",
    "# Primero, mostrar cÃ³mo se expande la consulta\n",
    "# Esto nos permite ver quÃ© tÃ©rminos adicionales agregÃ³ el LLM\n",
    "print(\"ğŸ” Consulta Expandida:\")\n",
    "print(query_expansion_chain.invoke({\"query\": query[\"input\"]}))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Invocar el pipeline RAG completo con la consulta\n",
    "# El pipeline automÃ¡ticamente:\n",
    "# 1. ExpandirÃ¡ la consulta\n",
    "# 2. RecuperarÃ¡ documentos relevantes\n",
    "# 3. GenerarÃ¡ una respuesta basada en el contexto\n",
    "response = rag_pipeline.invoke(query)\n",
    "\n",
    "# Mostrar la respuesta final del sistema RAG\n",
    "print(\"âœ… Respuesta:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efd86621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Consulta Expandida:\n",
      "Consulta expandida:\n",
      "\n",
      "(â€œCrewAIâ€ OR â€œCrew AIâ€ OR â€œCrew-AIâ€ OR â€œplataforma CrewAIâ€ OR â€œframework CrewAIâ€)  \n",
      "AND  \n",
      "(â€œagente inteligenteâ€ OR â€œagente conversacionalâ€ OR â€œagente autÃ³nomoâ€ OR â€œbot de IAâ€ OR â€œagente de inteligencia artificialâ€)  \n",
      "AND  \n",
      "(â€œinteligencia artificialâ€ OR â€œIAâ€ OR â€œmachine learningâ€ OR â€œaprendizaje automÃ¡ticoâ€ OR â€œNLPâ€ OR â€œprocesamiento de lenguaje naturalâ€ OR â€œLLMâ€ OR â€œmodelos de lenguajeâ€ OR â€œtransformersâ€)  \n",
      "AND  \n",
      "(â€œdocumentaciÃ³n tÃ©cnicaâ€ OR â€œguÃ­a de usoâ€ OR â€œguÃ­a de implementaciÃ³nâ€ OR â€œSDKâ€ OR â€œAPIâ€ OR â€œlibrerÃ­aâ€ OR â€œversiÃ³nâ€ OR â€œrelease notesâ€)  \n",
      "AND  \n",
      "(â€œarquitecturaâ€ OR â€œorquestaciÃ³n de agentesâ€ OR â€œdiseÃ±o multiagenteâ€ OR â€œintegraciÃ³nâ€ OR â€œcasos de usoâ€ OR â€œpatrones de diseÃ±oâ€ OR â€œbenchmarkâ€ OR â€œrendimientoâ€ OR â€œescalabilidadâ€ OR â€œdeploymentâ€ OR â€œCI/CDâ€)  \n",
      "AND  \n",
      "(â€œautomatizaciÃ³n de tareasâ€ OR â€œasistentes virtualesâ€ OR â€œsistemas de diÃ¡logoâ€ OR â€œchatbotsâ€ OR â€œworkflow inteligenteâ€ OR â€œpipeline de IAâ€)  \n",
      "\n",
      "Esta consulta combina sinÃ³nimos, tÃ©rminos tÃ©cnicos y contexto (arquitectura, integraciÃ³n, casos de uso, rendimiento) para maximizar la cobertura en la recuperaciÃ³n de documentaciÃ³n sobre los agentes de CrewAI.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "âœ… Respuesta:\n",
      "En CrewAI, los â€œagentesâ€ son componentes autÃ³nomos y especializados que, en lugar de funcionar de manera aislada, se organizan en equipos para cubrir distintos roles dentro de un flujo de trabajo estructurado. Sus caracterÃ­sticas clave son:\n",
      "\n",
      "1. Roles definidos  \n",
      "   - Investigador, planificador, ejecutor, revisor, etc.  \n",
      "   - Cada agente se centra en un subobjetivo concreto.\n",
      "\n",
      "2. ColaboraciÃ³n y orquestaciÃ³n  \n",
      "   - Intercambio de contexto estructurado (datos intermedios).  \n",
      "   - DelegaciÃ³n, consulta mutua y revisiones cruzadas entre agentes.\n",
      "\n",
      "3. Paralelismo y asincronÃ­a  \n",
      "   - InvocaciÃ³n paralela de herramientas y APIs.  \n",
      "   - Soporte para llamadas asÃ­ncronas y streaming de resultados.\n",
      "\n",
      "4. Flexibilidad de backends  \n",
      "   - Compatibilidad con mÃºltiples LLM (GPT, PaLM, LLaMA, etc.).  \n",
      "   - Se puede integrar en prototipos rÃ¡pidos o sistemas de producciÃ³n.\n",
      "\n",
      "5. IntegraciÃ³n con LangChain  \n",
      "   - Canales de extremo a extremo que conectan al LLM con bases de datos vectoriales, APIs, sistemas de recuperaciÃ³n de informaciÃ³n y otros.\n",
      "\n",
      "6. Casos de uso tÃ­picos  \n",
      "   - Estudios de mercado de varios pasos.  \n",
      "   - AnÃ¡lisis de documentos legales.  \n",
      "   - Desarrollo de productos.  \n",
      "   - Asistentes de codificaciÃ³n colaborativos.\n",
      "\n",
      "En conjunto, los agentes de CrewAI permiten descomponer tareas complejas en subprocesos especializados, intercambiar informaciÃ³n de manera ordenada y alcanzar objetivos globales con mayor eficiencia y transparencia.\n"
     ]
    }
   ],
   "source": [
    "# Paso 9: Ejecutar una segunda consulta sobre agentes de CrewAI\n",
    "\n",
    "# Definir una consulta mÃ¡s corta y menos especÃ­fica\n",
    "# Esta consulta se beneficiarÃ¡ especialmente de la expansiÃ³n\n",
    "query = {\"input\": \"Â¿Agentes CrewAI?\"}\n",
    "\n",
    "# Mostrar la consulta expandida para ver el enriquecimiento\n",
    "print(\"ğŸ” Consulta Expandida:\")\n",
    "print(query_expansion_chain.invoke({\"query\": query[\"input\"]}))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Invocar el pipeline RAG completo\n",
    "# Observa cÃ³mo una consulta muy corta (\"Â¿Agentes CrewAI?\") se convierte en\n",
    "# una bÃºsqueda enriquecida con contexto tÃ©cnico y sinÃ³nimos\n",
    "response = rag_pipeline.invoke(query)\n",
    "\n",
    "# Mostrar la respuesta generada\n",
    "print(\"âœ… Respuesta:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd03d32d",
   "metadata": {},
   "source": [
    "### ğŸ“Š Resumen: Beneficios de la ExpansiÃ³n de Consultas\n",
    "\n",
    "**Antes de la expansiÃ³n:**\n",
    "- Consulta: \"Langchain memory\"\n",
    "- Problema: Muy especÃ­fica, puede perder documentos relevantes que usen terminologÃ­a diferente\n",
    "\n",
    "**DespuÃ©s de la expansiÃ³n:**\n",
    "- Consulta expandida incluye: \"ConversationBufferMemory\", \"ConversationSummaryMemory\", \"memory management\", \"session context\", etc.\n",
    "- Resultado: Mejor cobertura de documentos relevantes\n",
    "\n",
    "**Casos de uso ideales:**\n",
    "- âœ… Consultas cortas o vagas\n",
    "- âœ… BÃºsquedas tÃ©cnicas que requieren terminologÃ­a especÃ­fica\n",
    "- âœ… Cuando el usuario no conoce los tÃ©rminos exactos\n",
    "- âœ… Mejorar recall (recuperaciÃ³n) sin sacrificar precisiÃ³n\n",
    "\n",
    "**Consideraciones:**\n",
    "- âš ï¸ Agrega latencia (~500ms-1s por la llamada al LLM)\n",
    "- âš ï¸ Puede sobre-expandir consultas ya muy especÃ­ficas\n",
    "- âš ï¸ Requiere una API key y tiene costo asociado"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
