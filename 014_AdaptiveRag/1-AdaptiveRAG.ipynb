{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "976ed62c",
   "metadata": {},
   "source": "### RAG Adaptativo (Adaptive RAG)\n\n**¿Qué es Adaptive RAG?**\n\nAdaptive RAG es una técnica avanzada que combina lo mejor de múltiples patrones RAG mediante **enrutamiento inteligente** y **auto-corrección**. El sistema toma decisiones adaptativas en cada paso:\n\n1. **Enrutamiento inicial**: Decide si buscar en vectorstore local o en la web\n2. **Evaluación de relevancia**: Califica documentos recuperados\n3. **Detección de alucinaciones**: Verifica que la respuesta esté fundamentada en los documentos\n4. **Validación de respuesta**: Confirma que la respuesta realmente conteste la pregunta\n5. **Auto-corrección**: Si algo falla, reescribe la consulta y reintenta\n\nEste es el enfoque más completo y robusto, ideal para aplicaciones de producción donde la calidad es crítica."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b759b62",
   "metadata": {},
   "outputs": [],
   "source": "# Importar librería para manejo de variables de entorno\nimport os\n# Importar función para cargar variables desde archivo .env\nfrom dotenv import load_dotenv\n\n# Cargar todas las variables de entorno desde el archivo .env\n# Esto permite mantener las claves API de forma segura fuera del código\nload_dotenv()\n\n# Configurar la clave API de OpenAI (para embeddings, LLM y evaluadores)\n# Necesaria para usar GPT-4o-mini y text-embedding-ada-002\nos.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n\n# Configurar la clave API de Tavily (motor de búsqueda web para IA)\n# Se usa cuando el router decide que la consulta necesita búsqueda web\nos.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5847e35f",
   "metadata": {},
   "outputs": [],
   "source": "### Construir Índice Vectorial\n\n# Importar divisor de texto que respeta los tokens del modelo\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n# Importar cargador de documentos desde páginas web\nfrom langchain_community.document_loaders import WebBaseLoader\n# Importar FAISS, base de datos vectorial en memoria (rápida y eficiente)\nfrom langchain_community.vectorstores import FAISS\n# Importar embeddings de OpenAI para convertir texto en vectores\nfrom langchain_openai import OpenAIEmbeddings\n\n### from langchain_cohere import CohereEmbeddings  # Alternativa: embeddings de Cohere\n\n# Configurar el modelo de embeddings\n# OpenAIEmbeddings() usa por defecto text-embedding-ada-002\nembd = OpenAIEmbeddings()\n\n# Definir las URLs de los documentos a indexar\n# Artículos del blog de Lilian Weng sobre IA, agentes y LLMs\nurls = [\n    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",           # Agentes de IA\n    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",  # Ingeniería de prompts\n    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",      # Ataques adversarios a LLMs\n]\n\n# Cargar los documentos desde las URLs\n# WebBaseLoader carga el contenido HTML de cada URL\ndocs = [WebBaseLoader(url).load() for url in urls]\n\n# Aplanar la lista de listas en una sola lista de documentos\n# Convierte [[doc1], [doc2], [doc3]] en [doc1, doc2, doc3]\ndocs_list = [item for sublist in docs for item in sublist]\n\n# Crear un divisor de texto basado en tokens (tiktoken de OpenAI)\n# chunk_size=500: cada fragmento tendrá máximo 500 tokens\n# chunk_overlap=50: habrá un solapamiento de 50 tokens entre fragmentos consecutivos\n# El overlap ayuda a mantener contexto entre chunks y mejora la recuperación\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=500, chunk_overlap=50\n)\n\n# Dividir los documentos en fragmentos más pequeños\n# Fragmentos pequeños = recuperación más precisa\ndoc_splits = text_splitter.split_documents(docs_list)\n\n# Crear el vectorstore (base de datos vectorial) con FAISS\n# from_documents() convierte cada fragmento en vector y los almacena\nvectorstore = FAISS.from_documents(\n    documents=doc_splits,\n    embedding=OpenAIEmbeddings()\n)\n\n# Crear un retriever (recuperador) desde el vectorstore\n# Interfaz que permite buscar documentos similares a una consulta\n# Por defecto usa similitud coseno y retorna k=4 documentos\nretriever = vectorstore.as_retriever()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41b449a",
   "metadata": {},
   "outputs": [],
   "source": "### Enrutador (Router)\n\n# Importar Literal para definir tipos con valores específicos\nfrom typing import Literal\n# Importar componentes para crear prompts estructurados\nfrom langchain_core.prompts import ChatPromptTemplate\n# Importar el modelo de chat de OpenAI\nfrom langchain_openai import ChatOpenAI\n# Importar Pydantic para validación de datos\nfrom pydantic import BaseModel, Field\n\n# Definir un modelo de datos con Pydantic para el enrutamiento\n# Este modelo fuerza al LLM a responder con una de las dos opciones\nclass RouteQuery(BaseModel):\n    \"\"\"Enruta una consulta del usuario a la fuente de datos más relevante.\"\"\"\n\n    # Campo que contendrá la decisión de enrutamiento\n    # Literal[\"vectorstore\", \"web_search\"] limita las opciones a solo estas dos\n    # ... indica que el campo es requerido\n    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n        ...,\n        description=\"Dada una pregunta del usuario, elige enrutarla a búsqueda web o a un vectorstore.\",\n    )\n\n# Crear una instancia del LLM para enrutamiento\n# model=\"gpt-4o-mini\": modelo rápido y económico (reemplazo de gpt-3.5-turbo)\n# temperature=0: respuestas determinísticas (sin aleatoriedad)\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n\n# Crear un LLM con salida estructurada según el modelo RouteQuery\n# with_structured_output() hace que el LLM responda exactamente en el formato esperado\nstructured_llm_router = llm.with_structured_output(RouteQuery)\n\n# Definir el prompt del sistema que instruye al LLM sobre su tarea de enrutamiento\n# Este prompt es crítico: define los criterios para decidir entre vectorstore y web\nsystem = \"\"\"Eres un experto en enrutar una pregunta del usuario a un vectorstore o búsqueda web.\nEl vectorstore contiene documentos relacionados con agentes, ingeniería de prompts y ataques adversarios.\nUsa el vectorstore para preguntas sobre estos temas. Para todo lo demás, usa búsqueda web.\"\"\"\n\n# Crear la plantilla de prompt completa\n# from_messages() crea un chat con dos roles: system (instrucciones) y human (input)\nroute_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),  # Instrucciones del sistema\n        (\"human\", \"{question}\"),  # Template para la pregunta del usuario\n    ]\n)\n\n# Encadenar el prompt con el LLM estructurado usando el operador |\n# Esto crea un \"chain\": prompt → LLM → salida estructurada (RouteQuery)\nquestion_router = route_prompt | structured_llm_router\n\n# Ejemplo 1: Prueba con pregunta que requiere búsqueda web\n# \"Cricket world cup 2023\" no está en los documentos locales (agentes, prompts, ataques)\n# El router debe decidir: web_search\nprint(\n    question_router.invoke(\n        {\"question\": \"Who won the Cricket world cup 2023 \"}\n    )\n)\n# Output esperado: datasource='web_search'"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac8d350",
   "metadata": {},
   "outputs": [],
   "source": "# Ejemplo 2: Prueba con pregunta que debería ir al vectorstore local\n# \"What are the types of agent memory?\" está relacionada con agentes (tema en el vectorstore)\n# El router debe decidir: vectorstore\nprint(question_router.invoke({\"question\": \"What are the types of agent memory?\"}))\n# Output esperado: datasource='vectorstore'"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2aac00",
   "metadata": {},
   "outputs": [],
   "source": "### Evaluador de Recuperación (Retrieval Grader)\n\n# Definir un modelo de datos con Pydantic para evaluar relevancia de documentos\nclass GradeDocuments(BaseModel):\n    \"\"\"Puntuación binaria para verificar la relevancia de documentos recuperados.\"\"\"\n\n    # Campo que contendrá la evaluación: 'yes' si es relevante, 'no' si no lo es\n    binary_score: str = Field(\n        description=\"Los documentos son relevantes a la pregunta, 'yes' o 'no'\"\n    )\n\n# Crear una instancia del LLM para evaluación de documentos\n# Usamos gpt-4o-mini para mantener costos bajos (muchas evaluaciones)\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n\n# Crear un LLM con salida estructurada según el modelo GradeDocuments\nstructured_llm_grader = llm.with_structured_output(GradeDocuments)\n\n# Definir el prompt del sistema para evaluar relevancia de documentos\n# IMPORTANTE: La prueba NO debe ser estricta, el objetivo es filtrar recuperaciones ERRÓNEAS\n# Es mejor dejar pasar un documento mediocre que filtrar uno potencialmente útil\nsystem = \"\"\"Eres un evaluador que analiza la relevancia de un documento recuperado respecto a una pregunta del usuario. \\n \n    Si el documento contiene palabra(s) clave o significado semántico relacionado con la pregunta del usuario, califícalo como relevante. \\n\n    No necesita ser una prueba estricta. El objetivo es filtrar recuperaciones erróneas. \\n\n    Da una puntuación binaria 'yes' o 'no' para indicar si el documento es relevante a la pregunta.\"\"\"\n\n# Crear la plantilla de prompt para evaluación\n# Recibe el documento recuperado y la pregunta del usuario\ngrade_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),  # Instrucciones del sistema\n        (\"human\", \"Documento recuperado: \\n\\n {document} \\n\\n Pregunta del usuario: {question}\"),\n    ]\n)\n\n# Encadenar el prompt con el LLM estructurado\n# Flujo: prompt (con document y question) → LLM → GradeDocuments\nretrieval_grader = grade_prompt | structured_llm_grader\n\n# Ejemplo de uso: evaluar si un documento es relevante\nquestion = \"agent memory\"  # Pregunta sobre memoria de agentes\n\n# Recuperar documentos similares usando el retriever\ndocs = retriever.invoke(question)\n\n# Obtener el contenido del segundo documento (índice 1)\ndoc_txt = docs[1].page_content\n\n# Evaluar la relevancia del documento\n# El LLM analiza si el documento contiene información sobre memoria de agentes\nprint(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))\n# Output esperado: binary_score='yes' (el documento habla sobre memoria de agentes)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9414eb5",
   "metadata": {},
   "outputs": [],
   "source": "### Generación RAG\n\n# Importar hub de LangChain para acceder a prompts compartidos\nfrom langchain import hub\n# Importar parser para convertir la salida del LLM en string\nfrom langchain_core.output_parsers import StrOutputParser\n\n# Obtener un prompt predefinido del hub de LangChain\n# \"rlm/rag-prompt\" es un prompt optimizado para RAG\nprompt = hub.pull(\"rlm/rag-prompt\")\n\n# Crear una instancia del LLM para generación de respuestas\n# model_name=\"gpt-4o-mini\": modelo rápido y económico\n# temperature=0: respuestas determinísticas\nllm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n\n# Función auxiliar para formatear documentos\n# Convierte una lista de objetos Document en un string con saltos de línea\ndef format_docs(docs):\n    # Une el contenido de cada documento con doble salto de línea\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n# Crear la cadena RAG completa usando el operador pipe (|)\n# Flujo: prompt → LLM → parser de string\nrag_chain = prompt | llm | StrOutputParser()\n\n# Ejecutar la cadena RAG con los documentos recuperados y la pregunta\n# invoke() genera una respuesta basada en el contexto de los documentos\ngeneration = rag_chain.invoke({\"context\": docs, \"question\": question})\n\n# Imprimir la respuesta generada\n# Esta respuesta estará basada en el contenido de los documentos recuperados\nprint(generation)\n# Output: Una respuesta sobre los tipos de memoria de agentes (short-term y long-term)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f699515",
   "metadata": {},
   "outputs": [],
   "source": "### Evaluador de Alucinaciones (Hallucination Grader)\n\n# Definir un modelo de datos para detectar alucinaciones\n# Una \"alucinación\" ocurre cuando el LLM genera información NO presente en los documentos\nclass GradeHallucinations(BaseModel):\n    \"\"\"Puntuación binaria para detectar alucinaciones en la respuesta generada.\"\"\"\n\n    # Campo que indica si la respuesta está fundamentada en los hechos\n    binary_score: str = Field(\n        description=\"La respuesta está fundamentada en los hechos, 'yes' o 'no'\"\n    )\n\n# Crear una instancia del LLM para detectar alucinaciones\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n\n# Crear un LLM con salida estructurada\nstructured_llm_grader = llm.with_structured_output(GradeHallucinations)\n\n# Definir el prompt del sistema para detección de alucinaciones\n# El evaluador verifica si la generación del LLM está SOPORTADA por los documentos recuperados\nsystem = \"\"\"Eres un evaluador que analiza si una generación de LLM está fundamentada en / soportada por un conjunto de hechos recuperados. \\n \n     Da una puntuación binaria 'yes' o 'no'. 'Yes' significa que la respuesta está fundamentada en / soportada por el conjunto de hechos.\"\"\"\n\n# Crear la plantilla de prompt\n# Recibe: documents (hechos recuperados) y generation (respuesta del LLM)\nhallucination_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"Conjunto de hechos: \\n\\n {documents} \\n\\n Generación del LLM: {generation}\"),\n    ]\n)\n\n# Encadenar el prompt con el LLM estructurado\n# Flujo: prompt (con documents y generation) → LLM → GradeHallucinations\nhallucination_grader = hallucination_prompt | structured_llm_grader\n\n# Evaluar si la generación contiene alucinaciones\n# El evaluador compara la respuesta generada con los documentos originales\nhallucination_grader.invoke({\"documents\": docs, \"generation\": generation})\n# Output esperado: GradeHallucinations(binary_score='yes')\n# Significa que la respuesta está fundamentada en los documentos"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ba5019",
   "metadata": {},
   "outputs": [],
   "source": "### Evaluador de Respuestas (Answer Grader)\n\n# Definir un modelo de datos para evaluar si la respuesta contesta la pregunta\n# Este evaluador verifica si la respuesta REALMENTE responde lo que el usuario preguntó\nclass GradeAnswer(BaseModel):\n    \"\"\"Puntuación binaria para evaluar si la respuesta aborda la pregunta.\"\"\"\n\n    # Campo que indica si la respuesta aborda/resuelve la pregunta\n    binary_score: str = Field(\n        description=\"La respuesta aborda la pregunta, 'yes' o 'no'\"\n    )\n\n# Crear una instancia del LLM para evaluar respuestas\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n\n# Crear un LLM con salida estructurada\nstructured_llm_grader = llm.with_structured_output(GradeAnswer)\n\n# Definir el prompt del sistema para evaluación de respuestas\n# El evaluador verifica si la respuesta RESUELVE/ABORDA la pregunta del usuario\nsystem = \"\"\"Eres un evaluador que analiza si una respuesta aborda / resuelve una pregunta \\n \n     Da una puntuación binaria 'yes' o 'no'. 'Yes' significa que la respuesta resuelve la pregunta.\"\"\"\n\n# Crear la plantilla de prompt\n# Recibe: question (pregunta del usuario) y generation (respuesta del LLM)\nanswer_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\"human\", \"Pregunta del usuario: \\n\\n {question} \\n\\n Generación del LLM: {generation}\"),\n    ]\n)\n\n# Encadenar el prompt con el LLM estructurado\n# Flujo: prompt (con question y generation) → LLM → GradeAnswer\nanswer_grader = answer_prompt | structured_llm_grader\n\n# Evaluar si la respuesta aborda la pregunta\n# El evaluador compara la pregunta original con la respuesta generada\nanswer_grader.invoke({\"question\": question, \"generation\": generation})\n# Output esperado: GradeAnswer(binary_score='yes')\n# Significa que la respuesta sí contesta la pregunta sobre memoria de agentes"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bc448f",
   "metadata": {},
   "outputs": [],
   "source": "### Reescritor de Preguntas (Question Re-writer)\n\n# Crear una instancia del LLM para reescribir preguntas\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n\n# Definir el prompt del sistema para reescritura de preguntas\n# Objetivo: optimizar la pregunta para RECUPERACIÓN EN VECTORSTORE\n# Diferencia vs CRAG: aquí optimizamos para vectorstore, CRAG optimiza para web\nsystem = \"\"\"Eres un reescritor de preguntas que convierte una pregunta de entrada en una mejor versión optimizada \\n \n     para recuperación en vectorstore. Observa la entrada e intenta razonar sobre la intención semántica / significado subyacente.\"\"\"\n\n# Crear la plantilla de prompt para reescritura\nre_write_prompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system),\n        (\n            \"human\",\n            \"Aquí está la pregunta inicial: \\n\\n {question} \\n Formula una pregunta mejorada.\",\n        ),\n    ]\n)\n\n# Crear la cadena de reescritura\n# Flujo: prompt con {question} → LLM → parser que convierte a string\nquestion_rewriter = re_write_prompt | llm | StrOutputParser()\n\n# Probar el reescritor con la pregunta de ejemplo\n# La pregunta \"agent memory\" es demasiado vaga\n# El reescritor debe generar una versión más detallada y específica\nquestion_rewriter.invoke({\"question\": question})\n# Output esperado: Una pregunta más específica como:\n# \"What are the key concepts and techniques related to agent memory in artificial intelligence?\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45060a79",
   "metadata": {},
   "outputs": [],
   "source": "### Herramienta de Búsqueda Web\n\n# Importar la herramienta de búsqueda de Tavily\n# Tavily es un motor de búsqueda optimizado para agentes de IA\nfrom langchain_community.tools.tavily_search import TavilySearchResults\n\n# Crear una instancia de la herramienta de búsqueda web\n# k=3: devuelve los 3 mejores resultados de búsqueda\n# Se usa cuando el router decide que la consulta necesita información de la web\nweb_search_tool = TavilySearchResults(k=3)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6084493",
   "metadata": {},
   "outputs": [],
   "source": "### Definición del Estado del Grafo\n\n# Importar List para tipar listas\nfrom typing import List\n# Importar TypedDict para crear diccionarios con tipos específicos\nfrom typing_extensions import TypedDict\n\n# Definir la clase GraphState que representa el estado del grafo en LangGraph\n# TypedDict permite especificar los tipos de datos de cada clave del diccionario\n# Este estado se pasa entre todos los nodos del grafo\nclass GraphState(TypedDict):\n    \"\"\"\n    Representa el estado de nuestro grafo.\n\n    Atributos:\n        question: La pregunta del usuario (original o reescrita)\n        generation: La respuesta generada por el LLM\n        documents: Lista de documentos recuperados (de vectorstore o web)\n    \"\"\"\n\n    # Pregunta del usuario (puede ser transformada por el reescritor)\n    question: str\n    \n    # Respuesta final generada por el LLM usando RAG\n    generation: str\n    \n    # Lista de documentos que se usarán como contexto\n    # Puede contener documentos del vectorstore local o resultados de búsqueda web\n    documents: List[str]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cee60fb",
   "metadata": {},
   "outputs": [],
   "source": "### Funciones de los Nodos del Grafo y Aristas Condicionales\n\n# Importar Document para crear objetos de documento\nfrom langchain.schema import Document\n\n\n# ========== NODOS DEL GRAFO ==========\n\n# NODO 1: Recuperar documentos del vectorstore local\ndef retrieve(state):\n    \"\"\"\n    Recupera documentos relevantes desde el vectorstore local.\n    \n    Args:\n        state (dict): El estado actual del grafo\n    \n    Returns:\n        state (dict): Estado actualizado con los documentos recuperados\n    \"\"\"\n    print(\"---RECUPERAR---\")\n    # Extraer la pregunta del estado\n    question = state[\"question\"]\n\n    # Recuperar documentos similares usando el retriever\n    # Busca en el vectorstore FAISS usando similitud coseno\n    documents = retriever.invoke(question)\n    \n    # Retornar el estado actualizado\n    return {\"documents\": documents, \"question\": question}\n\n\n# NODO 2: Generar respuesta usando RAG\ndef generate(state):\n    \"\"\"\n    Genera una respuesta usando los documentos como contexto.\n    \n    Args:\n        state (dict): El estado actual del grafo\n    \n    Returns:\n        state (dict): Estado actualizado con la generación del LLM\n    \"\"\"\n    print(\"---GENERAR---\")\n    # Extraer pregunta y documentos del estado\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n\n    # Generar respuesta usando la cadena RAG\n    # rag_chain toma el contexto y la pregunta, y genera una respuesta\n    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n    \n    # Retornar el estado completo con la respuesta generada\n    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n\n\n# NODO 3: Evaluar relevancia de documentos\ndef grade_documents(state):\n    \"\"\"\n    Determina si los documentos recuperados son relevantes a la pregunta.\n    Filtra documentos no relevantes.\n    \n    Args:\n        state (dict): El estado actual del grafo\n    \n    Returns:\n        state (dict): Estado con documentos filtrados (solo relevantes)\n    \"\"\"\n\n    print(\"---VERIFICAR RELEVANCIA DE DOCUMENTOS A LA PREGUNTA---\")\n    # Extraer pregunta y documentos del estado\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n\n    # Lista para almacenar solo los documentos relevantes\n    filtered_docs = []\n    \n    # Evaluar cada documento individualmente\n    for d in documents:\n        # Usar el evaluador (retrieval_grader) para calificar el documento\n        score = retrieval_grader.invoke(\n            {\"question\": question, \"document\": d.page_content}\n        )\n        # Extraer la puntuación binaria (yes/no)\n        grade = score.binary_score\n        \n        # Si el documento es relevante, agregarlo a la lista filtrada\n        if grade == \"yes\":\n            print(\"---CALIFICACIÓN: DOCUMENTO RELEVANTE---\")\n            filtered_docs.append(d)\n        # Si no es relevante, saltarlo\n        else:\n            print(\"---CALIFICACIÓN: DOCUMENTO NO RELEVANTE---\")\n            continue\n    \n    # Retornar estado con solo los documentos relevantes\n    return {\"documents\": filtered_docs, \"question\": question}\n\n\n# NODO 4: Transformar/reescribir la pregunta\ndef transform_query(state):\n    \"\"\"\n    Transforma la pregunta para producir una versión mejorada.\n    Se usa cuando los documentos no son relevantes o la respuesta no es útil.\n    \n    Args:\n        state (dict): El estado actual del grafo\n    \n    Returns:\n        state (dict): Estado con la pregunta reescrita\n    \"\"\"\n\n    print(\"---TRANSFORMAR CONSULTA---\")\n    # Extraer pregunta y documentos del estado\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n\n    # Reescribir la pregunta usando el reescritor (question_rewriter)\n    # Esto optimiza la pregunta para mejor recuperación en vectorstore\n    better_question = question_rewriter.invoke({\"question\": question})\n    \n    # Retornar estado con la pregunta mejorada\n    return {\"documents\": documents, \"question\": better_question}\n\n\n# NODO 5: Realizar búsqueda web\ndef web_search(state):\n    \"\"\"\n    Realiza búsqueda web usando Tavily.\n    Se usa cuando el router decide que la consulta necesita información de la web.\n    \n    Args:\n        state (dict): El estado actual del grafo\n    \n    Returns:\n        state (dict): Estado con documentos de búsqueda web\n    \"\"\"\n\n    print(\"---BÚSQUEDA WEB---\")\n    # Extraer pregunta del estado\n    question = state[\"question\"]\n\n    # Realizar búsqueda web usando la herramienta Tavily\n    # invoke() envía la consulta a Tavily y obtiene k=3 resultados\n    docs = web_search_tool.invoke({\"query\": question})\n    \n    # Unir el contenido de todos los resultados web en un solo string\n    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n    \n    # Crear un objeto Document con los resultados web\n    # IMPORTANTE: Reemplaza documents (no append), porque viene del router directo a web\n    web_results = Document(page_content=web_results)\n\n    # Retornar estado con los resultados web como documentos\n    return {\"documents\": web_results, \"question\": question}\n\n\n# ========== ARISTAS CONDICIONALES (DECISIONES) ==========\n\n# DECISIÓN 1: ¿Enrutar a vectorstore o búsqueda web?\ndef route_question(state):\n    \"\"\"\n    Enruta la pregunta a búsqueda web o RAG (vectorstore).\n    Esta es la PRIMERA decisión del flujo Adaptive RAG.\n    \n    Args:\n        state (dict): El estado actual del grafo\n    \n    Returns:\n        str: Nombre del siguiente nodo (\"web_search\" o \"vectorstore\")\n    \"\"\"\n\n    print(\"---ENRUTAR PREGUNTA---\")\n    # Extraer la pregunta del estado\n    question = state[\"question\"]\n    \n    # Usar el router para decidir la fuente de datos\n    # El router analiza la pregunta y decide: ¿vectorstore o web?\n    source = question_router.invoke({\"question\": question})\n    \n    # Si el router decide búsqueda web\n    if source.datasource == \"web_search\":\n        print(\"---ENRUTAR PREGUNTA A BÚSQUEDA WEB---\")\n        # Ir directamente al nodo web_search\n        return \"web_search\"\n    # Si el router decide vectorstore\n    elif source.datasource == \"vectorstore\":\n        print(\"---ENRUTAR PREGUNTA A RAG---\")\n        # Ir al nodo retrieve (recuperar de vectorstore)\n        return \"vectorstore\"\n\n\n# DECISIÓN 2: ¿Generar respuesta o transformar consulta?\ndef decide_to_generate(state):\n    \"\"\"\n    Determina si generar una respuesta o reescribir la pregunta.\n    Se ejecuta DESPUÉS de evaluar la relevancia de los documentos.\n    \n    Args:\n        state (dict): El estado actual del grafo\n    \n    Returns:\n        str: Nombre del siguiente nodo (\"transform_query\" o \"generate\")\n    \"\"\"\n\n    print(\"---EVALUAR DOCUMENTOS CALIFICADOS---\")\n    # Extraer documentos filtrados del estado\n    state[\"question\"]\n    filtered_documents = state[\"documents\"]\n\n    # Si NO hay documentos relevantes (todos fueron filtrados)\n    if not filtered_documents:\n        # Todos los documentos fueron filtrados por irrelevantes\n        # Necesitamos reescribir la pregunta y reintentar\n        print(\n            \"---DECISIÓN: TODOS LOS DOCUMENTOS NO SON RELEVANTES A LA PREGUNTA, TRANSFORMAR CONSULTA---\"\n        )\n        # Ir al nodo transform_query\n        return \"transform_query\"\n    # Si SÍ hay documentos relevantes\n    else:\n        # Tenemos al menos un documento relevante, podemos generar respuesta\n        print(\"---DECISIÓN: GENERAR---\")\n        # Ir al nodo generate\n        return \"generate\"\n\n\n# DECISIÓN 3: ¿La generación es útil y está fundamentada?\ndef grade_generation_v_documents_and_question(state):\n    \"\"\"\n    Determina si la generación está fundamentada en los documentos y responde la pregunta.\n    Esta es la evaluación FINAL que valida la calidad de la respuesta.\n    \n    Args:\n        state (dict): El estado actual del grafo\n    \n    Returns:\n        str: Decisión del siguiente paso (\"useful\", \"not useful\", \"not supported\")\n    \"\"\"\n\n    print(\"---VERIFICAR ALUCINACIONES---\")\n    # Extraer todos los valores necesarios del estado\n    question = state[\"question\"]\n    documents = state[\"documents\"]\n    generation = state[\"generation\"]\n\n    # PASO 1: Verificar si hay alucinaciones\n    # ¿La respuesta está fundamentada en los documentos?\n    score = hallucination_grader.invoke(\n        {\"documents\": documents, \"generation\": generation}\n    )\n    grade = score.binary_score\n\n    # Si la respuesta SÍ está fundamentada en los documentos (no hay alucinaciones)\n    if grade == \"yes\":\n        print(\"---DECISIÓN: GENERACIÓN ESTÁ FUNDAMENTADA EN DOCUMENTOS---\")\n        \n        # PASO 2: Verificar si la respuesta contesta la pregunta\n        print(\"---CALIFICAR GENERACIÓN vs PREGUNTA---\")\n        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n        grade = score.binary_score\n        \n        # Si la respuesta SÍ contesta la pregunta\n        if grade == \"yes\":\n            print(\"---DECISIÓN: GENERACIÓN ABORDA LA PREGUNTA---\")\n            # ¡Éxito! La respuesta es útil y fundamentada\n            return \"useful\"\n        # Si la respuesta NO contesta la pregunta\n        else:\n            print(\"---DECISIÓN: GENERACIÓN NO ABORDA LA PREGUNTA---\")\n            # La respuesta está fundamentada pero no contesta lo que se preguntó\n            # Necesitamos reescribir la pregunta y reintentar\n            return \"not useful\"\n    # Si la respuesta NO está fundamentada (contiene alucinaciones)\n    else:\n        print(\"---DECISIÓN: GENERACIÓN NO ESTÁ FUNDAMENTADA EN DOCUMENTOS, REINTENTAR---\")\n        # La respuesta tiene alucinaciones, regenerar\n        return \"not supported\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed76ecf",
   "metadata": {},
   "outputs": [],
   "source": "### Construcción del Flujo de Trabajo con LangGraph\n\n# Importar componentes de LangGraph para crear el flujo de trabajo\nfrom langgraph.graph import END, StateGraph, START\n\n# Crear una instancia del grafo con el estado definido (GraphState)\nworkflow = StateGraph(GraphState)\n\n# Definir los nodos del grafo\n# Cada nodo es una función que recibe y actualiza el estado\nworkflow.add_node(\"web_search\", web_search)          # Nodo: Búsqueda web con Tavily\nworkflow.add_node(\"retrieve\", retrieve)              # Nodo: Recuperar documentos del vectorstore\nworkflow.add_node(\"grade_documents\", grade_documents)  # Nodo: Evaluar relevancia de documentos\nworkflow.add_node(\"generate\", generate)              # Nodo: Generar respuesta final\nworkflow.add_node(\"transform_query\", transform_query)  # Nodo: Reescribir pregunta\n\n# ========== CONSTRUIR EL GRAFO ==========\n\n# ARISTA CONDICIONAL 1: START → ¿web_search o retrieve?\n# Esta es la PRIMERA decisión: el router decide la fuente de datos\nworkflow.add_conditional_edges(\n    START,                    # Nodo origen: inicio del grafo\n    route_question,           # Función de decisión: router\n    {\n        \"web_search\": \"web_search\",      # Si router dice \"web\" → ir a web_search\n        \"vectorstore\": \"retrieve\",       # Si router dice \"vectorstore\" → ir a retrieve\n    },\n)\n\n# ARISTA 2: web_search → generate\n# Después de búsqueda web, generar respuesta directamente\n# No necesita evaluación de relevancia (asumimos que Tavily da resultados relevantes)\nworkflow.add_edge(\"web_search\", \"generate\")\n\n# ARISTA 3: retrieve → grade_documents\n# Después de recuperar documentos locales, evaluar su relevancia\nworkflow.add_edge(\"retrieve\", \"grade_documents\")\n\n# ARISTA CONDICIONAL 2: grade_documents → ¿transform_query o generate?\n# Decisión basada en si hay documentos relevantes\nworkflow.add_conditional_edges(\n    \"grade_documents\",        # Nodo origen\n    decide_to_generate,       # Función de decisión\n    {\n        \"transform_query\": \"transform_query\",  # Si no hay docs relevantes → reescribir\n        \"generate\": \"generate\",                # Si hay docs relevantes → generar\n    },\n)\n\n# ARISTA 4: transform_query → retrieve\n# Después de reescribir la pregunta, reintentar recuperación\n# Esto crea un CICLO que permite múltiples intentos de recuperación\nworkflow.add_edge(\"transform_query\", \"retrieve\")\n\n# ARISTA CONDICIONAL 3: generate → ¿END, transform_query o generate?\n# Esta es la evaluación FINAL de la calidad de la respuesta\nworkflow.add_conditional_edges(\n    \"generate\",               # Nodo origen\n    grade_generation_v_documents_and_question,  # Función de decisión compleja\n    {\n        \"not supported\": \"generate\",       # Si hay alucinaciones → regenerar\n        \"useful\": END,                     # Si es útil y fundamentada → terminar\n        \"not useful\": \"transform_query\",   # Si no contesta → reescribir y reintentar\n    },\n)\n\n# Compilar el grafo en una aplicación ejecutable\napp = workflow.compile()\n\n# ========== FLUJO COMPLETO DEL ADAPTIVE RAG ==========\n#\n# START\n#   ↓\n# [route_question] ← DECISIÓN 1: ¿Vectorstore o Web?\n#   ↓\n#   ├─→ [web_search] → [generate] → [grade_generation...] ← DECISIÓN 3\n#   │\n#   └─→ [retrieve] → [grade_documents] ← DECISIÓN 2\n#         ↓\n#         ├─→ [generate] → [grade_generation...] ← DECISIÓN 3\n#         │                   ↓\n#         │                   ├─→ [useful] → END ✓\n#         │                   ├─→ [not useful] → [transform_query] → [retrieve] (ciclo)\n#         │                   └─→ [not supported] → [generate] (ciclo)\n#         │\n#         └─→ [transform_query] → [retrieve] (ciclo)\n#\n# CARACTERÍSTICAS CLAVE:\n# - Enrutamiento inteligente inicial (web vs vectorstore)\n# - Evaluación de relevancia de documentos\n# - Detección de alucinaciones\n# - Validación de que la respuesta conteste la pregunta\n# - Auto-corrección mediante ciclos de retroalimentación\n# - Múltiples intentos hasta obtener una respuesta de calidad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f22293",
   "metadata": {},
   "outputs": [],
   "source": "# Ejecutar el flujo completo de Adaptive RAG\n# Ejemplo 1: Pregunta que necesita búsqueda web\n# \"What is machine learning\" no está en los documentos locales (agentes, prompts, ataques)\n# El router debe enviarla a búsqueda web\n\napp.invoke({\"question\": \"What is machine learning\"})\n\n# FLUJO ESPERADO:\n# 1. route_question decide: \"web_search\"\n# 2. web_search busca en Tavily\n# 3. generate crea respuesta con resultados web\n# 4. grade_generation verifica: ¿fundamentada? ¿útil?\n# 5. Si todo está bien: retorna respuesta final"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a7d624",
   "metadata": {},
   "outputs": [],
   "source": "# Ejemplo 2: Pregunta que usa el vectorstore local\n# \"What is agent memory\" está relacionada con agentes (tema en el vectorstore)\n# El router debe enviarla al vectorstore local\n\napp.invoke({\"question\": \"What is agent memory\"})\n\n# FLUJO ESPERADO:\n# 1. route_question decide: \"vectorstore\"\n# 2. retrieve busca en FAISS\n# 3. grade_documents evalúa relevancia de cada documento\n# 4. Si hay docs relevantes: generate crea respuesta\n# 5. grade_generation verifica calidad\n# 6. Si no hay docs relevantes: transform_query → retrieve (reintenta)\n# 7. Si la respuesta tiene alucinaciones: generate (regenera)\n# 8. Si la respuesta no contesta: transform_query → retrieve (reintenta)\n# 9. Si todo está bien: retorna respuesta final"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046ff716",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}