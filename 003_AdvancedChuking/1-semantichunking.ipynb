{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c7c41e9",
   "metadata": {},
   "source": [
    "### Chunking Semántico (Semantic Chunking)\n",
    "- SemanticChunker es un divisor de documentos que utiliza la similitud de embeddings entre oraciones para decidir los límites de los fragmentos (chunks).\n",
    "\n",
    "- Asegura que cada fragmento sea semánticamente coherente y no se corte a mitad de una idea, como sucede con los divisores tradicionales basados en caracteres/tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "785b4e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar el modelo de transformers para generar embeddings de oraciones\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# Importar función para calcular similitud coseno entre vectores\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Importar numpy para operaciones numéricas\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39054f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 Chunks Semánticos:\n",
      "\n",
      "Chunk 1:\n",
      "LangChain es un framework para crear aplicaciones con LLM.\n",
      "\n",
      "Chunk 2:\n",
      "Langchain proporciona abstracciones modulares para combinar LLM con herramientas como OpenAI y Pinecone.\n",
      "\n",
      "Chunk 3:\n",
      "Permite crear cadenas, agentes, memoria y recuperadores.\n",
      "\n",
      "Chunk 4:\n",
      "La Torre Eiffel se encuentra en París.\n",
      "\n",
      "Chunk 5:\n",
      "Francia es un destino turístico popular.\n"
     ]
    }
   ],
   "source": [
    "## Inicializar el modelo de embeddings pre-entrenado (384 dimensiones)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "## Texto de ejemplo con diferentes temas\n",
    "text = \"\"\"\n",
    "LangChain es un framework para crear aplicaciones con LLM.\n",
    "Langchain proporciona abstracciones modulares para combinar LLM con herramientas como OpenAI y Pinecone.\n",
    "Permite crear cadenas, agentes, memoria y recuperadores.\n",
    "La Torre Eiffel se encuentra en París.\n",
    "Francia es un destino turístico popular.\n",
    "\"\"\"\n",
    "\n",
    "## Paso 1: Dividir el texto en oraciones individuales\n",
    "# Eliminar líneas vacías y espacios en blanco\n",
    "sentences = [s.strip() for s in text.split(\"\\n\") if s.strip()]\n",
    "\n",
    "## Paso 2: Generar embeddings (vectores) para cada oración\n",
    "# Cada oración se convierte en un vector numérico de 384 dimensiones\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# Paso 3: Inicializar parámetros para el chunking semántico\n",
    "threshold = 0.7  # Umbral de similitud: controla qué tan relacionadas deben estar las oraciones (0-1)\n",
    "chunks = []  # Lista para almacenar los fragmentos finales\n",
    "current_chunk = [sentences[0]]  # Comenzar con la primera oración\n",
    "\n",
    "## Paso 4: Agrupación semántica basada en el umbral de similitud\n",
    "\n",
    "for i in range(1, len(sentences)):\n",
    "    # Calcular similitud coseno entre la oración actual y la anterior\n",
    "    # Devuelve un valor entre -1 y 1 (1 = idénticas, 0 = sin relación)\n",
    "    sim = cosine_similarity(\n",
    "        [embeddings[i - 1]],  # Embedding de la oración anterior\n",
    "        [embeddings[i]]        # Embedding de la oración actual\n",
    "    )[0][0]\n",
    "\n",
    "    # Si la similitud es mayor o igual al umbral, agregar a chunk actual\n",
    "    if sim >= threshold:\n",
    "        current_chunk.append(sentences[i])\n",
    "    else:\n",
    "        # Si no hay suficiente similitud, guardar el chunk actual y empezar uno nuevo\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "        current_chunk = [sentences[i]]\n",
    "\n",
    "# Agregar el último chunk a la lista\n",
    "chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "# Mostrar los chunks resultantes\n",
    "print(\"\\n📌 Chunks Semánticos:\")\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    print(f\"\\nChunk {idx+1}:\\n{chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0908eeb1",
   "metadata": {},
   "source": [
    "### Pipeline RAG con Código Modular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0869f69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar modelo de sentence transformers para embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# Importar función para calcular similitud entre vectores\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Importar clase Document de LangChain para estructurar documentos\n",
    "from langchain.schema import Document\n",
    "# Importar FAISS para crear base de datos vectorial en memoria\n",
    "from langchain.vectorstores import FAISS\n",
    "# Importar embeddings de OpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "# Importar función para inicializar modelos de chat\n",
    "from langchain.chat_models import init_chat_model\n",
    "# Importar componentes para crear cadenas ejecutables\n",
    "from langchain.schema.runnable import RunnableLambda, RunnableMap\n",
    "# Importar clase para crear templates de prompts\n",
    "from langchain.prompts import PromptTemplate\n",
    "# Importar parser para convertir salida del LLM a string\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# Importar librería para manejo de variables de entorno\n",
    "import os\n",
    "# Configurar API key de Groq desde archivo .env\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49c799b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Chunker Semántico Personalizado con Umbral de Similitud\n",
    "\n",
    "class ThresholdSematicChunker:\n",
    "    \"\"\"\n",
    "    Clase que divide textos en fragmentos semánticamente coherentes\n",
    "    basándose en la similitud entre oraciones\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\", threshold=0.7):\n",
    "        # Inicializar el modelo de embeddings\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        # Definir el umbral de similitud (0-1)\n",
    "        self.threshold = threshold \n",
    "\n",
    "    def split(self, text: str):\n",
    "        \"\"\"\n",
    "        Divide un texto en chunks basándose en similitud semántica\n",
    "        \n",
    "        Args:\n",
    "            text: Texto a dividir\n",
    "            \n",
    "        Returns:\n",
    "            Lista de strings (chunks)\n",
    "        \"\"\"\n",
    "        # Dividir texto en oraciones usando el punto como separador\n",
    "        sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "        # Generar embeddings para todas las oraciones\n",
    "        embeddings = self.model.encode(sentences)\n",
    "        # Inicializar lista de chunks y chunk actual\n",
    "        chunks = []\n",
    "        current_chunk = [sentences[0]]\n",
    "\n",
    "        # Iterar sobre las oraciones comparando similitudes\n",
    "        for i in range(1, len(sentences)):\n",
    "            # Calcular similitud coseno entre oración actual y anterior\n",
    "            sim = cosine_similarity([embeddings[i - 1]], [embeddings[i]])[0][0]\n",
    "            # Si similitud >= umbral, agregar a chunk actual\n",
    "            if sim >= self.threshold:\n",
    "                current_chunk.append(sentences[i])\n",
    "            else:\n",
    "                # Si no, guardar chunk actual y empezar uno nuevo\n",
    "                chunks.append(\". \".join(current_chunk) + \".\")\n",
    "                current_chunk = [sentences[i]]\n",
    "\n",
    "        # Agregar el último chunk\n",
    "        chunks.append(\". \".join(current_chunk) + \".\")\n",
    "        return chunks\n",
    "    \n",
    "    def split_documents(self, docs):\n",
    "        \"\"\"\n",
    "        Divide una lista de documentos de LangChain en chunks semánticos\n",
    "        \n",
    "        Args:\n",
    "            docs: Lista de objetos Document\n",
    "            \n",
    "        Returns:\n",
    "            Lista de objetos Document con chunks\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # Procesar cada documento\n",
    "        for doc in docs:\n",
    "            # Dividir contenido en chunks y crear nuevos Documents\n",
    "            for chunk in self.split(doc.page_content):\n",
    "                # Preservar los metadatos originales del documento\n",
    "                result.append(Document(page_content=chunk, metadata=doc.metadata))\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc7efa10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content='\\nLangChain es un framework para crear aplicaciones con LLM.\\nLangchain proporciona abstracciones modulares para combinar LLM con herramientas como OpenAI y Pinecone.\\nPermite crear cadenas, agentes, memoria y recuperadores.\\nLa Torre Eiffel se encuentra en París.\\nFrancia es un destino turístico popular.\\n')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Texto de ejemplo con diferentes temas semánticos\n",
    "sample_text = \"\"\"\n",
    "LangChain es un framework para crear aplicaciones con LLM.\n",
    "Langchain proporciona abstracciones modulares para combinar LLM con herramientas como OpenAI y Pinecone.\n",
    "Permite crear cadenas, agentes, memoria y recuperadores.\n",
    "La Torre Eiffel se encuentra en París.\n",
    "Francia es un destino turístico popular.\n",
    "\"\"\"\n",
    "\n",
    "# Crear un objeto Document de LangChain con el texto\n",
    "doc = Document(page_content=sample_text)\n",
    "# Mostrar el documento\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af673440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='LangChain es un framework para crear aplicaciones con LLM.'),\n",
       " Document(metadata={}, page_content='Langchain proporciona abstracciones modulares para combinar LLM con herramientas como OpenAI y Pinecone.'),\n",
       " Document(metadata={}, page_content='Permite crear cadenas, agentes, memoria y recuperadores.'),\n",
       " Document(metadata={}, page_content='La Torre Eiffel se encuentra en París.'),\n",
       " Document(metadata={}, page_content='Francia es un destino turístico popular.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Aplicar Chunking Semántico\n",
    "\n",
    "# Crear instancia del chunker con umbral de 0.7 (70% de similitud mínima)\n",
    "chunker = ThresholdSematicChunker(threshold=0.7)\n",
    "# Dividir el documento en chunks semánticamente coherentes\n",
    "chunks = chunker.split_documents([doc])\n",
    "# Mostrar los chunks resultantes\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd4220a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Crear Vector Store (Base de Datos Vectorial)\n",
    "\n",
    "# Importar librería para manejo de variables de entorno\n",
    "import os\n",
    "# Configurar API key de OpenAI desde archivo .env\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "# Inicializar el modelo de embeddings de OpenAI\n",
    "embedding = OpenAIEmbeddings()\n",
    "# Crear base de datos vectorial FAISS a partir de los chunks\n",
    "# FAISS indexa los embeddings para búsqueda rápida por similitud\n",
    "vectorstore = FAISS.from_documents(chunks, embedding)\n",
    "# Convertir el vectorstore en un retriever para búsqueda de documentos relevantes\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c65f88b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Responde la pregunta basándote en el siguiente contexto:\\n\\n{context}\\n\\nPregunta: {question}\\n')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Template de Prompt para RAG\n",
    "\n",
    "# Crear template que define cómo formatear la información para el LLM\n",
    "# {context} será reemplazado por documentos recuperados\n",
    "# {question} será reemplazado por la pregunta del usuario\n",
    "template = \"\"\"Responde la pregunta basándote en el siguiente contexto:\n",
    "\n",
    "{context}\n",
    "\n",
    "Pregunta: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Crear objeto PromptTemplate a partir del template\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "# Mostrar el prompt\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "941a408a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Según el contexto, LangChain se utiliza para crear aplicaciones con LLM (Modelos de Lenguaje de Llama).\n"
     ]
    }
   ],
   "source": [
    "## Inicializar el Modelo de Lenguaje (LLM)\n",
    "\n",
    "# Inicializar modelo Gemma2-9b-it de Groq con temperatura 0.4\n",
    "# Temperatura baja = respuestas más deterministas y precisas\n",
    "llm = init_chat_model(model=\"groq:llama-3.1-8b-instant\", temperature=0.4)\n",
    "\n",
    "### Crear Cadena RAG usando LCEL (LangChain Expression Language)\n",
    "\n",
    "rag_chain = (\n",
    "    # RunnableMap ejecuta múltiples funciones en paralelo\n",
    "    RunnableMap(\n",
    "        {\n",
    "        # Lambda para recuperar documentos relevantes usando el retriever\n",
    "        \"context\": lambda x: retriever.invoke(x[\"question\"]),\n",
    "        # Lambda para pasar la pregunta sin modificar\n",
    "        \"question\": lambda x: x[\"question\"],  \n",
    "        }\n",
    "    )\n",
    "    # Operador | encadena componentes secuencialmente\n",
    "    | prompt  # Formatear contexto y pregunta en el template\n",
    "    | llm  # Enviar prompt al modelo de lenguaje\n",
    "    | StrOutputParser()  # Convertir respuesta del LLM a string\n",
    ")\n",
    "\n",
    "# Ejecutar consulta de ejemplo\n",
    "query = {\"question\": \"¿Para qué se utiliza LangChain?\"}\n",
    "# Invocar la cadena RAG con la pregunta\n",
    "result = rag_chain.invoke(query)\n",
    "\n",
    "# Mostrar la respuesta generada\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f17648",
   "metadata": {},
   "source": [
    "### Chunker Semántico con LangChain (Implementación Nativa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "074a8292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar embeddings de OpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "# Importar SemanticChunker experimental de LangChain\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "# Importar cargador de archivos de texto\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50907e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " chunk 1:\n",
      "LangChain es un framework para crear aplicaciones con LLM. Langchain proporciona abstracciones modulares para combinar LLM con herramientas como OpenAI y Pinecone. Permite crear cadenas, agentes, memoria y recuperadores.\n",
      "\n",
      " chunk 2:\n",
      "La Torre Eiffel se encuentra en ParÃ­s. Francia es un destino turÃ­stico popular.\n"
     ]
    }
   ],
   "source": [
    "## Cargar el documento desde archivo de texto\n",
    "loader = TextLoader(\"langchain_intro.txt\")\n",
    "# Cargar contenido del archivo\n",
    "docs = loader.load()\n",
    "\n",
    "## Inicializar modelo de embeddings de OpenAI\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "## Crear el chunker semántico de LangChain\n",
    "# Usa internamente embeddings para detectar cambios semánticos\n",
    "chunker = SemanticChunker(embedding)\n",
    "\n",
    "## Dividir los documentos en chunks semánticos\n",
    "chunks = chunker.split_documents(docs)\n",
    "\n",
    "## Mostrar resultados\n",
    "\n",
    "# Iterar sobre cada chunk generado\n",
    "for i, chunk in enumerate(chunks):\n",
    "    # Imprimir número de chunk y su contenido\n",
    "    print(f\"\\n chunk {i+1}:\\n{chunk.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c184ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
