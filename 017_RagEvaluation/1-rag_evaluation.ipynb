{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a782644a",
   "metadata": {},
   "source": [
    "### Evaluación de Chatbot y RAG\n",
    "\n",
    "La Generación Aumentada por Recuperación (RAG) es una técnica que mejora los Modelos de Lenguaje Grandes (LLMs) al proporcionarles conocimiento externo relevante. Se ha convertido en uno de los enfoques más utilizados para construir aplicaciones de LLM.\n",
    "\n",
    "Este tutorial te mostrará cómo evaluar tus aplicaciones RAG usando LangSmith. Aprenderás:\n",
    "\n",
    "1. Cómo crear conjuntos de datos de prueba\n",
    "2. Cómo ejecutar tu aplicación RAG sobre esos conjuntos de datos\n",
    "3. Cómo medir el rendimiento de tu aplicación usando diferentes métricas de evaluación\n",
    "\n",
    "#### Descripción General\n",
    "Un flujo de trabajo típico de evaluación de RAG consiste en tres pasos principales:\n",
    "\n",
    "1. Crear un conjunto de datos con preguntas y sus respuestas esperadas\n",
    "2. Ejecutar tu aplicación RAG sobre esas preguntas\n",
    "3. Usar evaluadores para medir qué tan bien funcionó tu aplicación, observando factores como:\n",
    " - Relevancia de la respuesta\n",
    " - Precisión de la respuesta\n",
    " - Calidad de la recuperación\n",
    " \n",
    "Para este tutorial, crearemos y evaluaremos un bot que responde preguntas sobre algunas de las perspicaces publicaciones del blog de Lilian Weng."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0581b36",
   "metadata": {},
   "source": [
    "### Evaluación de Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4f5cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar el módulo os para interactuar con el sistema operativo\n",
    "import os\n",
    "# Importar load_dotenv para cargar variables de entorno desde un archivo .env\n",
    "from dotenv import load_dotenv\n",
    "# Cargar las variables de entorno desde el archivo .env al entorno del sistema\n",
    "load_dotenv()\n",
    "\n",
    "# Configurar la clave de API de LangSmith desde las variables de entorno\n",
    "# LangSmith se usa para monitorear y evaluar aplicaciones LLM\n",
    "os.environ[\"LANGSMITH_API_KEY\"]=os.getenv(\"LANGSMITH_API_KEY\")\n",
    "# Configurar la clave de API de OpenAI desde las variables de entorno\n",
    "# OpenAI proporciona los modelos de lenguaje que usaremos\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "# Activar el rastreo de LangSmith para monitorear todas las llamadas a la API\n",
    "# Esto permite visualizar y analizar las interacciones con el modelo\n",
    "os.environ[\"LANGSMITH_TRACING\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a6d452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar el cliente de LangSmith para interactuar con la plataforma\n",
    "from langsmith import Client\n",
    "\n",
    "# Crear una instancia del cliente de LangSmith\n",
    "# Este cliente nos permite crear datasets, ejemplos y ejecutar evaluaciones\n",
    "client = Client()\n",
    "\n",
    "# Definir el nombre del dataset: estos son tus casos de prueba\n",
    "# Un dataset agrupa ejemplos de entrada/salida para evaluar el rendimiento del modelo\n",
    "dataset_name = \"Chatbots Evaluation\"\n",
    "# Crear un nuevo dataset en LangSmith con el nombre especificado\n",
    "# Esto devuelve un objeto dataset con un ID único\n",
    "dataset = client.create_dataset(dataset_name)\n",
    "# Crear múltiples ejemplos dentro del dataset\n",
    "# Cada ejemplo contiene:\n",
    "# - inputs: La pregunta que se hará al modelo\n",
    "# - outputs: La respuesta esperada/correcta (ground truth)\n",
    "client.create_examples(\n",
    "    # Especificar a qué dataset pertenecen estos ejemplos usando su ID\n",
    "    dataset_id=dataset.id,\n",
    "    # Lista de ejemplos con preguntas y respuestas esperadas\n",
    "    examples=[\n",
    "        {\n",
    "            # Primera pregunta: sobre LangChain\n",
    "            \"inputs\": {\"question\": \"What is LangChain?\"},\n",
    "            # Respuesta esperada: definición concisa de LangChain\n",
    "            \"outputs\": {\"answer\": \"A framework for building LLM applications\"},\n",
    "        },\n",
    "        {\n",
    "            # Segunda pregunta: sobre LangSmith\n",
    "            \"inputs\": {\"question\": \"What is LangSmith?\"},\n",
    "            # Respuesta esperada: definición de LangSmith como plataforma de observación\n",
    "            \"outputs\": {\"answer\": \"A platform for observing and evaluating LLM applications\"},\n",
    "        },\n",
    "        {\n",
    "            # Tercera pregunta: sobre OpenAI\n",
    "            \"inputs\": {\"question\": \"What is OpenAI?\"},\n",
    "            # Respuesta esperada: OpenAI como empresa creadora de LLMs\n",
    "            \"outputs\": {\"answer\": \"A company that creates Large Language Models\"},\n",
    "        },\n",
    "        {\n",
    "            # Cuarta pregunta: sobre Google\n",
    "            \"inputs\": {\"question\": \"What is Google?\"},\n",
    "            # Respuesta esperada: Google como empresa tecnológica conocida por búsquedas\n",
    "            \"outputs\": {\"answer\": \"A technology company known for search\"},\n",
    "        },\n",
    "        {\n",
    "            # Quinta pregunta: sobre Mistral\n",
    "            \"inputs\": {\"question\": \"What is Mistral?\"},\n",
    "            # Respuesta esperada: Mistral como empresa creadora de LLMs\n",
    "            \"outputs\": {\"answer\": \"A company that creates Large Language Models\"},\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20dd601",
   "metadata": {},
   "source": [
    "### Definir Métricas (LLM como Juez)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e88e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar la biblioteca de OpenAI para interactuar con sus modelos\n",
    "import openai\n",
    "# Importar wrappers de langsmith para envolver el cliente de OpenAI\n",
    "# Esto permite rastrear automáticamente todas las llamadas a OpenAI\n",
    "from langsmith import wrappers\n",
    " \n",
    "# Crear un cliente de OpenAI envuelto con funcionalidad de rastreo de LangSmith\n",
    "# Esto registrará todas las interacciones con la API en LangSmith para análisis\n",
    "openai_client=wrappers.wrap_openai(openai.OpenAI())\n",
    "\n",
    "# Instrucciones del sistema para el LLM evaluador\n",
    "# Define el rol del modelo como un profesor experto que califica respuestas\n",
    "eval_instructions = \"You are an expert professor specialized in grading students' answers to questions.\"\n",
    "\n",
    "# Función evaluadora de corrección que compara respuesta predicha vs respuesta correcta\n",
    "# Parámetros:\n",
    "# - inputs: diccionario con la pregunta original\n",
    "# - outputs: diccionario con la respuesta generada por el modelo\n",
    "# - reference_outputs: diccionario con la respuesta correcta esperada\n",
    "def correctness(inputs:dict,outputs:dict, reference_outputs:dict)->bool:\n",
    "      # Construir el contenido del mensaje para el evaluador\n",
    "      # Incluye la pregunta, respuesta correcta y respuesta del estudiante\n",
    "      user_content = f\"\"\"You are grading the following question:\n",
    "    {inputs['question']}\n",
    "    Here is the real answer:\n",
    "    {reference_outputs['answer']}\n",
    "    You are grading the following predicted answer:\n",
    "    {outputs['response']}\n",
    "    Respond with CORRECT or INCORRECT:\n",
    "    Grade:\n",
    "    \"\"\"\n",
    "      # Llamar al modelo GPT-4o-mini para evaluar la corrección\n",
    "      # temperature=0 hace que la respuesta sea determinística y consistente\n",
    "      response=openai_client.chat.completions.create(\n",
    "            # Usar el modelo mini para evaluaciones rápidas y económicas\n",
    "            model=\"gpt-4o-mini\",\n",
    "            # Temperatura 0 para respuestas determinísticas\n",
    "            temperature=0,\n",
    "            # Mensajes que incluyen el rol del sistema y el contenido a evaluar\n",
    "            messages=[\n",
    "                  # Mensaje del sistema con las instrucciones del evaluador\n",
    "                  {\"role\":\"system\",\"content\":eval_instructions},\n",
    "                  # Mensaje del usuario con la pregunta y respuestas a comparar\n",
    "                  {\"role\":\"user\",\"content\":user_content}\n",
    "            ]\n",
    "      # Extraer el contenido del mensaje de la primera opción de respuesta\n",
    "      ).choices[0].message.content\n",
    "\n",
    "      # Retornar True si la respuesta es \"CORRECT\", False en caso contrario\n",
    "      return response == \"CORRECT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4326a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métrica de Concisión - verifica si la respuesta generada no es excesivamente larga\n",
    "# El objetivo es asegurar que el modelo genere respuestas concisas y no verbosas\n",
    "\n",
    "# Función evaluadora de concisión\n",
    "# Parámetros:\n",
    "# - outputs: diccionario con la respuesta generada por el modelo\n",
    "# - reference_outputs: diccionario con la respuesta de referencia esperada\n",
    "def concision(outputs: dict, reference_outputs: dict) -> bool:\n",
    "    # Comparar la longitud de la respuesta generada vs la esperada\n",
    "    # La respuesta es considerada concisa si es menor a 2x la longitud de la referencia\n",
    "    # len() cuenta el número de caracteres en cada string\n",
    "    # Retorna 1 (True) si es concisa, 0 (False) si es demasiado larga\n",
    "    return int(len(outputs[\"response\"]) < 2 * len(reference_outputs[\"answer\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ad76f1",
   "metadata": {},
   "source": [
    "### Ejecutar Evaluaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42fd2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las instrucciones por defecto para el chatbot\n",
    "# Estas instrucciones guían al modelo para que responda de manera concisa\n",
    "default_instructions = \"Respond to the users question in a short, concise manner (one short sentence).\"\n",
    "\n",
    "# Función principal de la aplicación del chatbot\n",
    "# Esta función recibe una pregunta y genera una respuesta usando OpenAI\n",
    "# Parámetros:\n",
    "# - question: La pregunta del usuario (string)\n",
    "# - model: El modelo de OpenAI a usar (default: \"gpt-4o-mini\")\n",
    "# - instructions: Instrucciones del sistema para guiar al modelo\n",
    "def my_app(question: str, model: str = \"gpt-4o-mini\", instructions: str = default_instructions) -> str:\n",
    "    # Llamar a la API de OpenAI para generar una respuesta\n",
    "    return openai_client.chat.completions.create(\n",
    "        # Especificar qué modelo usar (gpt-4o-mini es rápido y económico)\n",
    "        model=model,\n",
    "        # Temperatura 0 para respuestas consistentes y determinísticas\n",
    "        temperature=0,\n",
    "        # Array de mensajes que define la conversación\n",
    "        messages=[\n",
    "            # Primer mensaje: instrucciones del sistema que definen el comportamiento del modelo\n",
    "            {\"role\": \"system\", \"content\": instructions},\n",
    "            # Segundo mensaje: la pregunta del usuario\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "    # Extraer el contenido de texto de la primera respuesta generada\n",
    "    ).choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e171d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función objetivo para LangSmith - se ejecuta para cada punto de datos del dataset\n",
    "# Esta función adapta la entrada del dataset al formato esperado por my_app\n",
    "# y devuelve la salida en el formato esperado por los evaluadores\n",
    "\n",
    "# Parámetros:\n",
    "# - inputs: diccionario con la pregunta del dataset\n",
    "def ls_target(inputs: str) -> dict:\n",
    "    # Llamar a my_app con la pregunta extraída de inputs\n",
    "    # Retornar un diccionario con clave \"response\" para que coincida con el formato esperado\n",
    "    return {\"response\": my_app(inputs[\"question\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc97aa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar la evaluación completa del chatbot\n",
    "# Esta función ejecuta el sistema de IA sobre todo el dataset y aplica los evaluadores\n",
    "experiment_results=client.evaluate(\n",
    "    # La función objetivo que se probará (nuestro sistema de IA)\n",
    "    ls_target,\n",
    "    # El nombre del dataset con los casos de prueba\n",
    "    data=dataset_name,\n",
    "    # Lista de funciones evaluadoras que medirán el rendimiento\n",
    "    # correctness: verifica si la respuesta es correcta comparada con la referencia\n",
    "    # concision: verifica si la respuesta es concisa (no más de 2x la longitud esperada)\n",
    "    evaluators=[correctness,concision],\n",
    "    # Prefijo para identificar este experimento en LangSmith\n",
    "    # Útil para comparar diferentes configuraciones o modelos\n",
    "    experiment_prefix=\"openai-4o-mini-chatbot\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9158ae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función objetivo modificada para probar un modelo diferente (GPT-4 Turbo)\n",
    "# Esta versión usa el modelo más avanzado GPT-4 Turbo en lugar de GPT-4o-mini\n",
    "# Permite comparar el rendimiento de diferentes modelos sobre el mismo dataset\n",
    "\n",
    "# Parámetros:\n",
    "# - inputs: diccionario con la pregunta del dataset\n",
    "def ls_target(inputs: str) -> dict:\n",
    "    # Llamar a my_app especificando explícitamente el modelo GPT-4 Turbo\n",
    "    # GPT-4 Turbo es más capaz pero también más costoso que gpt-4o-mini\n",
    "    return {\"response\": my_app(inputs[\"question\"],model=\"gpt-4-turbo\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e58d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar una segunda evaluación usando GPT-4 Turbo\n",
    "# Esto permite comparar directamente el rendimiento de GPT-4 Turbo vs GPT-4o-mini\n",
    "experiment_results=client.evaluate(\n",
    "    # La función objetivo con GPT-4 Turbo\n",
    "    ls_target,\n",
    "    # El mismo dataset de prueba para una comparación justa\n",
    "    data=dataset_name,\n",
    "    # Los mismos evaluadores para métricas consistentes\n",
    "    evaluators=[correctness,concision],\n",
    "    # Prefijo diferente para distinguir este experimento del anterior\n",
    "    # LangSmith mostrará ambos experimentos lado a lado para comparación\n",
    "    experiment_prefix=\"openai-4-turbo-chatbot\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbad8e8",
   "metadata": {},
   "source": [
    "### Evaluación para RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ddbc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CONFIGURACIÓN DEL SISTEMA RAG =====\n",
    "# RAG: Retrieval-Augmented Generation (Generación Aumentada por Recuperación)\n",
    "\n",
    "# Importar los componentes necesarios para construir el sistema RAG\n",
    "from langchain_community.document_loaders import WebBaseLoader  # Cargador de documentos desde URLs\n",
    "from langchain_core.vectorstores import InMemoryVectorStore  # Base de datos vectorial en memoria\n",
    "from langchain_openai import OpenAIEmbeddings  # Embeddings de OpenAI para convertir texto a vectores\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter  # Divisor de texto inteligente\n",
    "\n",
    "# Lista de URLs de los blogs de Lilian Weng que usaremos como base de conocimiento\n",
    "# Estos artículos cubren temas sobre agentes, prompt engineering y ataques adversarios\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",  # Artículo sobre agentes de IA\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",  # Artículo sobre ingeniería de prompts\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",  # Artículo sobre ataques adversarios a LLMs\n",
    "]\n",
    "\n",
    "# Cargar los documentos desde cada URL usando WebBaseLoader\n",
    "# List comprehension que itera sobre cada URL y carga su contenido\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "# Aplanar la lista de listas en una sola lista de documentos\n",
    "# [item for sublist in docs for item in sublist] convierte [[doc1], [doc2], [doc3]] en [doc1, doc2, doc3]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# Inicializar un divisor de texto que respeta los tokens del modelo\n",
    "# RecursiveCharacterTextSplitter divide el texto de manera inteligente\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    # Tamaño de cada chunk en tokens (250 tokens ≈ 187 palabras)\n",
    "    chunk_size=250,\n",
    "    # Sin superposición entre chunks (overlap=0)\n",
    "    # Un overlap > 0 ayudaría a mantener contexto entre chunks\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "# Dividir todos los documentos en chunks más pequeños\n",
    "# Esto es crucial para RAG porque permite encontrar fragmentos específicos relevantes\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Crear una base de datos vectorial en memoria con los chunks\n",
    "# Los documentos se convierten a embeddings (vectores numéricos) usando OpenAIEmbeddings\n",
    "vectorstore = InMemoryVectorStore.from_documents(\n",
    "    # Los chunks de documentos que queremos almacenar\n",
    "    documents=doc_splits,\n",
    "    # El modelo de embedding que convierte texto a vectores\n",
    "    # OpenAI usa text-embedding-ada-002 por defecto\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "\n",
    "# Convertir el vectorstore en un retriever (recuperador)\n",
    "# El retriever es responsable de buscar los documentos más relevantes\n",
    "# k=6 significa que recuperará los 6 chunks más similares a la consulta\n",
    "retriever = vectorstore.as_retriever(k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d655eb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar el retriever con una consulta de ejemplo\n",
    "# Esta llamada busca los 6 documentos más relevantes sobre \"agents\" (agentes)\n",
    "# El retriever usa similitud de coseno entre el embedding de la consulta y los embeddings de los documentos\n",
    "retriever.invoke(\"what is agents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506f13c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el modelo de lenguaje para el sistema RAG\n",
    "# Usar init_chat_model permite inicializar modelos de diferentes proveedores con una sintaxis unificada\n",
    "from langchain.chat_models import init_chat_model\n",
    "# Inicializar el modelo GPT-4o-mini de OpenAI\n",
    "# Formato: \"proveedor:modelo\"\n",
    "llm=init_chat_model(\"openai:gpt-4o-mini\")\n",
    "# Mostrar la configuración del modelo\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4fcdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar el decorador traceable para rastrear la ejecución en LangSmith\n",
    "from langsmith import traceable\n",
    "\n",
    "# Decorador @traceable() registra automáticamente cada llamada a esta función en LangSmith\n",
    "# Esto permite monitorear el rendimiento, costos y comportamiento del sistema RAG\n",
    "@traceable()\n",
    "def rag_bot(question:str)->dict:\n",
    "    \"\"\"\n",
    "    Bot RAG que responde preguntas usando documentos recuperados como contexto\n",
    "    \n",
    "    Proceso:\n",
    "    1. Recuperar documentos relevantes usando el retriever\n",
    "    2. Construir un prompt con los documentos como contexto\n",
    "    3. Generar una respuesta usando el LLM\n",
    "    \"\"\"\n",
    "    # PASO 1: Recuperar los documentos más relevantes para la pregunta\n",
    "    # El retriever usa búsqueda de similitud semántica para encontrar los 6 chunks más relevantes\n",
    "    docs=retriever.invoke(question)\n",
    "    \n",
    "    # PASO 2: Concatenar el contenido de todos los documentos recuperados en un solo string\n",
    "    # Cada documento se une con un espacio para crear el contexto\n",
    "    docs_string = \" \".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # PASO 3: Construir las instrucciones del sistema con el contexto de los documentos\n",
    "    # Estas instrucciones guían al modelo a responder basándose solo en los documentos proporcionados\n",
    "    instructions = f\"\"\"You are a helpful assistant who is good at analyzing source information and answering questions.       Use the following source documents to answer the user's questions.       If you don't know the answer, just say that you don't know.       Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Documents:\n",
    "{docs_string}\"\"\"\n",
    "    \n",
    "    # PASO 4: Invocar el LLM con las instrucciones y la pregunta\n",
    "    # El modelo genera una respuesta basándose en el contexto proporcionado\n",
    "    ai_msg=llm.invoke([\n",
    "         # Mensaje del sistema: instrucciones + contexto de documentos\n",
    "         {\"role\": \"system\", \"content\": instructions},\n",
    "         # Mensaje del usuario: la pregunta original\n",
    "         {\"role\": \"user\", \"content\": question},\n",
    "    ])\n",
    "    \n",
    "    # PASO 5: Retornar un diccionario con la respuesta y los documentos usados\n",
    "    # Esto permite evaluar tanto la respuesta como la calidad de la recuperación\n",
    "    return {\"answer\":ai_msg.content,\"documents\":docs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93958f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar el bot RAG con una pregunta sobre agentes\n",
    "# Esta llamada ejecutará todo el flujo: recuperación + generación\n",
    "# El resultado incluirá tanto la respuesta como los documentos usados\n",
    "rag_bot(\"What is agents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8fa531",
   "metadata": {},
   "source": [
    "### Dataset para Evaluación de RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79fc9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar el cliente de LangSmith nuevamente\n",
    "from langsmith import Client\n",
    "\n",
    "# Crear una instancia del cliente\n",
    "client=Client()\n",
    "\n",
    "# Definir los ejemplos para el dataset de evaluación de RAG\n",
    "# Estos ejemplos son preguntas específicas sobre el contenido de los blogs de Lilian Weng\n",
    "# Cada ejemplo tiene una pregunta (input) y una respuesta esperada (output)\n",
    "examples = [\n",
    "    {\n",
    "        # Pregunta 1: Sobre cómo los agentes ReAct usan la auto-reflexión\n",
    "        \"inputs\": {\"question\": \"How does the ReAct agent use self-reflection? \"},\n",
    "        # Respuesta esperada que describe la integración de razonamiento y acción\n",
    "        \"outputs\": {\"answer\": \"ReAct integrates reasoning and acting, performing actions - such tools like Wikipedia search API - and then observing / reasoning about the tool outputs.\"},\n",
    "    },\n",
    "    {\n",
    "        # Pregunta 2: Sobre los tipos de sesgos en few-shot prompting\n",
    "        \"inputs\": {\"question\": \"What are the types of biases that can arise with few-shot prompting?\"},\n",
    "        # Respuesta esperada que enumera tres tipos de sesgos\n",
    "        \"outputs\": {\"answer\": \"The biases that can arise with few-shot prompting include (1) Majority label bias, (2) Recency bias, and (3) Common token bias.\"},\n",
    "    },\n",
    "    {\n",
    "        # Pregunta 3: Sobre tipos de ataques adversarios\n",
    "        \"inputs\": {\"question\": \"What are five types of adversarial attacks?\"},\n",
    "        # Respuesta esperada que lista cinco tipos específicos de ataques\n",
    "        \"outputs\": {\"answer\": \"Five types of adversarial attacks are (1) Token manipulation, (2) Gradient based attack, (3) Jailbreak prompting, (4) Human red-teaming, (5) Model red-teaming.\"},\n",
    "    }\n",
    "]\n",
    "\n",
    "# Crear un nuevo dataset específico para evaluar el sistema RAG\n",
    "dataset_name=\"RAG Test Evaluation\"\n",
    "# Crear el dataset en LangSmith\n",
    "dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "# Agregar los ejemplos al dataset\n",
    "# Esto creará tres casos de prueba en LangSmith con IDs únicos\n",
    "client.create_examples(\n",
    "    # Asociar los ejemplos con el dataset creado\n",
    "    dataset_id=dataset.id,\n",
    "    # Los ejemplos definidos anteriormente\n",
    "    examples=examples\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c28a1bd",
   "metadata": {},
   "source": [
    "### Evaluadores o Métricas para RAG\n",
    "1. **Corrección (Correctness)**: Respuesta vs respuesta de referencia\n",
    "- **Objetivo**: Medir \"qué tan similar/correcta es la respuesta del RAG, en relación con una respuesta ground-truth\"\n",
    "- **Modo**: Requiere una respuesta ground truth (referencia) proporcionada a través de un dataset\n",
    "- **Evaluador**: Usar LLM como juez para evaluar la corrección de la respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dae1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar tipos para definir el esquema de salida estructurada\n",
    "from typing_extensions import Annotated,TypedDict\n",
    "\n",
    "# ===== ESQUEMA DE SALIDA PARA CORRECCIÓN =====\n",
    "\n",
    "# Definir el esquema de salida para la calificación de corrección\n",
    "# TypedDict permite definir la estructura exacta que el LLM debe generar\n",
    "class CorrectnessGrade(TypedDict):\n",
    "    # NOTA: El orden en que se definen los campos es el orden en que el modelo los generará\n",
    "    # Es útil poner explicaciones antes de respuestas porque obliga al modelo a pensar\n",
    "    # antes de generar su respuesta final\n",
    "    \n",
    "    # Campo 1: Explicación del razonamiento (generado primero)\n",
    "    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]\n",
    "    # Campo 2: Calificación booleana de corrección (generado después de la explicación)\n",
    "    correct: Annotated[bool, ..., \"True if the answer is correct, False otherwise.\"]\n",
    "\n",
    "# ===== INSTRUCCIONES PARA EL EVALUADOR DE CORRECCIÓN =====\n",
    "\n",
    "# Prompt detallado para el LLM evaluador que actúa como profesor\n",
    "correctness_instructions = \"\"\"You are a teacher grading a quiz. \n",
    "\n",
    "You will be given a QUESTION, the GROUND TRUTH (correct) ANSWER, and the STUDENT ANSWER. \n",
    "\n",
    "Here is the grade criteria to follow:\n",
    "(1) Grade the student answers based ONLY on their factual accuracy relative to the ground truth answer. \n",
    "(2) Ensure that the student answer does not contain any conflicting statements.\n",
    "(3) It is OK if the student answer contains more information than the ground truth answer, as long as it is factually accurate relative to the  ground truth answer.\n",
    "\n",
    "Correctness:\n",
    "A correctness value of True means that the student's answer meets all of the criteria.\n",
    "A correctness value of False means that the student's answer does not meet all of the criteria.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "\n",
    "Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "# Importar ChatOpenAI para crear el modelo evaluador\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Crear un LLM evaluador con salida estructurada\n",
    "# with_structured_output fuerza al modelo a generar JSON siguiendo el esquema CorrectnessGrade\n",
    "grader_llm=ChatOpenAI(model=\"gpt-4o-mini\",temperature=0).with_structured_output(\n",
    "    CorrectnessGrade,  # El esquema que debe seguir la salida\n",
    "    method=\"json_schema\",  # Método de generación estructurada\n",
    "    strict=True  # Modo estricto: la salida DEBE seguir exactamente el esquema\n",
    ")\n",
    "\n",
    "# ===== FUNCIÓN EVALUADORA DE CORRECCIÓN =====\n",
    "\n",
    "# Función evaluadora para la precisión de respuestas RAG\n",
    "def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Evaluador para la precisión de respuestas RAG\n",
    "    \n",
    "    Parámetros:\n",
    "    - inputs: diccionario con la pregunta original\n",
    "    - outputs: diccionario con la respuesta generada por el RAG\n",
    "    - reference_outputs: diccionario con la respuesta correcta esperada\n",
    "    \n",
    "    Retorna:\n",
    "    - bool: True si la respuesta es correcta, False en caso contrario\n",
    "    \"\"\"\n",
    "    # Construir el mensaje con la pregunta, respuesta correcta y respuesta del estudiante\n",
    "    answers = f\"\"\"\\\n",
    "QUESTION: {inputs['question']}\n",
    "GROUND TRUTH ANSWER: {reference_outputs['answer']}\n",
    "STUDENT ANSWER: {outputs['answer']}\"\"\"\n",
    "\n",
    "    # Ejecutar el evaluador LLM con las instrucciones y el contenido a evaluar\n",
    "    grade = grader_llm.invoke([\n",
    "        # Mensaje del sistema con instrucciones de cómo calificar\n",
    "        {\"role\": \"system\", \"content\": correctness_instructions}, \n",
    "        # Mensaje del usuario con la pregunta y respuestas a comparar\n",
    "        {\"role\": \"user\", \"content\": answers}\n",
    "    ])\n",
    "    \n",
    "    # Retornar solo el valor booleano de corrección\n",
    "    # grade es un diccionario con estructura {\"explanation\": \"...\", \"correct\": True/False}\n",
    "    return grade[\"correct\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caee8a4",
   "metadata": {},
   "source": [
    "### Relevancia: Respuesta vs entrada\n",
    "El flujo es similar al anterior, pero simplemente observamos las entradas y salidas sin necesitar las reference_outputs. Sin una respuesta de referencia no podemos calificar la precisión, pero aún podemos calificar la relevancia—es decir, si el modelo abordó la pregunta del usuario o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab9f60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== EVALUADOR DE RELEVANCIA =====\n",
    "# Este evaluador mide si la respuesta es relevante para la pregunta\n",
    "# No requiere una respuesta de referencia, solo evalúa si se abordó la pregunta\n",
    "\n",
    "# Esquema de salida para la calificación de relevancia\n",
    "class RelevanceGrade(TypedDict):\n",
    "    # Explicación del razonamiento (generada primero para forzar al modelo a pensar)\n",
    "    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]\n",
    "    # Calificación booleana de relevancia\n",
    "    relevant: Annotated[bool, ..., \"Provide the score on whether the answer addresses the question\"]\n",
    "\n",
    "# Instrucciones del prompt para evaluar relevancia\n",
    "relevance_instructions=\"\"\"You are a teacher grading a quiz. \n",
    "\n",
    "You will be given a QUESTION and a STUDENT ANSWER. \n",
    "\n",
    "Here is the grade criteria to follow:\n",
    "(1) Ensure the STUDENT ANSWER is concise and relevant to the QUESTION\n",
    "(2) Ensure the STUDENT ANSWER helps to answer the QUESTION\n",
    "\n",
    "Relevance:\n",
    "A relevance value of True means that the student's answer meets all of the criteria.\n",
    "A relevance value of False means that the student's answer does not meet all of the criteria.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "\n",
    "Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "# LLM evaluador de relevancia con salida estructurada\n",
    "# Usar GPT-4o para evaluaciones más precisas de relevancia\n",
    "relevance_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(\n",
    "    RelevanceGrade,  # Esquema de salida\n",
    "    method=\"json_schema\",  # Método de estructuración\n",
    "    strict=True  # Modo estricto\n",
    ")\n",
    "\n",
    "# Función evaluadora de relevancia\n",
    "def relevance(inputs: dict, outputs: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Evaluador simple para la utilidad de respuestas RAG\n",
    "    \n",
    "    Parámetros:\n",
    "    - inputs: diccionario con la pregunta original\n",
    "    - outputs: diccionario con la respuesta generada\n",
    "    \n",
    "    Retorna:\n",
    "    - bool: True si la respuesta es relevante, False en caso contrario\n",
    "    \"\"\"\n",
    "    # Construir el mensaje con la pregunta y la respuesta del estudiante\n",
    "    answer = f\"QUESTION: {inputs['question']}\\nSTUDENT ANSWER: {outputs['answer']}\"\n",
    "    \n",
    "    # Invocar el LLM evaluador\n",
    "    grade = relevance_llm.invoke([\n",
    "        # Instrucciones del sistema\n",
    "        {\"role\": \"system\", \"content\": relevance_instructions}, \n",
    "        # Contenido a evaluar\n",
    "        {\"role\": \"user\", \"content\": answer}\n",
    "    ])\n",
    "    \n",
    "    # Retornar el valor booleano de relevancia\n",
    "    return grade[\"relevant\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b97886",
   "metadata": {},
   "source": [
    "### Fundamentación (Groundedness): Respuesta vs documentos recuperados\n",
    "Otra forma útil de evaluar respuestas sin necesitar respuestas de referencia es verificar si la respuesta está justificada por (o \"fundamentada en\") los documentos recuperados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87044454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== EVALUADOR DE FUNDAMENTACIÓN (GROUNDEDNESS) =====\n",
    "# Este evaluador verifica si la respuesta está basada en los documentos recuperados\n",
    "# Detecta \"alucinaciones\" donde el modelo inventa información no presente en los documentos\n",
    "\n",
    "# Esquema de salida para la calificación de fundamentación\n",
    "class GroundedGrade(TypedDict):\n",
    "    # Explicación del razonamiento\n",
    "    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]\n",
    "    # Calificación booleana: True si está fundamentada, False si alucina\n",
    "    grounded: Annotated[bool, ..., \"Provide the score on if the answer hallucinates from the documents\"]\n",
    "\n",
    "# Instrucciones del prompt para evaluar fundamentación\n",
    "grounded_instructions = \"\"\"You are a teacher grading a quiz. \n",
    "\n",
    "You will be given FACTS and a STUDENT ANSWER. \n",
    "\n",
    "Here is the grade criteria to follow:\n",
    "(1) Ensure the STUDENT ANSWER is grounded in the FACTS. \n",
    "(2) Ensure the STUDENT ANSWER does not contain \"hallucinated\" information outside the scope of the FACTS.\n",
    "\n",
    "Grounded:\n",
    "A grounded value of True means that the student's answer meets all of the criteria.\n",
    "A grounded value of False means that the student's answer does not meet all of the criteria.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "\n",
    "Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "# LLM evaluador de fundamentación con salida estructurada\n",
    "grounded_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(\n",
    "    GroundedGrade,  # Esquema de salida\n",
    "    method=\"json_schema\",\n",
    "    strict=True\n",
    ")\n",
    "\n",
    "# Función evaluadora de fundamentación\n",
    "def groundedness(inputs: dict, outputs: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Evaluador simple para la fundamentación de respuestas RAG\n",
    "    \n",
    "    Verifica si la respuesta está basada en los documentos recuperados\n",
    "    o si el modelo está \"alucinando\" información no presente en ellos\n",
    "    \n",
    "    Parámetros:\n",
    "    - inputs: diccionario con la pregunta (no usado en esta evaluación)\n",
    "    - outputs: diccionario con la respuesta y los documentos recuperados\n",
    "    \n",
    "    Retorna:\n",
    "    - bool: True si está fundamentada, False si alucina\n",
    "    \"\"\"\n",
    "    # Concatenar el contenido de todos los documentos recuperados\n",
    "    # Estos son los \"FACTS\" que el modelo debería haber usado\n",
    "    doc_string = \"\\n\\n\".join(doc.page_content for doc in outputs[\"documents\"])\n",
    "    \n",
    "    # Construir el mensaje con los hechos y la respuesta del estudiante\n",
    "    answer = f\"FACTS: {doc_string}\\nSTUDENT ANSWER: {outputs['answer']}\"\n",
    "    \n",
    "    # Invocar el LLM evaluador\n",
    "    grade = grounded_llm.invoke([\n",
    "        {\"role\": \"system\", \"content\": grounded_instructions},\n",
    "        {\"role\": \"user\", \"content\": answer}\n",
    "    ])\n",
    "    \n",
    "    # Retornar el valor booleano de fundamentación\n",
    "    return grade[\"grounded\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aaaa0b",
   "metadata": {},
   "source": [
    "### Relevancia de Recuperación: Documentos recuperados vs entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36ac507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== EVALUADOR DE RELEVANCIA DE RECUPERACIÓN =====\n",
    "# Este evaluador mide si los documentos recuperados son relevantes para la pregunta\n",
    "# Es crucial para evaluar la calidad del componente de recuperación del sistema RAG\n",
    "\n",
    "# Esquema de salida para la calificación de relevancia de recuperación\n",
    "class RetrievalRelevanceGrade(TypedDict):\n",
    "    # Explicación del razonamiento\n",
    "    explanation: Annotated[str, ..., \"Explain your reasoning for the score\"]\n",
    "    # Calificación booleana: True si los documentos son relevantes\n",
    "    relevant: Annotated[bool, ..., \"True if the retrieved documents are relevant to the question, False otherwise\"]\n",
    "\n",
    "# Instrucciones del prompt para evaluar relevancia de recuperación\n",
    "retrieval_relevance_instructions = \"\"\"You are a teacher grading a quiz. \n",
    "\n",
    "You will be given a QUESTION and a set of FACTS provided by the student. \n",
    "\n",
    "Here is the grade criteria to follow:\n",
    "(1) You goal is to identify FACTS that are completely unrelated to the QUESTION\n",
    "(2) If the facts contain ANY keywords or semantic meaning related to the question, consider them relevant\n",
    "(3) It is OK if the facts have SOME information that is unrelated to the question as long as (2) is met\n",
    "\n",
    "Relevance:\n",
    "A relevance value of True means that the FACTS contain ANY keywords or semantic meaning related to the QUESTION and are therefore relevant.\n",
    "A relevance value of False means that the FACTS are completely unrelated to the QUESTION.\n",
    "\n",
    "Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n",
    "\n",
    "Avoid simply stating the correct answer at the outset.\"\"\"\n",
    "\n",
    "# LLM evaluador de relevancia de recuperación con salida estructurada\n",
    "retrieval_relevance_llm = ChatOpenAI(model=\"gpt-4o\", temperature=0).with_structured_output(\n",
    "    RetrievalRelevanceGrade,  # Esquema de salida\n",
    "    method=\"json_schema\",\n",
    "    strict=True\n",
    ")\n",
    "\n",
    "# Función evaluadora de relevancia de recuperación\n",
    "def retrieval_relevance(inputs: dict, outputs: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Evaluador para la relevancia de documentos recuperados\n",
    "    \n",
    "    Mide si el retriever está recuperando documentos relevantes\n",
    "    para responder la pregunta del usuario\n",
    "    \n",
    "    Parámetros:\n",
    "    - inputs: diccionario con la pregunta original\n",
    "    - outputs: diccionario con los documentos recuperados\n",
    "    \n",
    "    Retorna:\n",
    "    - bool: True si los documentos son relevantes, False si no lo son\n",
    "    \"\"\"\n",
    "    # Concatenar el contenido de todos los documentos recuperados\n",
    "    # Estos son los \"FACTS\" que el retriever encontró\n",
    "    doc_string = \"\\n\\n\".join(doc.page_content for doc in outputs[\"documents\"])\n",
    "    \n",
    "    # Construir el mensaje con los hechos y la pregunta\n",
    "    answer = f\"FACTS: {doc_string}\\nQUESTION: {inputs['question']}\"\n",
    "\n",
    "    # Ejecutar el evaluador LLM\n",
    "    grade = retrieval_relevance_llm.invoke([\n",
    "        # Instrucciones del sistema\n",
    "        {\"role\": \"system\", \"content\": retrieval_relevance_instructions}, \n",
    "        # Contenido a evaluar\n",
    "        {\"role\": \"user\", \"content\": answer}\n",
    "    ])\n",
    "    \n",
    "    # Retornar el valor booleano de relevancia\n",
    "    return grade[\"relevant\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28f88ec",
   "metadata": {},
   "source": [
    "### Ejecutar la evaluación completa del sistema RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc096fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== EJECUTAR EVALUACIÓN COMPLETA DEL SISTEMA RAG =====\n",
    "\n",
    "# Función objetivo que envuelve el bot RAG para la evaluación\n",
    "def target(inputs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Función wrapper que adapta rag_bot al formato esperado por client.evaluate\n",
    "    \n",
    "    Parámetros:\n",
    "    - inputs: diccionario con la pregunta del dataset\n",
    "    \n",
    "    Retorna:\n",
    "    - dict: respuesta y documentos del bot RAG\n",
    "    \"\"\"\n",
    "    return rag_bot(inputs[\"question\"])\n",
    "\n",
    "# Ejecutar la evaluación completa con múltiples evaluadores\n",
    "experiment_results = client.evaluate(\n",
    "    # La función objetivo (sistema RAG) que se evaluará\n",
    "    target,\n",
    "    # El dataset de prueba con preguntas y respuestas esperadas\n",
    "    data=dataset_name,\n",
    "    # Lista de CUATRO evaluadores que miden diferentes aspectos:\n",
    "    # 1. correctness: ¿La respuesta es correcta vs la referencia?\n",
    "    # 2. groundedness: ¿La respuesta está basada en los documentos recuperados?\n",
    "    # 3. relevance: ¿La respuesta es relevante para la pregunta?\n",
    "    # 4. retrieval_relevance: ¿Los documentos recuperados son relevantes?\n",
    "    evaluators=[correctness, groundedness, relevance, retrieval_relevance],\n",
    "    # Prefijo para identificar este experimento en LangSmith\n",
    "    experiment_prefix=\"rag-doc-relevance\",\n",
    "    # Metadatos adicionales para documentar la configuración del experimento\n",
    "    metadata={\"version\": \"LCEL context, gpt-4-0125-preview\"},\n",
    ")\n",
    "\n",
    "# Convertir los resultados a un DataFrame de pandas para análisis local\n",
    "# Esto permite visualizar y analizar los resultados directamente en el notebook\n",
    "experiment_results.to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGUdemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
