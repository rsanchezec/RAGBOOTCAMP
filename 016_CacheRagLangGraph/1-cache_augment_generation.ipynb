{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a86aa13",
   "metadata": {},
   "source": "### ¿Qué es la Generación Aumentada con Caché (CAG)?\n\nCAG (Cache-Augmented Generation) es un enfoque sin recuperación que omite el paso habitual de consultar fuentes de conocimiento externas en tiempo de inferencia. En su lugar, precarga documentos relevantes en la ventana de contexto extendida del LLM, precalcula la caché de pares clave-valor (KV) del modelo, y reutiliza esto durante la inferencia, de modo que el modelo puede generar respuestas sin pasos adicionales de recuperación."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e6e537",
   "metadata": {},
   "outputs": [],
   "source": "# Importamos las librerías necesarias para configuración del entorno\nimport os  # Para manejar variables de entorno del sistema operativo\nfrom dotenv import load_dotenv  # Para cargar variables desde archivo .env\n\n# Cargamos las variables de entorno desde el archivo .env\nload_dotenv()\n\n# Configuramos la API key de OpenAI desde las variables de entorno\n# Esta clave es necesaria para autenticarnos con el servicio de OpenAI\nos.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n\n# Importamos la función para inicializar modelos de chat de LangChain\nfrom langchain.chat_models import init_chat_model\n\n# Inicializamos el modelo de lenguaje GPT-4o-mini de OpenAI\n# Este es un modelo más económico y rápido que GPT-4, ideal para demostraciones\nllm = init_chat_model(\"openai:gpt-4o-mini\")\n\n# Mostramos la configuración del modelo para verificar que se inicializó correctamente\nllm"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d950ef",
   "metadata": {},
   "outputs": [],
   "source": "### Variable de Caché\n\n# Creamos un diccionario vacío para almacenar las respuestas cacheadas\n# La clave será la consulta del usuario y el valor será la respuesta del modelo\n# Este es un caché simple en memoria que persiste mientras el notebook está en ejecución\nModel_Cache = {}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48802156",
   "metadata": {},
   "outputs": [],
   "source": "# Importamos el módulo time para medir tiempos de ejecución\nimport time\n\ndef cache_model(query):\n    \"\"\"\n    Función que implementa un sistema de caché simple para el modelo.\n    Si la consulta ya existe en caché, retorna la respuesta guardada.\n    Si no existe, ejecuta el modelo y guarda el resultado en caché.\n    \"\"\"\n    # Registramos el tiempo de inicio\n    start_time = time.time()\n    \n    # Verificamos si la consulta ya existe en el caché\n    if Model_Cache.get(query):\n        # Si hay un cache hit (acierto), imprimimos un mensaje\n        print(\"**ACIERTO DE CACHÉ**\")\n        \n        # Calculamos el tiempo transcurrido (será casi 0 para cache hits)\n        end_time = time.time()\n        elapsed_time = end_time - start_time\n        \n        # Mostramos el tiempo de ejecución\n        print(f\"TIEMPO DE EJECUCIÓN: {elapsed_time:.2f} segundos\")\n        \n        # Retornamos la respuesta del caché sin llamar al modelo\n        return Model_Cache.get(query)\n    else:\n        # Si no hay cache hit (fallo de caché), debemos ejecutar el modelo\n        print(\"***FALLO DE CACHÉ – EJECUTANDO MODELO***\")\n        \n        # Registramos el tiempo de inicio de la llamada al modelo\n        start_time = time.time()\n        \n        # Invocamos el modelo LLM con la consulta\n        # Esta es la operación costosa que queremos evitar con el caché\n        response = llm.invoke(query)\n        \n        # Calculamos el tiempo que tomó la llamada al modelo\n        end_time = time.time()\n        elapsed = end_time - start_time\n        \n        # Mostramos el tiempo de ejecución (será varios segundos)\n        print(f\"TIEMPO DE EJECUCIÓN: {elapsed:.2f} segundos\")\n        \n        # Guardamos la respuesta en el caché para futuras consultas idénticas\n        Model_Cache[query] = response\n        \n        # Retornamos la respuesta\n        return response"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b3c6ff",
   "metadata": {},
   "outputs": [],
   "source": "# Primera llamada con la consulta \"hi\"\n# Esta será un fallo de caché porque es la primera vez que hacemos esta pregunta\nresponse = cache_model(\"hi\")\n\n# Mostramos la respuesta completa del modelo\nresponse"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6407acf4",
   "metadata": {},
   "outputs": [],
   "source": "# Inspeccionamos el contenido del diccionario de caché\n# Después de la primera llamada, ahora contiene una entrada para \"hi\"\nModel_Cache"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32188896",
   "metadata": {},
   "outputs": [],
   "source": "# Segunda llamada con la misma consulta \"hi\"\n# Esta vez será un acierto de caché porque ya existe la respuesta guardada\n# Notarás que el tiempo de ejecución es prácticamente 0 segundos\nresponse = cache_model(\"hi\")\n\n# Mostramos la respuesta (será exactamente la misma que la primera vez)\nresponse"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d65ff0a",
   "metadata": {},
   "outputs": [],
   "source": "# Probamos con una consulta más compleja que requiere una respuesta extensa\nquery = \"¿puedes darme 500 palabras sobre langgraph?\"\n\n# Primera vez con esta consulta = fallo de caché\n# El modelo tomará varios segundos para generar 500 palabras\nresponse = cache_model(query)\n\n# Imprimimos la respuesta generada\nprint(response)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37e3685",
   "metadata": {},
   "outputs": [],
   "source": "# Repetimos la misma consulta exacta\nquery = \"¿puedes darme 500 palabras sobre langgraph?\"\n\n# Esta vez será un acierto de caché\n# La respuesta será instantánea (~0.00 segundos) comparado con los 17+ segundos anteriores\n# Esto demuestra el beneficio del caché: ahorro de tiempo y costo de API\nresponse = cache_model(query)\n\n# Imprimimos la respuesta (idéntica a la anterior pero recuperada del caché)\nprint(response)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bc19f8",
   "metadata": {},
   "outputs": [],
   "source": "# Probamos con una consulta MUY SIMILAR pero no idéntica\n# Nota: eliminamos \"can you\" del inicio de la pregunta\nquery = \"dame 500 palabras sobre langgraph?\"\n\n# Esto será un FALLO de caché porque el string es diferente\n# Este es el problema del caché simple basado en diccionarios:\n# - Solo funciona con coincidencias exactas de texto\n# - No entiende similitud semántica\n# - \"can you give me\" vs \"give me\" se tratan como consultas completamente diferentes\nresponse = cache_model(query)\n\n# El modelo generará una nueva respuesta (tomará ~12+ segundos)\nprint(response)"
  },
  {
   "cell_type": "markdown",
   "id": "5083b6b7",
   "metadata": {},
   "source": "### CAG Avanzado\n\nEn esta sección implementamos un sistema de caché semántico más sofisticado que utiliza embeddings vectoriales para detectar preguntas similares, no solo idénticas."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf663496",
   "metadata": {},
   "outputs": [],
   "source": "# Importaciones para el sistema CAG avanzado\n\n# Importación para compatibilidad con anotaciones de tipo\nfrom __future__ import annotations\n\n# Tipos para definir estructuras de datos con type hints\nfrom typing import TypedDict, List, Optional\n\n# Módulo time para gestión de TTL (Time To Live) del caché\nimport time\n\n# ---- LangGraph / LangChain ----\n# StateGraph: para construir flujos de trabajo basados en grafos de estado\n# END: constante que marca el final del grafo\nfrom langgraph.graph import StateGraph, END\n\n# MemorySaver: gestor de checkpoints para persistencia de estado\nfrom langgraph.checkpoint.memory import MemorySaver\n\n# Document: clase que representa un documento con contenido y metadatos\nfrom langchain_core.documents import Document\n\n# ChatOpenAI: cliente para modelos de chat de OpenAI\nfrom langchain_openai import ChatOpenAI\n\n# HuggingFaceEmbeddings: para generar embeddings usando modelos de Hugging Face\nfrom langchain.embeddings import HuggingFaceEmbeddings\n\n# ---- FAISS vector stores ----\n# FAISS: biblioteca de Facebook para búsqueda de similitud vectorial eficiente\nimport faiss\n\n# FAISS wrapper de LangChain con funcionalidad adicional\nfrom langchain_community.vectorstores import FAISS\n\n# InMemoryDocstore: almacén de documentos en memoria\nfrom langchain_community.docstore.in_memory import InMemoryDocstore"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb0e1ef",
   "metadata": {},
   "outputs": [],
   "source": "# ================= CONFIGURACIÓN =================\n\n# Modelo de embeddings: usamos un modelo compacto de sentence-transformers\n# Este modelo genera vectores de 384 dimensiones\nEMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  \n\n# Dimensionalidad de los vectores de embedding (debe coincidir con el modelo)\nVECTOR_DIM = 384\n\n# Modelo LLM a utilizar: GPT-4o-mini es rápido y económico\nLLM_MODEL = \"gpt-4o-mini\"\n\n# Temperatura del LLM: 0 = determinístico, respuestas consistentes\nLLM_TEMPERATURE = 0\n\n# Número de documentos a recuperar del vector store RAG\nRETRIEVE_TOP_K = 4\n\n# Número de resultados a revisar del caché semántico\nCACHE_TOP_K = 3\n\n# Umbral de distancia para considerar un acierto de caché\n# Valores más bajos = mayor similitud requerida\n# FAISS usa distancia L2: valores más bajos indican mayor similitud\nCACHE_DISTANCE_THRESHOLD = 0.45\n\n# TTL (Time To Live) opcional para entradas de caché en segundos\n# 0 = deshabilitado (el caché nunca expira)\nCACHE_TTL_SEC = 0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298cde63",
   "metadata": {},
   "outputs": [],
   "source": "# ================= ESTADO DEL GRAFO =================\n\nclass RAGState(TypedDict):\n    \"\"\"\n    Define el estado que se pasa entre nodos del grafo LangGraph.\n    Cada campo representa una parte del flujo de trabajo RAG con caché.\n    \"\"\"\n    # Pregunta original del usuario (sin modificar)\n    question: str\n    \n    # Pregunta normalizada (minúsculas, limpia) para búsqueda en caché\n    normalized_question: str\n    \n    # Lista de documentos recuperados del vector store\n    context_docs: List[Document]\n    \n    # Respuesta generada por el LLM o recuperada del caché\n    answer: Optional[str]\n    \n    # Lista de citas que referencian las fuentes utilizadas\n    citations: List[str]\n    \n    # Bandera que indica si la respuesta provino del caché\n    cache_hit: bool"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55558c9a",
   "metadata": {},
   "outputs": [],
   "source": "# ============== VARIABLES GLOBALES ===================\n\n# Importamos HuggingFaceEmbeddings para generar embeddings\nfrom langchain_huggingface import HuggingFaceEmbeddings\n\n# Inicializamos el modelo de embeddings\n# Este modelo se descargará la primera vez que se ejecute (~90MB)\n# Convierte texto en vectores de 384 dimensiones que capturan significado semántico\nEMBED = HuggingFaceEmbeddings(model_name=EMBED_MODEL)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9791f01",
   "metadata": {},
   "outputs": [],
   "source": "# ----- CACHÉ DE PREGUNTAS/RESPUESTAS (INICIALIZACIÓN VACÍA) -----\n\n# Creamos un índice FAISS con distancia L2 (Euclidiana)\n# L2: menor distancia = mayor similitud\n# El índice está vacío inicialmente (se llenará con el uso)\nqa_index = faiss.IndexFlatL2(VECTOR_DIM)\n\n# Envolvemos el índice FAISS en el wrapper de LangChain para funcionalidad adicional\nQA_CACHE = FAISS(\n    # Función de embedding para convertir texto a vectores\n    embedding_function=EMBED,\n    \n    # El índice FAISS que acabamos de crear\n    index=qa_index,\n    \n    # Docstore vacío que almacenará los metadatos de las consultas\n    docstore=InMemoryDocstore({}),\n    \n    # Mapeo vacío de IDs de índice a IDs de docstore\n    index_to_docstore_id={}\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff36b896",
   "metadata": {},
   "outputs": [],
   "source": "# Verificamos que el caché QA se inicializó correctamente\n# Mostrará el objeto FAISS con su configuración\nQA_CACHE"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080fa22e",
   "metadata": {},
   "outputs": [],
   "source": "# ----- ALMACÉN RAG (SOLO PARA DEMOSTRACIÓN) -----\n\n# Creamos un vector store FAISS con documentos de ejemplo sobre LangGraph\n# En producción, esto contendría tu base de conocimiento completa\nRAG_STORE = FAISS.from_texts(\n    # Lista de textos de ejemplo que servirán como nuestra base de conocimiento\n    texts=[\n        \"LangGraph te permite componer flujos de trabajo de LLM con estado como grafos.\",\n        \"En LangGraph, los nodos pueden ser cacheados; el caché de nodos memoriza salidas indexadas por entradas durante un TTL.\",\n        \"Retrieval-Augmented Generation (RAG) recupera contexto externo y lo inyecta en prompts.\",\n        \"El caché semántico reutiliza respuestas previas cuando nuevas preguntas son semánticamente similares.\"\n    ],\n    # Usamos el mismo modelo de embeddings para mantener consistencia\n    embedding=EMBED,\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cc311a",
   "metadata": {},
   "outputs": [],
   "source": "# Inicializamos el modelo de lenguaje OpenAI con la configuración definida\nLLM = ChatOpenAI(\n    model=LLM_MODEL,           # Modelo a usar (gpt-4o-mini)\n    temperature=LLM_TEMPERATURE # Temperatura 0 = respuestas determinísticas\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43f16f6",
   "metadata": {},
   "outputs": [],
   "source": "# Verificamos la configuración del LLM\nLLM"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997e26d3",
   "metadata": {},
   "outputs": [],
   "source": "# ================ FUNCIONES DE NODOS DEL GRAFO ===================\n\ndef normalize_query(state: RAGState) -> RAGState:\n    \"\"\"\n    Normaliza la pregunta del usuario para mejorar la búsqueda en caché.\n    Convierte a minúsculas y elimina espacios en blanco innecesarios.\n    \"\"\"\n    # Obtenemos la pregunta del estado, o string vacío si no existe\n    q = (state[\"question\"] or \"\").strip()\n    \n    # Normalizamos a minúsculas para búsqueda case-insensitive\n    state[\"normalized_question\"] = q.lower()\n    \n    return state\n\n\ndef semantic_cache_lookup(state: RAGState) -> RAGState:\n    \"\"\"\n    Busca en el caché semántico si existe una pregunta similar previa.\n    Usa embeddings vectoriales y similitud por distancia L2.\n    \"\"\"\n    # Obtenemos la pregunta normalizada\n    q = state[\"normalized_question\"]\n    \n    # Por defecto, asumimos que no hay acierto de caché\n    state[\"cache_hit\"] = False\n\n    # Si la pregunta está vacía, no hay nada que buscar\n    if not q:\n        return state\n\n    # ✅ Protección: FAISS falla si el índice está vacío y pedimos k>0\n    # Verificamos que el índice existe y tiene al menos un vector\n    if getattr(QA_CACHE, \"index\", None) is None or QA_CACHE.index.ntotal == 0:\n        return state\n\n    # Buscamos las k preguntas más similares en el caché\n    # similarity_search_with_score retorna (Document, distance) con menor=mejor\n    hits = QA_CACHE.similarity_search_with_score(q, k=CACHE_TOP_K)\n    \n    # Si no hay resultados, retornamos sin cambios\n    if not hits:\n        return state\n\n    # Obtenemos el mejor resultado (menor distancia = mayor similitud)\n    best_doc, dist = hits[0]\n\n    # Verificación opcional de TTL (Time To Live)\n    if CACHE_TTL_SEC > 0:\n        # Obtenemos el timestamp de cuando se guardó en caché\n        ts = best_doc.metadata.get(\"ts\")\n        \n        # Si el timestamp no existe o la entrada expiró, ignoramos el caché\n        if ts is None or (time.time() - float(ts)) > CACHE_TTL_SEC:\n            return state\n\n    # Verificamos si la distancia está por debajo del umbral\n    # Para L2: menor distancia = mayor similitud\n    if dist <= CACHE_DISTANCE_THRESHOLD:\n        # Extraemos la respuesta guardada en los metadatos\n        cached_answer = best_doc.metadata.get(\"answer\")\n        \n        if cached_answer:\n            # ¡Acierto de caché! Usamos la respuesta guardada\n            state[\"answer\"] = cached_answer\n            state[\"citations\"] = [\"(cache)\"]  # Indicamos que vino del caché\n            state[\"cache_hit\"] = True\n\n    return state\n\n\ndef respond_from_cache(state: RAGState) -> RAGState:\n    \"\"\"\n    Nodo pasante que simplemente retorna el estado.\n    La respuesta ya fue establecida por semantic_cache_lookup.\n    \"\"\"\n    return state\n\n\ndef retrieve(state: RAGState) -> RAGState:\n    \"\"\"\n    Recupera documentos relevantes del vector store RAG.\n    Solo se ejecuta si no hubo acierto de caché.\n    \"\"\"\n    # Obtenemos la pregunta normalizada\n    q = state[\"normalized_question\"]\n    \n    # Buscamos los k documentos más similares en el RAG store\n    docs = RAG_STORE.similarity_search(q, k=RETRIEVE_TOP_K)\n    \n    # Guardamos los documentos recuperados en el estado\n    state[\"context_docs\"] = docs\n    \n    return state\n\n\ndef generate(state: RAGState) -> RAGState:\n    \"\"\"\n    Genera una respuesta usando el LLM con el contexto recuperado.\n    Implementa el patrón RAG: usa documentos como contexto para la respuesta.\n    \"\"\"\n    # Obtenemos la pregunta original (sin normalizar)\n    q = state[\"question\"]\n    \n    # Obtenemos los documentos de contexto recuperados\n    docs = state.get(\"context_docs\", [])\n    \n    # Formateamos el contexto concatenando todos los documentos\n    # Añadimos marcadores [doc-i] para referencias\n    ctx = \"\\n\\n\".join([f\"[doc-{i}] {d.page_content}\" for i, d in enumerate(docs, start=1)])\n\n    # Mensaje del sistema que define el rol y comportamiento del asistente\n    system = (\n        \"Eres un asistente RAG preciso. Usa el contexto cuando sea útil. \"\n        \"Cita con marcadores [doc-i] si usas un hecho del contexto.\"\n    )\n    \n    # Mensaje del usuario con la pregunta y el contexto\n    user = f\"Pregunta: {q}\\n\\nContexto:\\n{ctx}\\n\\nEscribe una respuesta concisa con citas.\"\n\n    # Invocamos el LLM con los mensajes formateados\n    resp = LLM.invoke([\n        {\"role\": \"system\", \"content\": system},\n        {\"role\": \"user\", \"content\": user}\n    ])\n    \n    # Guardamos la respuesta generada\n    state[\"answer\"] = resp.content\n    \n    # Creamos las citas para todos los documentos usados\n    state[\"citations\"] = [f\"[doc-{i}]\" for i in range(1, len(docs) + 1)]\n    \n    return state\n\n\ndef cache_write(state: RAGState) -> RAGState:\n    \"\"\"\n    Escribe la pregunta y respuesta en el caché semántico.\n    Permite reutilizar esta respuesta para preguntas similares futuras.\n    \"\"\"\n    # Obtenemos la pregunta normalizada y la respuesta\n    q = state[\"normalized_question\"]\n    a = state.get(\"answer\")\n    \n    # Solo guardamos si tenemos tanto pregunta como respuesta\n    if not q or not a:\n        return state\n\n    # Añadimos la pregunta al caché FAISS\n    QA_CACHE.add_texts(\n        # El texto es la pregunta (se convertirá a embedding)\n        texts=[q],\n        \n        # Los metadatos contienen la respuesta y timestamp\n        metadatas=[{\n            \"answer\": a,           # La respuesta a cachear\n            \"ts\": time.time(),     # Timestamp para TTL\n        }]\n    )\n    \n    return state"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462f44af",
   "metadata": {},
   "outputs": [],
   "source": "# ============== CONSTRUCCIÓN DEL GRAFO ==============\n\n# Creamos un grafo de estado usando RAGState como esquema\ngraph = StateGraph(RAGState)\n\n# Añadimos todos los nodos al grafo\n# Cada nodo es una función que transforma el estado\ngraph.add_node(\"normalize_query\", normalize_query)\ngraph.add_node(\"semantic_cache_lookup\", semantic_cache_lookup)\ngraph.add_node(\"respond_from_cache\", respond_from_cache)\ngraph.add_node(\"retrieve\", retrieve)\ngraph.add_node(\"generate\", generate)\ngraph.add_node(\"cache_write\", cache_write)\n\n# Definimos el punto de entrada: siempre empezamos normalizando la pregunta\ngraph.set_entry_point(\"normalize_query\")\n\n# Después de normalizar, buscamos en el caché\ngraph.add_edge(\"normalize_query\", \"semantic_cache_lookup\")\n\n# Función de decisión para el edge condicional\ndef _branch(state: RAGState) -> str:\n    \"\"\"\n    Decide el siguiente nodo basándose en si hubo acierto de caché.\n    - Si cache_hit = True -> responder desde caché (saltar RAG)\n    - Si cache_hit = False -> ejecutar flujo RAG completo\n    \"\"\"\n    return \"respond_from_cache\" if state.get(\"cache_hit\") else \"retrieve\"\n\n# Añadimos un edge condicional después de la búsqueda en caché\n# La función _branch decide qué camino tomar\ngraph.add_conditional_edges(\n    \"semantic_cache_lookup\",  # Desde este nodo\n    _branch,                   # Usando esta función de decisión\n    {\n        # Mapeo de valores de retorno a nodos destino\n        \"respond_from_cache\": \"respond_from_cache\",\n        \"retrieve\": \"retrieve\"\n    }\n)\n\n# Si respondemos desde caché, terminamos inmediatamente\ngraph.add_edge(\"respond_from_cache\", END)\n\n# Si no hay caché, vamos a retrieve -> generate -> cache_write -> END\ngraph.add_edge(\"retrieve\", \"generate\")\ngraph.add_edge(\"generate\", \"cache_write\")\ngraph.add_edge(\"cache_write\", END)\n\n# Creamos un gestor de memoria para persistencia de estado (opcional)\nmemory = MemorySaver()\n\n# Compilamos el grafo con el checkpointer\napp = graph.compile(checkpointer=memory)\n\n# Mostramos el grafo compilado\napp"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9bde2c",
   "metadata": {},
   "outputs": [],
   "source": "# ================= DEMOSTRACIÓN ===================\n\nif __name__ == \"__main__\":\n    # Configuración del thread para memoria persistente\n    thread_cfg = {\"configurable\": {\"thread_id\": \"demo-user-1\"}}\n\n    # Primera pregunta: \"¿Qué es LangGraph?\"\n    q1 = \"¿Qué es LangGraph?\"\n    \n    # Invocamos el grafo con la pregunta\n    # Como es la primera vez, será un fallo de caché y ejecutará RAG completo\n    out1 = app.invoke(\n        {\n            \"question\": q1,           # Pregunta del usuario\n            \"context_docs\": [],       # Inicialmente vacío\n            \"citations\": []           # Inicialmente vacío\n        }, \n        thread_cfg\n    )\n    \n    # Imprimimos los resultados\n    print(\"Respuesta:\", out1[\"answer\"])\n    print(\"Citas:\", out1.get(\"citations\"))\n    print(\"¿Acierto de caché?:\", out1.get(\"cache_hit\"))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2039bde5",
   "metadata": {},
   "outputs": [],
   "source": "# Segunda pregunta: pregunta SIMILAR pero con diferente redacción\nq1 = \"¿Explica sobre LangGraph?\"\n\n# Esta pregunta es semánticamente similar a \"¿Qué es LangGraph?\"\n# El sistema de caché semántico debería detectar la similitud\n# y retornar la respuesta cacheada sin ejecutar RAG\nout1 = app.invoke(\n    {\n        \"question\": q1, \n        \"context_docs\": [], \n        \"citations\": []\n    }, \n    thread_cfg\n)\n\n# Imprimimos los resultados\nprint(\"Respuesta:\", out1[\"answer\"])\nprint(\"Citas:\", out1.get(\"citations\"))\n# Esperamos cache_hit = True porque la pregunta es similar\nprint(\"¿Acierto de caché?:\", out1.get(\"cache_hit\"))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8d9021",
   "metadata": {},
   "outputs": [],
   "source": "# Tercera pregunta: pregunta DIFERENTE sobre un aspecto específico\nq1 = \"¿Explica sobre los agentes de LangGraph?\"\n\n# Esta pregunta es sobre un tema relacionado pero diferente\n# \"agentes de LangGraph\" es más específico que \"LangGraph\" en general\n# Esperamos un fallo de caché porque la similitud no será suficientemente alta\nout1 = app.invoke(\n    {\n        \"question\": q1, \n        \"context_docs\": [], \n        \"citations\": []\n    }, \n    thread_cfg\n)\n\n# Imprimimos los resultados\nprint(\"Respuesta:\", out1[\"answer\"])\nprint(\"Citas:\", out1.get(\"citations\"))\n# Esperamos cache_hit = False porque la pregunta es suficientemente diferente\nprint(\"¿Acierto de caché?:\", out1.get(\"cache_hit\"))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d890c065",
   "metadata": {},
   "outputs": [],
   "source": "# Cuarta pregunta: reformulación de la pregunta anterior\nq1 = \"¿Explica sobre agentes en Langgraph?\"\n\n# Esta pregunta es casi idéntica a la anterior, solo cambia ligeramente la redacción:\n# - \"agentes de LangGraph\" vs \"agentes en Langgraph\"\n# El caché semántico debería reconocer esta similitud\n# y retornar la respuesta cacheada de la pregunta anterior\nout1 = app.invoke(\n    {\n        \"question\": q1, \n        \"context_docs\": [], \n        \"citations\": []\n    }, \n    thread_cfg\n)\n\n# Imprimimos los resultados\nprint(\"Respuesta:\", out1[\"answer\"])\nprint(\"Citas:\", out1.get(\"citations\"))\n# Esperamos cache_hit = True porque es semánticamente idéntica a la pregunta anterior\nprint(\"¿Acierto de caché?:\", out1.get(\"cache_hit\"))"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}