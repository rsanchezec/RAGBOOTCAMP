{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": "### Base de Datos Vectorial Pinecone\n\n",
   "metadata": {
    "id": "KTayvli5n5oi"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VnRuXb5Pn4op"
   },
   "outputs": [],
   "source": "## Crea tu índice y clave API desde aquí https://www.pinecone.io/\n# Variable para almacenar tu clave API de Pinecone\n# IMPORTANTE: Obtén tu clave API registrándote en https://www.pinecone.io/\n# En producción, esta clave debe estar en variables de entorno, no en el código\napi_key=\"provide your apikey\""
  },
  {
   "cell_type": "code",
   "source": "# Instalar las dependencias necesarias para trabajar con Pinecone y LangChain\n# -qU: opciones para instalar de forma silenciosa (quiet) y actualizar (upgrade)\n# langchain: framework principal para construir aplicaciones con LLMs\n# langchain-pinecone: integración de LangChain con Pinecone para almacenamiento vectorial\n# langchain-openai: integración para usar modelos y embeddings de OpenAI\n!pip install -qU langchain langchain-pinecone langchain-openai",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S4qBMk_ooF7e",
    "outputId": "1640a029-0a6c-41fd-b41b-2c38785719af"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Importar la clase Pinecone para interactuar con la base de datos vectorial de Pinecone\nfrom pinecone import Pinecone\n\n# Crear una instancia del cliente de Pinecone usando la clave API\n# Este objeto 'pc' se usará para crear y gestionar índices en Pinecone\npc=Pinecone(api_key=api_key)",
   "metadata": {
    "id": "Xk3bV02uoNJG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Importar la clase OpenAIEmbeddings para crear embeddings usando modelos de OpenAI\nfrom langchain_openai import OpenAIEmbeddings\n\n# Crear una instancia del modelo de embeddings de OpenAI\n# model: \"text-embedding-3-small\" es un modelo eficiente y económico de embeddings\n# dimensions: 1024 especifica el tamaño del vector (debe coincidir con la dimensión del índice de Pinecone)\n# api_key: clave de API de OpenAI para autenticación (IMPORTANTE: usar variables de entorno en producción)\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\",dimensions=1024,api_key=\"openai_api_key\")",
   "metadata": {
    "id": "6FkmtNbdoNEN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Importar ServerlessSpec para configurar el índice en modo serverless (sin servidor)\nfrom pinecone import ServerlessSpec\n\n# Nombre del índice que se creará en Pinecone (puedes cambiarlo si lo deseas)\nindex_name = \"rag\"  # cambiar si se desea\n\n# Verificar si el índice ya existe en Pinecone\n# Si no existe, crear un nuevo índice con las especificaciones indicadas\nif not pc.has_index(index_name):\n    # Crear el índice en Pinecone\n    pc.create_index(\n        name=index_name,  # Nombre del índice\n        dimension=1024,  # Dimensión de los vectores (debe coincidir con los embeddings)\n        metric=\"cosine\",  # Métrica de similitud: \"cosine\" para similitud del coseno\n        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),  # Configuración serverless en AWS región us-east-1\n    )\n\n# Obtener una referencia al índice (ya sea recién creado o existente)\n# Este objeto 'index' se usará para todas las operaciones en el índice\nindex = pc.Index(index_name)",
   "metadata": {
    "id": "pUEG87c5oNBV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Mostrar el objeto índice para verificar que se conectó correctamente\nindex",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AG6u6hNVoM-Q",
    "outputId": "349a377d-bf69-4e8f-a05e-a37f6ed888b7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Importar PineconeVectorStore para usar Pinecone como almacén de vectores con LangChain\nfrom langchain_pinecone import PineconeVectorStore\n\n# Crear una instancia del vector store que conecta LangChain con Pinecone\n# index: el índice de Pinecone donde se almacenarán los vectores\n# embedding: el modelo de embeddings de OpenAI que convertirá textos en vectores\nvector_store = PineconeVectorStore(index=index, embedding=embeddings)",
   "metadata": {
    "id": "Roi5exMmoM1H"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Mostrar el objeto vector_store para verificar la configuración\nvector_store",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5g2vYCJo1rR",
    "outputId": "86b507f6-62ff-4786-c114-25efc0352c45"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Importar la clase Document para crear objetos de documento con contenido y metadatos\nfrom langchain_core.documents import Document\n\n# Crear el primer documento: Tweet sobre desayuno\ndocument_1 = Document(\n    page_content=\"Desayuné panqueques con chispas de chocolate y huevos revueltos esta mañana.\",\n    metadata={\"source\": \"tweet\"},\n)\n\n# Crear el segundo documento: Noticia sobre el clima\ndocument_2 = Document(\n    page_content=\"El pronóstico del tiempo para mañana es nublado y cubierto, con una máxima de 62 grados.\",\n    metadata={\"source\": \"news\"},\n)\n\n# Crear el tercer documento: Tweet sobre un proyecto con LangChain\ndocument_3 = Document(\n    page_content=\"Construyendo un nuevo proyecto emocionante con LangChain - ¡ven a verlo!\",\n    metadata={\"source\": \"tweet\"},\n)\n\n# Crear el cuarto documento: Noticia sobre un robo bancario\ndocument_4 = Document(\n    page_content=\"Los ladrones asaltaron el banco de la ciudad y robaron 1 millón de dólares en efectivo.\",\n    metadata={\"source\": \"news\"},\n)\n\n# Crear el quinto documento: Tweet sobre una película\ndocument_5 = Document(\n    page_content=\"¡Guau! Esa fue una película increíble. No puedo esperar para verla de nuevo.\",\n    metadata={\"source\": \"tweet\"},\n)\n\n# Crear el sexto documento: Artículo web sobre el iPhone\ndocument_6 = Document(\n    page_content=\"¿Vale la pena el precio del nuevo iPhone? Lee esta reseña para descubrirlo.\",\n    metadata={\"source\": \"website\"},\n)\n\n# Crear el séptimo documento: Artículo web sobre futbolistas\ndocument_7 = Document(\n    page_content=\"Los 10 mejores jugadores de fútbol del mundo en este momento.\",\n    metadata={\"source\": \"website\"},\n)\n\n# Crear el octavo documento: Tweet sobre LangGraph\ndocument_8 = Document(\n    page_content=\"¡LangGraph es el mejor framework para construir aplicaciones con estado y agentes!\",\n    metadata={\"source\": \"tweet\"},\n)\n\n# Crear el noveno documento: Noticia sobre la bolsa de valores\ndocument_9 = Document(\n    page_content=\"El mercado de valores cayó 500 puntos hoy debido a temores de una recesión.\",\n    metadata={\"source\": \"news\"},\n)\n\n# Crear el décimo documento: Tweet con sentimiento negativo\ndocument_10 = Document(\n    page_content=\"Tengo el mal presentimiento de que voy a ser eliminado :(\",\n    metadata={\"source\": \"tweet\"},\n)\n\n# Crear una lista con todos los documentos para facilitar su procesamiento\ndocuments = [\n    document_1,\n    document_2,\n    document_3,\n    document_4,\n    document_5,\n    document_6,\n    document_7,\n    document_8,\n    document_9,\n    document_10,\n]",
   "metadata": {
    "id": "0xg_Yi8Fo2p2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Agregar todos los documentos al vector store de Pinecone\n# Este método convierte cada documento en un vector usando embeddings de OpenAI\n# y los almacena en el índice de Pinecone en la nube\n# Retorna una lista de IDs únicos (UUIDs) generados para cada documento almacenado\nvector_store.add_documents(documents=documents)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7vIFd9vo7RR",
    "outputId": "6dcf8c54-6bc5-4782-8c6f-87d64aa35d2e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "### Consulta Directa\n\n# Realizar una búsqueda de similaridad con filtros en el vector store de Pinecone\n# La consulta busca documentos sobre LangChain y abstracciones para trabajar con LLMs\n# k=2: limita los resultados a los 2 documentos más similares\n# filter={\"source\": \"tweet\"}: solo busca en documentos cuya fuente sea \"tweet\"\nresults = vector_store.similarity_search(\n    \"LangChain proporciona abstracciones para facilitar el trabajo con LLMs\",\n    k=2,\n    filter={\"source\": \"tweet\"},\n)\n# Iterar sobre los resultados e imprimir el contenido y metadatos de cada documento\nfor res in results:\n    print(f\"* {res.page_content} [{res.metadata}]\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SymzuvqVo-_2",
    "outputId": "0cc4a6af-648b-429d-c715-f9f6f57d5b6a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Realizar una búsqueda de similaridad con puntuación de similitud\n# similarity_search_with_score retorna tanto los documentos como su puntuación de similitud\n# La consulta pregunta sobre el clima de mañana\n# k=1: retorna solo el documento más similar\n# filter={\"source\": \"news\"}: filtra solo documentos de noticias\n# La puntuación indica qué tan similar es el documento a la consulta\nresults = vector_store.similarity_search_with_score(\n    \"¿Hará calor mañana?\", k=1, filter={\"source\": \"news\"}\n)\n# Mostrar cada resultado con su puntuación de similitud formateada\nfor res, score in results:\n    print(f\"* [SIM={score:3f}] {res.page_content} [{res.metadata}]\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a-fY5h4JpDN-",
    "outputId": "807d8c23-0e46-49a5-aef8-6f262eda395e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "### Retriever (Recuperador)\n\n# Convertir el vector store en un retriever con búsqueda por umbral de similitud\n# search_type=\"similarity_score_threshold\": solo retorna documentos con similitud >= al umbral\n# search_kwargs: parámetros de búsqueda\n#   - k=1: retorna máximo 1 documento\n#   - score_threshold=0.4: solo retorna documentos con puntuación de similitud >= 0.4\nretriever = vector_store.as_retriever(\n    search_type=\"similarity_score_threshold\",\n    search_kwargs={\"k\": 1, \"score_threshold\": 0.4},\n)\n# Invocar el retriever con una consulta sobre crimen bancario\n# filter={\"source\": \"news\"}: filtra solo documentos de noticias\n# Retorna los documentos que cumplen con el umbral de similitud\nretriever.invoke(\"Robar del banco es un crimen\", filter={\"source\": \"news\"})",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IkcjANiwpFw5",
    "outputId": "4febdd72-e32f-465c-8642-bfcec41db18a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "n6PJuN7ZpJjk"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}