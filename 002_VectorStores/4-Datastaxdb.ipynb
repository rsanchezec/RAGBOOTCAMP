{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": "### ASTRADB VectorStore\nPasa de la idea de aplicación a producción con la Plataforma de IA con Astra DB, la base de datos de ultra-baja latencia hecha para IA y Langflow, el IDE RAG de bajo código.\nhttps://www.datastax.com/",
   "metadata": {
    "id": "rTroqVVzluHj"
   }
  },
  {
   "cell_type": "markdown",
   "source": "### ASTRADB VectorStore",
   "metadata": {
    "id": "CG4O8j5_iWmQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sf3FMT8yiEfx",
    "outputId": "d4e679b7-079a-4d7b-c395-24ec4f882ae7"
   },
   "outputs": [],
   "source": "# Instalar las dependencias necesarias para trabajar con AstraDB y LangChain\n# langchain: framework principal para construir aplicaciones con LLMs (versión 0.3.x)\n# langchain-core: componentes centrales de LangChain (versión 0.3.x)\n# langchain-astradb: integración de LangChain con AstraDB para almacenamiento vectorial (versión 0.6.x)\n!pip install \\\n    \"langchain>=0.3.23,<0.4\" \\\n    \"langchain-core>=0.3.52,<0.4\" \\\n    \"langchain-astradb>=0.6,<0.7\""
  },
  {
   "cell_type": "code",
   "source": "# Instalar el paquete langchain_openai para usar embeddings de OpenAI\n# Este paquete proporciona acceso a los modelos de embeddings de OpenAI\n# que convierten texto en vectores numéricos para búsqueda semántica\n!pip install langchain_openai",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WoR9wNN4iaf3",
    "outputId": "e7704b4f-7b5d-40da-8a5a-1f57134d2673"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "### Configuración\n\n# URL del endpoint de la API de AstraDB\n# Este es el punto de conexión único para tu instancia de base de datos en la nube de DataStax\n# IMPORTANTE: Reemplaza esta URL con la de tu propia instancia de AstraDB\nASTRA_DB_API_ENDPOINT = os.getenv(\"ASTRA_DB_API_ENDPOINT\", \"https://tu-endpoint.apps.astra.datastax.com\")\n\n# Token de autenticación para acceder a AstraDB\n# Este token proporciona credenciales de acceso a tu base de datos AstraDB\n# IMPORTANTE: Este token debe estar en el archivo .env, NO en el código\nASTRA_DB_APPLICATION_TOKEN = os.getenv(\"ASTRA_DB_APPLICATION_TOKEN\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Importar la clase OpenAIEmbeddings para crear embeddings usando modelos de OpenAI\nfrom langchain_openai import OpenAIEmbeddings\n\n# Crear una instancia del modelo de embeddings de OpenAI\n# model: \"text-embedding-3-small\" es un modelo de embeddings eficiente y económico\n# dimensions: 1024 especifica el tamaño del vector resultante (más dimensiones = más precisión pero más costo)\n# La API key se obtiene automáticamente de la variable de entorno OPENAI_API_KEY\nembeddings = OpenAIEmbeddings(\n    model=\"text-embedding-3-small\",\n    dimensions=1024\n)",
   "metadata": {
    "id": "DdbGUdd-iacy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Importar la clase OpenAIEmbeddings para crear embeddings usando modelos de OpenAI\nfrom langchain_openai import OpenAIEmbeddings\n\n# Crear una instancia del modelo de embeddings de OpenAI\n# model: \"text-embedding-3-small\" es un modelo de embeddings eficiente y económico\n# dimensions: 1024 especifica el tamaño del vector resultante (más dimensiones = más precisión pero más costo)\n# api_key: clave de API de OpenAI para autenticación (IMPORTANTE: usar variables de entorno en producción)\nembeddings=OpenAIEmbeddings(model=\"text-embedding-3-small\",dimensions=1024,)",
   "metadata": {
    "id": "aWA0bZTOiaZh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Mostrar la configuración del objeto embeddings\n# Esto permite verificar que el modelo se inicializó correctamente con todos sus parámetros\nembeddings",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0bmQZYiniaWV",
    "outputId": "52a27988-efcd-4ff5-c4db-eff3a7aeec1c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Importar la clase AstraDBVectorStore para usar AstraDB como almacén de vectores\nfrom langchain_astradb import AstraDBVectorStore\n\n# Crear una instancia del vector store conectado a AstraDB\n# embedding: el modelo de embeddings que convertirá textos en vectores\n# api_endpoint: URL del endpoint de la API de AstraDB para conectarse a la base de datos\n# collection_name: nombre de la colección donde se almacenarán los vectores\n# token: token de autenticación para acceder a AstraDB\n# namespace: espacio de nombres para organizar las colecciones (None usa el predeterminado)\nvector_store=AstraDBVectorStore(\n    embedding=embeddings,\n    api_endpoint=ASTRA_DB_API_ENDPOINT,\n    collection_name=\"astra_vector_langchain\",\n    token=ASTRA_DB_APPLICATION_TOKEN,\n    namespace=None,\n\n)\n# Mostrar el objeto vector_store para verificar la conexión\nvector_store",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JUmrERG9iaQd",
    "outputId": "78a22d62-27c9-4daa-83b8-659fd24fa6f1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Importar la clase Document para crear objetos de documento con contenido y metadatos\nfrom langchain_core.documents import Document\n\n# Crear el primer documento: Tweet sobre desayuno\ndocument_1 = Document(\n    page_content=\"Desayuné panqueques con chispas de chocolate y huevos revueltos esta mañana.\",\n    metadata={\"source\": \"tweet\"},\n)\n\n# Crear el segundo documento: Noticia sobre el clima\ndocument_2 = Document(\n    page_content=\"El pronóstico del tiempo para mañana es nublado y cubierto, con una máxima de 62 grados.\",\n    metadata={\"source\": \"news\"},\n)\n\n# Crear el tercer documento: Tweet sobre un proyecto con LangChain\ndocument_3 = Document(\n    page_content=\"Construyendo un nuevo proyecto emocionante con LangChain - ¡ven a verlo!\",\n    metadata={\"source\": \"tweet\"},\n)\n\n# Crear el cuarto documento: Noticia sobre un robo bancario\ndocument_4 = Document(\n    page_content=\"Los ladrones asaltaron el banco de la ciudad y robaron 1 millón de dólares en efectivo.\",\n    metadata={\"source\": \"news\"},\n)\n\n# Crear el quinto documento: Tweet sobre una película\ndocument_5 = Document(\n    page_content=\"¡Guau! Esa fue una película increíble. No puedo esperar para verla de nuevo.\",\n    metadata={\"source\": \"tweet\"},\n)\n\n# Crear el sexto documento: Artículo web sobre el iPhone\ndocument_6 = Document(\n    page_content=\"¿Vale la pena el precio del nuevo iPhone? Lee esta reseña para descubrirlo.\",\n    metadata={\"source\": \"website\"},\n)\n\n# Crear el séptimo documento: Artículo web sobre futbolistas\ndocument_7 = Document(\n    page_content=\"Los 10 mejores jugadores de fútbol del mundo en este momento.\",\n    metadata={\"source\": \"website\"},\n)\n\n# Crear el octavo documento: Tweet sobre LangGraph\ndocument_8 = Document(\n    page_content=\"¡LangGraph es el mejor framework para construir aplicaciones con estado y agentes!\",\n    metadata={\"source\": \"tweet\"},\n)\n\n# Crear el noveno documento: Noticia sobre la bolsa de valores\ndocument_9 = Document(\n    page_content=\"El mercado de valores cayó 500 puntos hoy debido a temores de una recesión.\",\n    metadata={\"source\": \"news\"},\n)\n\n# Crear el décimo documento: Tweet con sentimiento negativo\ndocument_10 = Document(\n    page_content=\"Tengo el mal presentimiento de que voy a ser eliminado :(\",\n    metadata={\"source\": \"tweet\"},\n)\n\n# Crear una lista con todos los documentos para facilitar su procesamiento\ndocuments = [\n    document_1,\n    document_2,\n    document_3,\n    document_4,\n    document_5,\n    document_6,\n    document_7,\n    document_8,\n    document_9,\n    document_10,\n]\n# Mostrar la lista de documentos\ndocuments",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mwoez06VkPVP",
    "outputId": "c8132aa3-4564-4d73-c64d-0baf1406c723"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Agregar todos los documentos al vector store de AstraDB\n# Este método convierte cada documento en un vector usando los embeddings de OpenAI\n# y los almacena en la colección de AstraDB en la nube\n# Retorna una lista de IDs únicos generados para cada documento almacenado\nvector_store.add_documents(documents=documents)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KUz80mVRiaKt",
    "outputId": "98ce58e9-dc64-4529-aceb-d40a9fecdff0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "### Búsqueda desde el Vector Store DB\n\n# Realizar una búsqueda de similaridad en el vector store de AstraDB\n# La consulta \"Cómo está el clima\" se convierte en un vector usando embeddings\n# Se compara con todos los vectores almacenados y retorna los más similares (por defecto 4 resultados)\nvector_store.similarity_search(\"Cómo está el clima\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pny4p7VdiaBw",
    "outputId": "55c9384c-cb31-40c7-e4ec-07342ed37426"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Realizar una búsqueda de similaridad con filtros\n# Busca documentos similares a la consulta sobre LangChain y abstracciones para LLMs\n# k=3: limita los resultados a los 3 documentos más similares\n# filter={\"source\": \"tweet\"}: solo busca en documentos cuya fuente sea \"tweet\"\n# Itera sobre los resultados y muestra el contenido y metadatos de cada documento\nresults = vector_store.similarity_search(\n    \"LangChain proporciona abstracciones para facilitar el trabajo con LLMs\",\n    k=3,\n    filter={\"source\": \"tweet\"},\n)\nfor res in results:\n    print(f'* \"{res.page_content}\", metadata={res.metadata}')",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "63rta9Sfk1vZ",
    "outputId": "7caeee76-a3cd-426d-e522-823f85fbe6d2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Realizar una búsqueda de similaridad con puntuación de similitud\n# similarity_search_with_score retorna tanto los documentos como su puntuación de similitud\n# La puntuación indica qué tan similar es cada documento a la consulta (valores más altos = mayor similitud)\n# k=3: retorna los 3 documentos más similares\n# filter={\"source\": \"tweet\"}: filtra solo tweets\n# Muestra cada resultado con su puntuación de similitud (SIM) formateada a 2 decimales\nresults = vector_store.similarity_search_with_score(\n    \"LangChain proporciona abstracciones para facilitar el trabajo con LLMs\",\n    k=3,\n    filter={\"source\": \"tweet\"},\n)\nfor res, score in results:\n    print(f'* [SIM={score:.2f}] \"{res.page_content}\", metadata={res.metadata}')",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "faIybJwglD1O",
    "outputId": "e25d5325-4961-4247-c448-83c788bb67fd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "### Retriever (Recuperador)\n\n# Convertir el vector store en un retriever con búsqueda por umbral de similitud\n# search_type=\"similarity_score_threshold\": solo retorna documentos con similitud >= al umbral\n# search_kwargs: parámetros de búsqueda\n#   - k=1: retorna máximo 1 documento\n#   - score_threshold=0.5: solo retorna documentos con puntuación de similitud >= 0.5\nretriever=vector_store.as_retriever(\n  search_type=\"similarity_score_threshold\",\n    search_kwargs={\"k\": 1, \"score_threshold\": 0.5},\n)\n# Invocar el retriever con una consulta sobre crimen bancario\n# filter={\"source\": \"news\"}: filtra solo documentos de noticias\nretriever.invoke(\"Robar del banco es un crimen\", filter={\"source\": \"news\"})",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6I81FX7lIKu",
    "outputId": "c1c3cfa3-aa2f-415b-bd58-3d58dfc57099"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "### Retriever (Recuperador)\n\n# Convertir el vector store en un retriever usando búsqueda MMR (Maximal Marginal Relevance)\n# search_type=\"mmr\": MMR equilibra relevancia con diversidad en los resultados\n# MMR evita retornar documentos muy similares entre sí, maximizando la cobertura de información\n# search_kwargs: \n#   - k=1: retorna máximo 1 documento\nretriever=vector_store.as_retriever(\n  search_type=\"mmr\",\n    search_kwargs={\"k\": 1},\n)\n# Invocar el retriever con una consulta sobre crimen bancario\n# filter={\"source\": \"news\"}: filtra solo documentos de noticias\nretriever.invoke(\"Robar del banco es un crimen\", filter={\"source\": \"news\"})",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ro267A9ulY82",
    "outputId": "c259395a-05d8-4534-e299-c7982f584fbd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "l08jC-ublbqr"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}