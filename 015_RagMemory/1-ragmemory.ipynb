{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "988f23cc",
   "metadata": {},
   "source": "### RAG con Memoria Persistente usando LangGraph\n\nEn este notebook aprenderemos a implementar un sistema RAG (Retrieval-Augmented Generation) que mantiene el historial de conversaciones usando la funcionalidad de memoria persistente de LangGraph."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d89f7f",
   "metadata": {},
   "outputs": [],
   "source": "# Importamos las librerías necesarias para configuración del entorno\nimport os  # Para manejar variables de entorno del sistema operativo\nfrom dotenv import load_dotenv  # Para cargar variables desde archivo .env\n\n# Cargamos las variables de entorno desde el archivo .env\nload_dotenv()\n\n# Configuramos la API key de OpenAI desde las variables de entorno\n# Esto es necesario para autenticarnos con el servicio de OpenAI\nos.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n\n# Importamos la función para inicializar modelos de chat de LangChain\nfrom langchain.chat_models import init_chat_model\n\n# Inicializamos el modelo de lenguaje GPT-5 de OpenAI\n# Este será el LLM (Large Language Model) que usaremos para las respuestas\nllm = init_chat_model(\"openai:gpt-5\")\n\n# Mostramos la configuración del modelo para verificar que se inicializó correctamente\nllm"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5254d3a",
   "metadata": {},
   "outputs": [],
   "source": "# Importamos la clase OpenAIEmbeddings de LangChain\n# Los embeddings son representaciones vectoriales de texto que capturan significado semántico\nfrom langchain_openai import OpenAIEmbeddings\n\n# Inicializamos el modelo de embeddings de OpenAI\n# Por defecto usa 'text-embedding-ada-002' que convierte texto en vectores de 1536 dimensiones\nembeddings = OpenAIEmbeddings()\n\n# Mostramos la configuración del embedding para verificar sus parámetros\nembeddings"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fd8121",
   "metadata": {},
   "outputs": [],
   "source": "## Ingesta y Procesamiento de Documentos\n\n# Importamos BeautifulSoup para parsear HTML\nimport bs4\n\n# Importamos el cargador de documentos web de LangChain\nfrom langchain_community.document_loaders import WebBaseLoader\n\n# Importamos la clase Document que representa un documento con contenido y metadatos\nfrom langchain_core.documents import Document\n\n# Importamos el splitter de texto recursivo para dividir documentos en chunks\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Importamos tipos para type hints y mejor documentación del código\nfrom typing_extensions import List, TypedDict"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4312ad",
   "metadata": {},
   "outputs": [],
   "source": "# Cargamos y procesamos el contenido de un blog sobre agentes de IA\n\n# Creamos un loader para extraer contenido web\nloader = WebBaseLoader(\n    # URL del blog de Lilian Weng sobre agentes de IA\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n    \n    # Configuramos BeautifulSoup para extraer solo las secciones relevantes\n    bs_kwargs=dict(\n        # SoupStrainer filtra el HTML para extraer solo elementos específicos\n        parse_only=bs4.SoupStrainer(\n            # Extraemos solo elementos con estas clases CSS\n            # Esto nos da el título, encabezado y contenido del post, ignorando navegación/footer\n            class_=(\"post-content\", \"post-title\", \"post-header\")\n        )\n    ),\n)\n\n# Ejecutamos el loader para descargar y parsear el contenido\n# docs será una lista de objetos Document con page_content y metadata\ndocs = loader.load()\n\n# Mostramos los documentos cargados para verificar el contenido\ndocs"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31af3689",
   "metadata": {},
   "outputs": [],
   "source": "## Chunking (División en fragmentos)\n\n# Creamos un splitter de texto recursivo para dividir documentos largos en chunks manejables\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,        # Tamaño máximo de cada chunk en caracteres\n    chunk_overlap=200       # Superposición entre chunks para mantener contexto entre fragmentos\n)\n\n# Dividimos todos los documentos cargados en chunks más pequeños\n# Esto es crucial para RAG porque:\n# 1. Los modelos tienen límites de contexto\n# 2. Chunks más pequeños mejoran la precisión de la búsqueda semántica\n# 3. Permite recuperar solo las partes relevantes del documento\nall_splits = text_splitter.split_documents(docs)\n\n# Mostramos los chunks resultantes\nall_splits"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71edc955",
   "metadata": {},
   "outputs": [],
   "source": "## Vector Store (Almacén de Vectores)\n\n# Importamos FAISS, una biblioteca de Facebook para búsqueda de similitud eficiente\nfrom langchain_community.vectorstores import FAISS\n\n# Creamos un vector store FAISS a partir de nuestros documentos fragmentados\nvector_store = FAISS.from_documents(\n    documents=all_splits,    # Los chunks de documentos que creamos anteriormente\n    embedding=embeddings     # El modelo de embeddings de OpenAI para convertir texto a vectores\n)\n\n# FAISS crea un índice optimizado para búsqueda de similitud por coseno\n# Cada documento se convierte en un vector de 1536 dimensiones\n# El índice permite búsquedas rápidas de los vectores más similares a una consulta\n\n# Mostramos cuántos vectores se almacenaron en el índice\nprint(f\"Vector store creado con {vector_store.index.ntotal} vectores\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acf1997",
   "metadata": {},
   "outputs": [],
   "source": "# Importamos el decorador tool de LangChain para crear herramientas personalizadas\n# Las herramientas son funciones que los agentes pueden invocar\nfrom langchain.agents import tool"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc473ea",
   "metadata": {},
   "outputs": [],
   "source": "# Definimos una herramienta de recuperación usando el decorador @tool\n@tool()\ndef retrieve(query: str):\n    \"\"\"Recupera información relacionada con la consulta del usuario\"\"\"\n    \n    # Buscamos los 2 documentos más similares a la consulta en el vector store\n    # similarity_search utiliza similitud de coseno entre embeddings\n    retrieved_docs = vector_store.similarity_search(query, k=2)\n    \n    # Serializamos los documentos recuperados en un formato legible\n    # Incluimos tanto los metadatos (fuente) como el contenido de cada documento\n    serialized = \"\\n\\n\".join(\n        (f\"Fuente: {doc.metadata}\\nContenido: {doc.page_content}\")\n        for doc in retrieved_docs\n    )\n    \n    # Retornamos tanto la versión serializada (para el LLM) como los documentos originales\n    return serialized, retrieved_docs"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355f938a",
   "metadata": {},
   "outputs": [],
   "source": "# Importamos los componentes necesarios de LangGraph para construir el flujo del agente\n\n# SystemMessage: para crear mensajes del sistema que guían el comportamiento del LLM\nfrom langchain_core.messages import SystemMessage\n\n# MemorySaver: gestor de checkpoints que persiste el estado de la conversación\nfrom langgraph.checkpoint.memory import MemorySaver\n\n# END: constante que indica el final del grafo\n# MessagesState: estado que mantiene la lista de mensajes de la conversación\n# StateGraph: clase para construir grafos de estado\nfrom langgraph.graph import END, MessagesState, StateGraph\n\n# ToolNode: nodo que ejecuta herramientas\n# tools_condition: función que decide si llamar herramientas o terminar\nfrom langgraph.prebuilt import ToolNode, tools_condition"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8ad4fe",
   "metadata": {},
   "outputs": [],
   "source": "# Paso 1: Generar un mensaje de IA que puede incluir una llamada a herramienta\n\ndef query_or_respond(state: MessagesState):\n    \"\"\"Genera una llamada a herramienta para recuperación o responde directamente.\"\"\"\n    \n    # Vinculamos las herramientas al LLM\n    # Esto permite que el modelo decida cuándo invocar la herramienta 'retrieve'\n    llm_with_tools = llm.bind_tools([retrieve])\n    \n    # Invocamos el LLM con el historial de mensajes del estado\n    # El modelo analiza la conversación y decide si necesita recuperar información\n    response = llm_with_tools.invoke(state[\"messages\"])\n    \n    # MessagesState añade mensajes al estado en lugar de sobrescribirlos\n    # Esto mantiene el historial completo de la conversación\n    return {\"messages\": [response]}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d60ac7b",
   "metadata": {},
   "outputs": [],
   "source": "# Paso 2: Ejecutar la recuperación de información\n\n# Creamos un nodo de herramientas que puede ejecutar nuestra función 'retrieve'\n# ToolNode es una clase preconfigurada que:\n# 1. Extrae las llamadas a herramientas del mensaje de IA\n# 2. Ejecuta las herramientas correspondientes\n# 3. Retorna los resultados como mensajes de herramienta (ToolMessage)\ntools = ToolNode([retrieve])\n\n# Mostramos la configuración del nodo de herramientas\ntools"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e2750e",
   "metadata": {},
   "outputs": [],
   "source": "# Paso 3: Generar una respuesta usando el contenido recuperado\n\ndef generate(state: MessagesState):\n    \"\"\"Genera una respuesta basada en el contexto recuperado.\"\"\"\n    \n    # Obtenemos los mensajes de herramienta más recientes\n    # Recorremos los mensajes en orden inverso para obtener los resultados más recientes\n    recent_tool_messages = []\n    for message in reversed(state[\"messages\"]):\n        # Si el mensaje es de tipo \"tool\" (resultado de herramienta), lo guardamos\n        if message.type == \"tool\":\n            recent_tool_messages.append(message)\n        else:\n            # Dejamos de buscar cuando encontramos un mensaje que no es de herramienta\n            break\n    \n    # Revertimos el orden para tener los mensajes en orden cronológico\n    tool_messages = recent_tool_messages[::-1]\n\n    # Formateamos el contenido recuperado en un formato legible\n    # Concatenamos el contenido de todos los mensajes de herramienta\n    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n    \n    # Creamos el mensaje del sistema con el contexto recuperado\n    # Este mensaje instruye al LLM sobre cómo usar el contexto\n    system_message_content = (\n        \"Eres un asistente para tareas de respuesta a preguntas. \"\n        \"Usa los siguientes fragmentos de contexto recuperado para responder \"\n        \"la pregunta. Si no sabes la respuesta, di que no lo sabes. \"\n        \"Usa un máximo de tres oraciones y mantén la respuesta concisa.\"\n        \"\\n\\n\"\n        f\"{docs_content}\"\n    )\n    \n    # Filtramos los mensajes de conversación para incluir solo:\n    # - Mensajes humanos (preguntas del usuario)\n    # - Mensajes del sistema\n    # - Mensajes de IA que no contienen llamadas a herramientas\n    conversation_messages = [\n        message\n        for message in state[\"messages\"]\n        if message.type in (\"human\", \"system\")\n        or (message.type == \"ai\" and not message.tool_calls)\n    ]\n    \n    # Construimos el prompt completo: mensaje del sistema + historial de conversación\n    prompt = [SystemMessage(system_message_content)] + conversation_messages\n\n    # Invocamos el LLM con el prompt completo para generar la respuesta final\n    response = llm.invoke(prompt)\n    \n    # Retornamos la respuesta como un nuevo mensaje en el estado\n    return {\"messages\": [response]}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ad1199",
   "metadata": {},
   "outputs": [],
   "source": "# Construimos el grafo de estado que orquesta el flujo RAG\n\n# Creamos un constructor de grafo con MessagesState\n# MessagesState mantiene automáticamente el historial de mensajes\ngraph_builder = StateGraph(MessagesState)\n\n# Añadimos los tres nodos principales al grafo:\n# 1. query_or_respond: decide si necesita recuperar información\ngraph_builder.add_node(query_or_respond)\n# 2. tools: ejecuta la herramienta de recuperación\ngraph_builder.add_node(tools)\n# 3. generate: genera la respuesta final con el contexto\ngraph_builder.add_node(generate)\n\n# Definimos el punto de entrada del grafo\n# Todas las conversaciones comienzan con query_or_respond\ngraph_builder.set_entry_point(\"query_or_respond\")\n\n# Añadimos un edge condicional desde query_or_respond\n# tools_condition examina si el mensaje de IA contiene llamadas a herramientas:\n# - Si hay llamadas a herramientas -> va a \"tools\"\n# - Si no hay llamadas a herramientas -> va a END (termina)\ngraph_builder.add_conditional_edges(\n    \"query_or_respond\",\n    tools_condition,\n    {END: END, \"tools\": \"tools\"},\n)\n\n# Después de ejecutar las herramientas, siempre vamos a \"generate\"\n# Este edge conecta la recuperación con la generación de respuesta\ngraph_builder.add_edge(\"tools\", \"generate\")\n\n# Después de generar la respuesta, terminamos\n# END indica que el flujo ha completado\ngraph_builder.add_edge(\"generate\", END)\n\n# Creamos el gestor de memoria para persistir el estado entre conversaciones\n# MemorySaver guarda checkpoints del estado completo (incluyendo mensajes)\nmemory = MemorySaver()\n\n# Compilamos el grafo con el checkpointer\n# Esto habilita la memoria persistente entre llamadas\ngraph = graph_builder.compile(checkpointer=memory)\n\n# Mostramos el grafo compilado\ngraph"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4380a472",
   "metadata": {},
   "outputs": [],
   "source": "# Especificamos un ID para el hilo de conversación\n# El thread_id identifica de manera única esta conversación\n# Permite recuperar el historial de mensajes en futuras llamadas\nconfig = {\"configurable\": {\"thread_id\": \"abc123\"}}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff3da05",
   "metadata": {},
   "outputs": [],
   "source": "# Mensaje de entrada del usuario\ninput_message = \"Hola\"\n\n# Ejecutamos el grafo con streaming\n# stream() genera valores intermedios mientras el grafo se ejecuta\nfor step in graph.stream(\n    # Creamos el estado inicial con un mensaje del usuario\n    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n    # stream_mode=\"values\" retorna el estado completo en cada paso\n    stream_mode=\"values\",\n    # Pasamos la configuración con el thread_id para memoria persistente\n    config=config,\n):\n    # Imprimimos el último mensaje de cada paso con formato\n    # pretty_print() muestra el mensaje con colores y formato legible\n    step[\"messages\"][-1].pretty_print()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa78f09",
   "metadata": {},
   "outputs": [],
   "source": "# Probamos una búsqueda de similitud directa en el vector store\n# Esto nos muestra qué documentos se recuperarían para esta consulta\nvector_store.similarity_search(\"¿Qué es la Descomposición de Tareas?\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820f3253",
   "metadata": {},
   "outputs": [],
   "source": "# Segunda interacción: pregunta sobre Task Decomposition\ninput_message = \"¿Qué es la Descomposición de Tareas?\"\n\n# Ejecutamos el grafo nuevamente con la misma configuración\n# Como usamos el mismo thread_id, el grafo tiene acceso al historial previo\nfor step in graph.stream(\n    # Nuevo mensaje del usuario\n    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n    stream_mode=\"values\",\n    # Mismo thread_id = misma conversación\n    config=config,\n):\n    # Mostramos cada mensaje generado durante el flujo\n    step[\"messages\"][-1].pretty_print()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3099dff",
   "metadata": {},
   "outputs": [],
   "source": "# Tercera interacción: pregunta de seguimiento que requiere contexto\n# \"hacerlo\" se refiere a \"Task Decomposition\" de la pregunta anterior\ninput_message = \"¿Puedes buscar algunas formas comunes de hacerlo?\"\n\n# Esta pregunta demuestra la memoria persistente del sistema:\n# El agente entiende que \"hacerlo\" se refiere a Task Decomposition\n# porque tiene acceso al historial completo de la conversación\nfor step in graph.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n    stream_mode=\"values\",\n    config=config,\n):\n    step[\"messages\"][-1].pretty_print()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f8e3d2",
   "metadata": {},
   "outputs": [],
   "source": "### Historial de Conversación\n\n# Recuperamos el historial completo de la conversación desde el estado guardado\n# get_state() recupera el último checkpoint guardado para este thread_id\nchat_history = graph.get_state(config).values[\"messages\"]\n\n# Iteramos sobre todos los mensajes e imprimimos cada uno con formato\n# Esto muestra la conversación completa incluyendo:\n# - Mensajes del usuario (Human)\n# - Respuestas de IA (AI)\n# - Llamadas a herramientas (Tool calls)\n# - Resultados de herramientas (Tool)\nfor message in chat_history:\n    message.pretty_print()"
  },
  {
   "cell_type": "markdown",
   "id": "8330e361",
   "metadata": {},
   "source": "### Arquitectura de Agente ReAct - Memoria Persistente\n\nEn esta sección usamos la función preconfigurada `create_react_agent` que implementa el patrón ReAct (Reasoning + Acting) de manera simplificada."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87352187",
   "metadata": {},
   "outputs": [],
   "source": "# Mostramos la herramienta de recuperación que definimos anteriormente\n# Esta es la misma herramienta que usaremos con el agente ReAct\nretrieve"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8de4a9",
   "metadata": {},
   "outputs": [],
   "source": "# Importamos la función helper para crear agentes ReAct\nfrom langgraph.prebuilt import create_react_agent\n\n# Creamos un nuevo gestor de memoria para este agente\nmemory = MemorySaver()\n\n# Creamos un agente ReAct preconfigurado\n# create_react_agent es una función helper que construye automáticamente:\n# 1. Un grafo con el patrón ReAct (razonamiento + acción)\n# 2. Manejo de herramientas integrado\n# 3. Gestión de memoria persistente\nagent_executor = create_react_agent(\n    llm,              # El modelo de lenguaje\n    [retrieve],       # Lista de herramientas disponibles\n    checkpointer=memory  # Gestor de checkpoints para memoria persistente\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb64bbe7",
   "metadata": {},
   "outputs": [],
   "source": "# Mostramos la configuración del agente ejecutor\n# Esto muestra la estructura del grafo compilado con todos sus nodos\nagent_executor"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a693a482",
   "metadata": {},
   "outputs": [],
   "source": "# Configuramos un nuevo thread_id para una conversación diferente\n# Este es independiente del thread_id \"abc123\" que usamos antes\nconfig = {\"configurable\": {\"thread_id\": \"def234\"}}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be78b881",
   "metadata": {},
   "outputs": [],
   "source": "# Mensaje de prueba que incluye múltiples tareas\n# Este mensaje prueba la capacidad del agente para:\n# 1. Responder una pregunta\n# 2. Realizar una acción de seguimiento basada en la respuesta\ninput_message = (\n    \"¿Cuál es el método estándar para la Descomposición de Tareas?\\n\\n\"\n    \"Una vez que obtengas la respuesta, busca extensiones comunes de ese método.\"\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5ab1eb",
   "metadata": {},
   "outputs": [],
   "source": "# Ejecutamos el agente ReAct con streaming\n\n# Iteramos sobre los eventos generados por el agente\nfor event in agent_executor.stream(\n    # Estado inicial con el mensaje del usuario\n    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n    # stream_mode=\"values\" retorna el estado completo en cada paso\n    stream_mode=\"values\",\n    # Configuración con el thread_id para memoria persistente\n    config=config\n):\n    # Imprimimos el último mensaje de cada evento\n    # Esto mostrará:\n    # 1. El mensaje del usuario\n    # 2. Las llamadas a herramientas que hace el agente\n    # 3. Los resultados de las herramientas\n    # 4. La respuesta final del agente\n    event[\"messages\"][-1].pretty_print()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGUdemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}