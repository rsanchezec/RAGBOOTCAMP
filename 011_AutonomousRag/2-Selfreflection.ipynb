{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üß† ¬øQu√© es la Auto-Reflexi√≥n en RAG?\n",
        "Auto-reflexi√≥n = El LLM eval√∫a su propia salida:\n",
        "\"¬øEs esto claro, completo y preciso?\"\n",
        "\n",
        "#### Auto-Reflexi√≥n en RAG usando LangGraph, dise√±aremos un flujo de trabajo donde el agente:\n",
        "\n",
        "1. Genera una respuesta inicial usando el contexto recuperado\n",
        "2. Reflexiona sobre esa respuesta con un paso dedicado de LLM auto-cr√≠tico\n",
        "3. Si no est√° satisfecho, puede revisar la consulta, recuperar de nuevo, o regenerar la respuesta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Udemy\\RAGBootcamp\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Importaci√≥n del m√≥dulo os para interactuar con el sistema operativo (variables de entorno, rutas, etc.)\n",
        "import os\n",
        "\n",
        "# Importaci√≥n de List desde typing para definir tipos de datos (listas tipadas)\n",
        "from typing import List\n",
        "\n",
        "# Importaci√≥n de BaseModel desde pydantic para crear modelos de datos con validaci√≥n autom√°tica\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# Importaci√≥n de OpenAIEmbeddings para generar embeddings (representaciones vectoriales) usando la API de OpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Importaci√≥n de Document, la clase base de LangChain para representar documentos con contenido y metadatos\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Importaci√≥n de RecursiveCharacterTextSplitter para dividir textos largos en chunks (fragmentos) de manera recursiva\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Importaci√≥n de FAISS, una biblioteca de Facebook para b√∫squeda eficiente de similitud en vectores\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Importaci√≥n de TextLoader para cargar archivos de texto plano (.txt) como documentos\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "# Importaci√≥n de StateGraph para crear grafos de estado (flujos de trabajo con nodos y aristas)\n",
        "# END es un marcador especial que indica el final del grafo\n",
        "from langgraph.graph import StateGraph, END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Cargar modelos LLM\n",
        "\n",
        "# Importaci√≥n del m√≥dulo os para acceder a variables de entorno\n",
        "import os\n",
        "\n",
        "# Importaci√≥n de init_chat_model para inicializar modelos de chat de diferentes proveedores\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# Importaci√≥n de load_dotenv para cargar variables de entorno desde un archivo .env\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Configuraci√≥n de la variable de entorno OPENAI_API_KEY con el valor obtenido del archivo .env\n",
        "# Esto permite autenticarse con la API de OpenAI\n",
        "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Inicializaci√≥n del modelo de lenguaje usando GPT-4o (optimizado) de OpenAI\n",
        "# Este ser√° el LLM principal para generar respuestas y reflexiones\n",
        "llm=init_chat_model(\"openai:gpt-4o\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carga del archivo de texto \"internal_docs.txt\" usando TextLoader\n",
        "# .load() devuelve una lista de objetos Document con el contenido del archivo\n",
        "docs = TextLoader(\"internal_docs.txt\").load()\n",
        "\n",
        "# Divisi√≥n de los documentos en chunks (fragmentos) m√°s peque√±os usando RecursiveCharacterTextSplitter\n",
        "# chunk_size=500: cada fragmento tendr√° aproximadamente 500 caracteres\n",
        "# chunk_overlap=50: habr√° una superposici√≥n de 50 caracteres entre fragmentos consecutivos (para mantener contexto)\n",
        "chunks = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50).split_documents(docs)\n",
        "\n",
        "# Creaci√≥n de un vector store (base de datos vectorial) usando FAISS\n",
        "# .from_documents() toma los chunks y genera embeddings usando OpenAIEmbeddings()\n",
        "# Estos embeddings permiten b√∫squedas de similitud sem√°ntica\n",
        "vectorstore = FAISS.from_documents(chunks, OpenAIEmbeddings())\n",
        "\n",
        "# Conversi√≥n del vector store en un retriever (recuperador)\n",
        "# El retriever es la interfaz para realizar b√∫squedas de documentos relevantes basadas en una consulta\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# 2. Definici√≥n del Estado\n",
        "# -------------------------\n",
        "\n",
        "# Clase que define el estado del flujo de trabajo RAG con auto-reflexi√≥n\n",
        "# Hereda de BaseModel (Pydantic) para validaci√≥n autom√°tica de tipos y datos\n",
        "class RAGReflectionState(BaseModel):\n",
        "    # question: la pregunta del usuario (tipo string, campo obligatorio)\n",
        "    question: str\n",
        "    \n",
        "    # retrieved_docs: lista de documentos recuperados del vector store\n",
        "    # Por defecto es una lista vac√≠a []\n",
        "    retrieved_docs: List[Document] = []\n",
        "    \n",
        "    # answer: la respuesta generada por el LLM\n",
        "    # Por defecto es una cadena vac√≠a \"\"\n",
        "    answer: str = \"\"\n",
        "    \n",
        "    # reflection: la evaluaci√≥n/reflexi√≥n del LLM sobre su propia respuesta\n",
        "    # Por defecto es una cadena vac√≠a \"\"\n",
        "    reflection: str = \"\"\n",
        "    \n",
        "    # revised: indica si la respuesta necesita ser revisada (True) o est√° aprobada (False)\n",
        "    # Por defecto es False (respuesta aprobada)\n",
        "    revised: bool = False\n",
        "    \n",
        "    # attempts: contador de intentos de generaci√≥n de respuesta\n",
        "    # Por defecto es 0, se incrementa cada vez que se genera una respuesta\n",
        "    attempts: int = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# 3. Nodos (Funciones del Grafo)\n",
        "# -------------------------\n",
        "\n",
        "# a. Recuperar Documentos (Retrieve)\n",
        "# Esta funci√≥n recupera documentos relevantes del vector store bas√°ndose en la pregunta\n",
        "def retrieve_docs(state: RAGReflectionState) -> RAGReflectionState:\n",
        "    # Invoca al retriever con la pregunta del estado actual\n",
        "    # Devuelve una lista de documentos similares sem√°nticamente\n",
        "    docs = retriever.invoke(state.question)\n",
        "    \n",
        "    # Retorna una copia actualizada del estado con los documentos recuperados\n",
        "    # .model_copy(update={...}) crea una nueva instancia del estado con campos actualizados\n",
        "    return state.model_copy(update={\"retrieved_docs\": docs})\n",
        "\n",
        "# b. Generar Respuesta (Generate Answer)\n",
        "# Esta funci√≥n genera una respuesta usando el LLM bas√°ndose en los documentos recuperados\n",
        "def generate_answer(state: RAGReflectionState) -> RAGReflectionState:\n",
        "    \n",
        "    # Concatena el contenido de todos los documentos recuperados con dos saltos de l√≠nea entre ellos\n",
        "    # Esto crea un contexto unificado para el LLM\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in state.retrieved_docs])\n",
        "    \n",
        "    # Construcci√≥n del prompt para el LLM\n",
        "    # Incluye instrucciones, el contexto recuperado y la pregunta del usuario\n",
        "    prompt = f\"\"\"\n",
        "Usa el siguiente contexto para responder la pregunta:\n",
        "\n",
        "Contexto:\n",
        "{context}\n",
        "\n",
        "Pregunta:\n",
        "{state.question}\n",
        "\"\"\"\n",
        "    # Invoca al LLM con el prompt, extrae el contenido de la respuesta y elimina espacios en blanco\n",
        "    answer = llm.invoke(prompt).content.strip()\n",
        "    \n",
        "    # Retorna el estado actualizado con la respuesta generada y el contador de intentos incrementado en 1\n",
        "    return state.model_copy(update={\"answer\": answer, \"attempts\": state.attempts + 1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# c. Auto-Reflexi√≥n (Self-Reflect)\n",
        "# Esta funci√≥n eval√∫a la calidad de la respuesta generada usando el mismo LLM como cr√≠tico\n",
        "def reflect_on_answer(state: RAGReflectionState) -> RAGReflectionState:\n",
        "    \n",
        "    # Construcci√≥n del prompt de reflexi√≥n\n",
        "    # Le pide al LLM que eval√∫e si la respuesta responde completamente la pregunta\n",
        "    prompt = f\"\"\"\n",
        "Reflexiona sobre la siguiente respuesta para ver si responde completamente la pregunta.\n",
        "Indica S√ç si est√° completa y correcta, o NO con una explicaci√≥n.\n",
        "\n",
        "Pregunta: {state.question}\n",
        "\n",
        "Respuesta: {state.answer}\n",
        "\n",
        "Responde as√≠:\n",
        "Reflexi√≥n: S√ç o NO\n",
        "Explicaci√≥n: ...\n",
        "\"\"\"\n",
        "    # Invoca al LLM con el prompt de reflexi√≥n y obtiene el resultado\n",
        "    result = llm.invoke(prompt).content\n",
        "    \n",
        "    # Verifica si la reflexi√≥n contiene \"reflection: yes\" (en min√∫sculas)\n",
        "    # is_ok ser√° True si el LLM aprob√≥ la respuesta, False si necesita revisi√≥n\n",
        "    is_ok = \"reflection: yes\" in result.lower()\n",
        "    \n",
        "    # Retorna el estado actualizado con:\n",
        "    # - reflection: el texto completo de la evaluaci√≥n del LLM\n",
        "    # - revised: True si necesita revisi√≥n (not is_ok), False si est√° aprobada\n",
        "    return state.model_copy(update={\"reflection\": result, \"revised\": not is_ok})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# d. Finalizador (Finalizer)\n",
        "# Esta funci√≥n marca el final del flujo de trabajo\n",
        "# Simplemente retorna el estado sin modificaciones\n",
        "def finalize(state: RAGReflectionState) -> RAGReflectionState:\n",
        "    # Retorna el estado tal como est√° (sin cambios)\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# 4. Grafo Dirigido Ac√≠clico (DAG) de LangGraph\n",
        "# -------------------------\n",
        "\n",
        "# Creaci√≥n del constructor del grafo de estado usando la clase RAGReflectionState\n",
        "# Este grafo define el flujo de trabajo completo del sistema RAG con auto-reflexi√≥n\n",
        "builder = StateGraph(RAGReflectionState)\n",
        "\n",
        "# Agregado de nodos al grafo\n",
        "# Cada nodo es una funci√≥n que procesa y transforma el estado\n",
        "\n",
        "# Nodo \"retriever\": recupera documentos relevantes del vector store\n",
        "builder.add_node(\"retriever\", retrieve_docs)\n",
        "\n",
        "# Nodo \"responder\": genera una respuesta usando el LLM bas√°ndose en los documentos recuperados\n",
        "builder.add_node(\"responder\", generate_answer)\n",
        "\n",
        "# Nodo \"reflector\": eval√∫a la calidad de la respuesta generada (auto-cr√≠tica)\n",
        "builder.add_node(\"reflector\", reflect_on_answer)\n",
        "\n",
        "# Nodo \"done\": finaliza el flujo de trabajo\n",
        "builder.add_node(\"done\", finalize)\n",
        "\n",
        "# Establecer el punto de entrada del grafo (primer nodo a ejecutar)\n",
        "# El flujo comienza en \"retriever\"\n",
        "builder.set_entry_point(\"retriever\")\n",
        "\n",
        "# Definici√≥n de aristas (edges) - conexiones directas entre nodos\n",
        "\n",
        "# Despu√©s de \"retriever\", siempre ir a \"responder\"\n",
        "builder.add_edge(\"retriever\", \"responder\")\n",
        "\n",
        "# Despu√©s de \"responder\", siempre ir a \"reflector\"\n",
        "builder.add_edge(\"responder\", \"reflector\")\n",
        "\n",
        "# Arista condicional despu√©s de \"reflector\"\n",
        "# La funci√≥n lambda decide el siguiente nodo bas√°ndose en el estado:\n",
        "# - Si revised=False (respuesta aprobada) O attempts>=2 (m√°ximo de intentos alcanzado) ‚Üí ir a \"done\"\n",
        "# - Si revised=True (necesita revisi√≥n) Y attempts<2 ‚Üí volver a \"retriever\" para intentar de nuevo\n",
        "builder.add_conditional_edges(\n",
        "    \"reflector\",\n",
        "    lambda s: \"done\" if not s.revised or s.attempts >= 2 else \"retriever\"\n",
        ")\n",
        "\n",
        "# Despu√©s de \"done\", ir a END (marcador especial que termina el grafo)\n",
        "builder.add_edge(\"done\", END)\n",
        "\n",
        "# Compilaci√≥n del grafo para hacerlo ejecutable\n",
        "# .compile() valida la estructura y crea el grafo final\n",
        "graph = builder.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üß† Respuesta Final:\n",
            " El contexto proporcionado no aborda espec√≠ficamente las variantes de transformadores en despliegues de producci√≥n. Sin embargo, en el √°mbito del aprendizaje autom√°tico y la inteligencia artificial, algunas variantes comunes de los transformadores que suelen utilizarse en despliegues de producci√≥n incluyen:\n",
            "\n",
            "1. **BERT (Bidirectional Encoder Representations from Transformers):** Muy utilizado en tareas de procesamiento del lenguaje natural (NLP) como clasificaci√≥n de texto, an√°lisis de sentimiento y respuesta a preguntas.\n",
            "\n",
            "2. **GPT (Generative Pre-trained Transformer):** Con versiones como GPT-2 y GPT-3, estos modelos son populares para generaci√≥n de texto y aplicaciones de NLP que requieren creaci√≥n de contenido.\n",
            "\n",
            "3. **Transformer-XL:** Dise√±ado para manejar dependencias a largo plazo en tareas secuenciales, mejorando el rendimiento en comparaci√≥n con transformers est√°ndar.\n",
            "\n",
            "4. **RoBERTa (A Robustly Optimized BERT Pretraining Approach):** Una versi√≥n optimizada de BERT que ajusta su entrenamiento para mejorar el rendimiento en diversas tareas.\n",
            "\n",
            "5. **DistilBERT:** Una versi√≥n m√°s ligera y r√°pida de BERT que mantiene una gran parte de su rendimiento, ideal para escenarios donde los recursos computacionales son limitados.\n",
            "\n",
            "6. **ALBERT (A Lite BERT):** Optimiza BERT mediante la reducci√≥n de par√°metros, mejorando la eficiencia en tiempo y tama√±o de modelo sin comprometer mucho el rendimiento.\n",
            "\n",
            "Estas variantes son aplicadas en entornos de producci√≥n donde se requiere un balance entre precisi√≥n, eficiencia y uso de recursos.\n",
            "\n",
            "üîÅ Log de Reflexi√≥n:\n",
            " Reflexi√≥n: S√ç\n",
            "\n",
            "Explicaci√≥n: La respuesta aborda de manera adecuada y completa la pregunta sobre las variantes de transformers en despliegues de producci√≥n. Proporciona una lista de variantes comunes como BERT, GPT, Transformer-XL, RoBERTa, DistilBERT y ALBERT, explicando brevemente sus usos y caracter√≠sticas relevantes en entornos de producci√≥n. Adem√°s, menciona el contexto de aplicaciones espec√≠ficas como el procesamiento del lenguaje natural (NLP), que es donde estos modelos son frecuentemente utilizados. Aunque no se abordan implementaciones detalladas o casos espec√≠ficos de despliegue, la respuesta cubre la pregunta en t√©rminos de identificar las variantes m√°s utilizadas en producci√≥n.\n",
            "üîÑ Intentos Totales: 2\n"
          ]
        }
      ],
      "source": [
        "# -------------------------\n",
        "# 5. Ejecutar el Agente\n",
        "# -------------------------\n",
        "\n",
        "# Verifica si este script se est√° ejecutando como programa principal (no importado como m√≥dulo)\n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    # Definici√≥n de la pregunta del usuario\n",
        "    # Esta ser√° la consulta que el sistema RAG intentar√° responder\n",
        "    user_query = \"¬øCu√°les son las variantes de transformers en despliegues de producci√≥n?\"\n",
        "    \n",
        "    # Creaci√≥n del estado inicial con la pregunta del usuario\n",
        "    # Los dem√°s campos (retrieved_docs, answer, etc.) usar√°n sus valores por defecto\n",
        "    init_state = RAGReflectionState(question=user_query)\n",
        "    \n",
        "    # Invocaci√≥n del grafo con el estado inicial\n",
        "    # .invoke() ejecuta todo el flujo de trabajo y retorna el estado final\n",
        "    result = graph.invoke(init_state)\n",
        "\n",
        "    # Impresi√≥n de la respuesta final generada por el sistema\n",
        "    print(\"\\nüß† Respuesta Final:\\n\", result[\"answer\"])\n",
        "    \n",
        "    # Impresi√≥n del log de reflexi√≥n (evaluaci√≥n de la respuesta)\n",
        "    print(\"\\nüîÅ Log de Reflexi√≥n:\\n\", result[\"reflection\"])\n",
        "    \n",
        "    # Impresi√≥n del n√∫mero total de intentos realizados\n",
        "    print(\"üîÑ Intentos Totales:\", result[\"attempts\"])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
