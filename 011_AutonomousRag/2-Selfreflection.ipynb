{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 🧠 ¿Qué es la Auto-Reflexión en RAG?\n",
        "Auto-reflexión = El LLM evalúa su propia salida:\n",
        "\"¿Es esto claro, completo y preciso?\"\n",
        "\n",
        "#### Auto-Reflexión en RAG usando LangGraph, diseñaremos un flujo de trabajo donde el agente:\n",
        "\n",
        "1. Genera una respuesta inicial usando el contexto recuperado\n",
        "2. Reflexiona sobre esa respuesta con un paso dedicado de LLM auto-crítico\n",
        "3. Si no está satisfecho, puede revisar la consulta, recuperar de nuevo, o regenerar la respuesta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Udemy\\RAGBootcamp\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Importación del módulo os para interactuar con el sistema operativo (variables de entorno, rutas, etc.)\n",
        "import os\n",
        "\n",
        "# Importación de List desde typing para definir tipos de datos (listas tipadas)\n",
        "from typing import List\n",
        "\n",
        "# Importación de BaseModel desde pydantic para crear modelos de datos con validación automática\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# Importación de OpenAIEmbeddings para generar embeddings (representaciones vectoriales) usando la API de OpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Importación de Document, la clase base de LangChain para representar documentos con contenido y metadatos\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Importación de RecursiveCharacterTextSplitter para dividir textos largos en chunks (fragmentos) de manera recursiva\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Importación de FAISS, una biblioteca de Facebook para búsqueda eficiente de similitud en vectores\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Importación de TextLoader para cargar archivos de texto plano (.txt) como documentos\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "# Importación de StateGraph para crear grafos de estado (flujos de trabajo con nodos y aristas)\n",
        "# END es un marcador especial que indica el final del grafo\n",
        "from langgraph.graph import StateGraph, END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Cargar modelos LLM\n",
        "\n",
        "# Importación del módulo os para acceder a variables de entorno\n",
        "import os\n",
        "\n",
        "# Importación de init_chat_model para inicializar modelos de chat de diferentes proveedores\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# Importación de load_dotenv para cargar variables de entorno desde un archivo .env\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Configuración de la variable de entorno OPENAI_API_KEY con el valor obtenido del archivo .env\n",
        "# Esto permite autenticarse con la API de OpenAI\n",
        "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Inicialización del modelo de lenguaje usando GPT-4o (optimizado) de OpenAI\n",
        "# Este será el LLM principal para generar respuestas y reflexiones\n",
        "llm=init_chat_model(\"openai:gpt-4o\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carga del archivo de texto \"internal_docs.txt\" usando TextLoader\n",
        "# .load() devuelve una lista de objetos Document con el contenido del archivo\n",
        "docs = TextLoader(\"internal_docs.txt\").load()\n",
        "\n",
        "# División de los documentos en chunks (fragmentos) más pequeños usando RecursiveCharacterTextSplitter\n",
        "# chunk_size=500: cada fragmento tendrá aproximadamente 500 caracteres\n",
        "# chunk_overlap=50: habrá una superposición de 50 caracteres entre fragmentos consecutivos (para mantener contexto)\n",
        "chunks = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50).split_documents(docs)\n",
        "\n",
        "# Creación de un vector store (base de datos vectorial) usando FAISS\n",
        "# .from_documents() toma los chunks y genera embeddings usando OpenAIEmbeddings()\n",
        "# Estos embeddings permiten búsquedas de similitud semántica\n",
        "vectorstore = FAISS.from_documents(chunks, OpenAIEmbeddings())\n",
        "\n",
        "# Conversión del vector store en un retriever (recuperador)\n",
        "# El retriever es la interfaz para realizar búsquedas de documentos relevantes basadas en una consulta\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# 2. Definición del Estado\n",
        "# -------------------------\n",
        "\n",
        "# Clase que define el estado del flujo de trabajo RAG con auto-reflexión\n",
        "# Hereda de BaseModel (Pydantic) para validación automática de tipos y datos\n",
        "class RAGReflectionState(BaseModel):\n",
        "    # question: la pregunta del usuario (tipo string, campo obligatorio)\n",
        "    question: str\n",
        "    \n",
        "    # retrieved_docs: lista de documentos recuperados del vector store\n",
        "    # Por defecto es una lista vacía []\n",
        "    retrieved_docs: List[Document] = []\n",
        "    \n",
        "    # answer: la respuesta generada por el LLM\n",
        "    # Por defecto es una cadena vacía \"\"\n",
        "    answer: str = \"\"\n",
        "    \n",
        "    # reflection: la evaluación/reflexión del LLM sobre su propia respuesta\n",
        "    # Por defecto es una cadena vacía \"\"\n",
        "    reflection: str = \"\"\n",
        "    \n",
        "    # revised: indica si la respuesta necesita ser revisada (True) o está aprobada (False)\n",
        "    # Por defecto es False (respuesta aprobada)\n",
        "    revised: bool = False\n",
        "    \n",
        "    # attempts: contador de intentos de generación de respuesta\n",
        "    # Por defecto es 0, se incrementa cada vez que se genera una respuesta\n",
        "    attempts: int = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# 3. Nodos (Funciones del Grafo)\n",
        "# -------------------------\n",
        "\n",
        "# a. Recuperar Documentos (Retrieve)\n",
        "# Esta función recupera documentos relevantes del vector store basándose en la pregunta\n",
        "def retrieve_docs(state: RAGReflectionState) -> RAGReflectionState:\n",
        "    # Invoca al retriever con la pregunta del estado actual\n",
        "    # Devuelve una lista de documentos similares semánticamente\n",
        "    docs = retriever.invoke(state.question)\n",
        "    \n",
        "    # Retorna una copia actualizada del estado con los documentos recuperados\n",
        "    # .model_copy(update={...}) crea una nueva instancia del estado con campos actualizados\n",
        "    return state.model_copy(update={\"retrieved_docs\": docs})\n",
        "\n",
        "# b. Generar Respuesta (Generate Answer)\n",
        "# Esta función genera una respuesta usando el LLM basándose en los documentos recuperados\n",
        "def generate_answer(state: RAGReflectionState) -> RAGReflectionState:\n",
        "    \n",
        "    # Concatena el contenido de todos los documentos recuperados con dos saltos de línea entre ellos\n",
        "    # Esto crea un contexto unificado para el LLM\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in state.retrieved_docs])\n",
        "    \n",
        "    # Construcción del prompt para el LLM\n",
        "    # Incluye instrucciones, el contexto recuperado y la pregunta del usuario\n",
        "    prompt = f\"\"\"\n",
        "Usa el siguiente contexto para responder la pregunta:\n",
        "\n",
        "Contexto:\n",
        "{context}\n",
        "\n",
        "Pregunta:\n",
        "{state.question}\n",
        "\"\"\"\n",
        "    # Invoca al LLM con el prompt, extrae el contenido de la respuesta y elimina espacios en blanco\n",
        "    answer = llm.invoke(prompt).content.strip()\n",
        "    \n",
        "    # Retorna el estado actualizado con la respuesta generada y el contador de intentos incrementado en 1\n",
        "    return state.model_copy(update={\"answer\": answer, \"attempts\": state.attempts + 1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# c. Auto-Reflexión (Self-Reflect)\n",
        "# Esta función evalúa la calidad de la respuesta generada usando el mismo LLM como crítico\n",
        "def reflect_on_answer(state: RAGReflectionState) -> RAGReflectionState:\n",
        "    \n",
        "    # Construcción del prompt de reflexión\n",
        "    # Le pide al LLM que evalúe si la respuesta responde completamente la pregunta\n",
        "    prompt = f\"\"\"\n",
        "Reflexiona sobre la siguiente respuesta para ver si responde completamente la pregunta.\n",
        "Indica SÍ si está completa y correcta, o NO con una explicación.\n",
        "\n",
        "Pregunta: {state.question}\n",
        "\n",
        "Respuesta: {state.answer}\n",
        "\n",
        "Responde así:\n",
        "Reflexión: SÍ o NO\n",
        "Explicación: ...\n",
        "\"\"\"\n",
        "    # Invoca al LLM con el prompt de reflexión y obtiene el resultado\n",
        "    result = llm.invoke(prompt).content\n",
        "    \n",
        "    # Verifica si la reflexión contiene \"reflection: yes\" (en minúsculas)\n",
        "    # is_ok será True si el LLM aprobó la respuesta, False si necesita revisión\n",
        "    is_ok = \"reflection: yes\" in result.lower()\n",
        "    \n",
        "    # Retorna el estado actualizado con:\n",
        "    # - reflection: el texto completo de la evaluación del LLM\n",
        "    # - revised: True si necesita revisión (not is_ok), False si está aprobada\n",
        "    return state.model_copy(update={\"reflection\": result, \"revised\": not is_ok})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# d. Finalizador (Finalizer)\n",
        "# Esta función marca el final del flujo de trabajo\n",
        "# Simplemente retorna el estado sin modificaciones\n",
        "def finalize(state: RAGReflectionState) -> RAGReflectionState:\n",
        "    # Retorna el estado tal como está (sin cambios)\n",
        "    return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# 4. Grafo Dirigido Acíclico (DAG) de LangGraph\n",
        "# -------------------------\n",
        "\n",
        "# Creación del constructor del grafo de estado usando la clase RAGReflectionState\n",
        "# Este grafo define el flujo de trabajo completo del sistema RAG con auto-reflexión\n",
        "builder = StateGraph(RAGReflectionState)\n",
        "\n",
        "# Agregado de nodos al grafo\n",
        "# Cada nodo es una función que procesa y transforma el estado\n",
        "\n",
        "# Nodo \"retriever\": recupera documentos relevantes del vector store\n",
        "builder.add_node(\"retriever\", retrieve_docs)\n",
        "\n",
        "# Nodo \"responder\": genera una respuesta usando el LLM basándose en los documentos recuperados\n",
        "builder.add_node(\"responder\", generate_answer)\n",
        "\n",
        "# Nodo \"reflector\": evalúa la calidad de la respuesta generada (auto-crítica)\n",
        "builder.add_node(\"reflector\", reflect_on_answer)\n",
        "\n",
        "# Nodo \"done\": finaliza el flujo de trabajo\n",
        "builder.add_node(\"done\", finalize)\n",
        "\n",
        "# Establecer el punto de entrada del grafo (primer nodo a ejecutar)\n",
        "# El flujo comienza en \"retriever\"\n",
        "builder.set_entry_point(\"retriever\")\n",
        "\n",
        "# Definición de aristas (edges) - conexiones directas entre nodos\n",
        "\n",
        "# Después de \"retriever\", siempre ir a \"responder\"\n",
        "builder.add_edge(\"retriever\", \"responder\")\n",
        "\n",
        "# Después de \"responder\", siempre ir a \"reflector\"\n",
        "builder.add_edge(\"responder\", \"reflector\")\n",
        "\n",
        "# Arista condicional después de \"reflector\"\n",
        "# La función lambda decide el siguiente nodo basándose en el estado:\n",
        "# - Si revised=False (respuesta aprobada) O attempts>=2 (máximo de intentos alcanzado) → ir a \"done\"\n",
        "# - Si revised=True (necesita revisión) Y attempts<2 → volver a \"retriever\" para intentar de nuevo\n",
        "builder.add_conditional_edges(\n",
        "    \"reflector\",\n",
        "    lambda s: \"done\" if not s.revised or s.attempts >= 2 else \"retriever\"\n",
        ")\n",
        "\n",
        "# Después de \"done\", ir a END (marcador especial que termina el grafo)\n",
        "builder.add_edge(\"done\", END)\n",
        "\n",
        "# Compilación del grafo para hacerlo ejecutable\n",
        "# .compile() valida la estructura y crea el grafo final\n",
        "graph = builder.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🧠 Respuesta Final:\n",
            " El contexto proporcionado no aborda específicamente las variantes de transformadores en despliegues de producción. Sin embargo, en el ámbito del aprendizaje automático y la inteligencia artificial, algunas variantes comunes de los transformadores que suelen utilizarse en despliegues de producción incluyen:\n",
            "\n",
            "1. **BERT (Bidirectional Encoder Representations from Transformers):** Muy utilizado en tareas de procesamiento del lenguaje natural (NLP) como clasificación de texto, análisis de sentimiento y respuesta a preguntas.\n",
            "\n",
            "2. **GPT (Generative Pre-trained Transformer):** Con versiones como GPT-2 y GPT-3, estos modelos son populares para generación de texto y aplicaciones de NLP que requieren creación de contenido.\n",
            "\n",
            "3. **Transformer-XL:** Diseñado para manejar dependencias a largo plazo en tareas secuenciales, mejorando el rendimiento en comparación con transformers estándar.\n",
            "\n",
            "4. **RoBERTa (A Robustly Optimized BERT Pretraining Approach):** Una versión optimizada de BERT que ajusta su entrenamiento para mejorar el rendimiento en diversas tareas.\n",
            "\n",
            "5. **DistilBERT:** Una versión más ligera y rápida de BERT que mantiene una gran parte de su rendimiento, ideal para escenarios donde los recursos computacionales son limitados.\n",
            "\n",
            "6. **ALBERT (A Lite BERT):** Optimiza BERT mediante la reducción de parámetros, mejorando la eficiencia en tiempo y tamaño de modelo sin comprometer mucho el rendimiento.\n",
            "\n",
            "Estas variantes son aplicadas en entornos de producción donde se requiere un balance entre precisión, eficiencia y uso de recursos.\n",
            "\n",
            "🔁 Log de Reflexión:\n",
            " Reflexión: SÍ\n",
            "\n",
            "Explicación: La respuesta aborda de manera adecuada y completa la pregunta sobre las variantes de transformers en despliegues de producción. Proporciona una lista de variantes comunes como BERT, GPT, Transformer-XL, RoBERTa, DistilBERT y ALBERT, explicando brevemente sus usos y características relevantes en entornos de producción. Además, menciona el contexto de aplicaciones específicas como el procesamiento del lenguaje natural (NLP), que es donde estos modelos son frecuentemente utilizados. Aunque no se abordan implementaciones detalladas o casos específicos de despliegue, la respuesta cubre la pregunta en términos de identificar las variantes más utilizadas en producción.\n",
            "🔄 Intentos Totales: 2\n"
          ]
        }
      ],
      "source": [
        "# -------------------------\n",
        "# 5. Ejecutar el Agente\n",
        "# -------------------------\n",
        "\n",
        "# Verifica si este script se está ejecutando como programa principal (no importado como módulo)\n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    # Definición de la pregunta del usuario\n",
        "    # Esta será la consulta que el sistema RAG intentará responder\n",
        "    user_query = \"¿Cuáles son las variantes de transformers en despliegues de producción?\"\n",
        "    \n",
        "    # Creación del estado inicial con la pregunta del usuario\n",
        "    # Los demás campos (retrieved_docs, answer, etc.) usarán sus valores por defecto\n",
        "    init_state = RAGReflectionState(question=user_query)\n",
        "    \n",
        "    # Invocación del grafo con el estado inicial\n",
        "    # .invoke() ejecuta todo el flujo de trabajo y retorna el estado final\n",
        "    result = graph.invoke(init_state)\n",
        "\n",
        "    # Impresión de la respuesta final generada por el sistema\n",
        "    print(\"\\n🧠 Respuesta Final:\\n\", result[\"answer\"])\n",
        "    \n",
        "    # Impresión del log de reflexión (evaluación de la respuesta)\n",
        "    print(\"\\n🔁 Log de Reflexión:\\n\", result[\"reflection\"])\n",
        "    \n",
        "    # Impresión del número total de intentos realizados\n",
        "    print(\"🔄 Intentos Totales:\", result[\"attempts\"])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
