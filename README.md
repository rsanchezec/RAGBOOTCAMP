# ğŸš€ RAG Bootcamp

<div align="center">

![Python Version](https://img.shields.io/badge/python-3.12-blue.svg)
![LangChain](https://img.shields.io/badge/LangChain-0.3-green.svg)
![License](https://img.shields.io/badge/license-MIT-blue.svg)

**Un curso completo sobre Retrieval-Augmented Generation (RAG) usando LangChain**

[CaracterÃ­sticas](#-caracterÃ­sticas-principales) â€¢
[InstalaciÃ³n](#-instalaciÃ³n) â€¢
[Vector Stores](#-bases-de-datos-vectoriales-vector-stores) â€¢
[BÃºsqueda HÃ­brida](#-estrategias-de-bÃºsqueda-hÃ­brida) â€¢
[Query Enhancement](#-mejora-de-consultas-query-enhancement) â€¢
[RAG Multimodal](#ï¸-rag-multimodal) â€¢
[LangGraph](#-fundamentos-de-langgraph) â€¢
[Agentes](#-arquitectura-de-agentes) â€¢
[Debugging](#-debugging-con-langgraph-studio) â€¢
[RAG AgÃ©ntico](#-rag-agÃ©ntico) â€¢
[RAG AutÃ³nomo](#-rag-autÃ³nomo) â€¢
[Multi-Agente](#-sistemas-rag-multi-agente) â€¢
[RAG Correctivo](#-rag-correctivo-corrective-rag) â€¢
[RAG Adaptativo](#-rag-adaptativo-adaptive-rag) â€¢
[RAG Memory](#-rag-con-memoria-persistente) â€¢
[Cache RAG](#ï¸-cache-augmented-generation-cag) â€¢
[Estructura](#-estructura-del-proyecto) â€¢
[Uso](#-guÃ­a-de-uso)

</div>

---

## ğŸ“‹ DescripciÃ³n

RAG Bootcamp es un proyecto educativo diseÃ±ado para enseÃ±ar los fundamentos de la construcciÃ³n de sistemas RAG (Retrieval-Augmented Generation) utilizando LangChain. El repositorio estÃ¡ estructurado como una ruta de aprendizaje progresiva con Jupyter notebooks que cubren desde la ingesta de datos hasta la implementaciÃ³n de bases de datos vectoriales.

Este proyecto es ideal para:
- ğŸ“ Estudiantes que quieren aprender sobre RAG y embeddings
- ğŸ’» Desarrolladores que construyen aplicaciones de IA generativa
- ğŸ”¬ Investigadores explorando tÃ©cnicas de recuperaciÃ³n de informaciÃ³n
- ğŸ¢ Profesionales implementando soluciones empresariales con LLMs

## âœ¨ CaracterÃ­sticas Principales

- ğŸ“š **Ingesta de Datos Completa**: Manejo de mÃºltiples formatos (PDF, DOCX, CSV, JSON, bases de datos)
- ğŸ§© **Estrategias de Chunking**: Diferentes tÃ©cnicas de divisiÃ³n de texto optimizadas
- ğŸ¯ **Advanced Chunking**: TÃ©cnicas avanzadas de semantic chunking para mejor recuperaciÃ³n
- ğŸ”¢ **MÃºltiples Embeddings**: Soporte para OpenAI, HuggingFace y sentence-transformers
- ğŸ’¾ **5 Bases de Datos Vectoriales**: ChromaDB, FAISS, InMemory, AstraDB y Pinecone
- ğŸ” **BÃºsqueda HÃ­brida Avanzada**: Dense+Sparse, Reranking y MMR para recuperaciÃ³n Ã³ptima
- ğŸ¯ **Mejora de Consultas**: Query Expansion, Query Decomposition y HyDE para optimizar bÃºsquedas
- ğŸ–¼ï¸ **RAG Multimodal**: Procesamiento de PDFs con imÃ¡genes usando CLIP y GPT-4 Vision
- ğŸ¯ **Filtrado de Metadatos**: BÃºsquedas precisas con filtros personalizados
- ğŸ”„ **LangGraph Basics**: ConstrucciÃ³n de grafos de estado con LangGraph para chatbots y agentes
- ğŸ¤– **Arquitectura de Agentes**: ImplementaciÃ³n de agentes ReAct con herramientas y memoria
- ğŸ› **Debugging con LangGraph Studio**: ConfiguraciÃ³n y depuraciÃ³n de agentes con LangGraph Studio
- ğŸ¯ **RAG AgÃ©ntico**: Sistema RAG con capacidades de razonamiento, evaluaciÃ³n y auto-correcciÃ³n
- ğŸ§  **RAG AutÃ³nomo**: Chain-of-Thought, Auto-reflexiÃ³n, DescomposiciÃ³n de Consultas, RecuperaciÃ³n Iterativa y SÃ­ntesis Multi-Fuente
- ğŸ¤– **Sistemas RAG Multi-Agente**: Arquitecturas colaborativas, con supervisor y jerÃ¡rquicas con equipos especializados
- ğŸ”§ **RAG Correctivo (CRAG)**: EvaluaciÃ³n automÃ¡tica de relevancia, reescritura de consultas y bÃºsqueda web adaptativa
- ğŸ¯ **RAG Adaptativo (Adaptive RAG)**: Enrutamiento inteligente, detecciÃ³n de alucinaciones y auto-correcciÃ³n con ciclos de retroalimentaciÃ³n
- ğŸ’¾ **RAG con Memoria Persistente**: ImplementaciÃ³n de memoria conversacional con LangGraph y MemorySaver para mantener contexto entre interacciones
- âš¡ **Cache-Augmented Generation (CAG)**: Sistema de cachÃ© semÃ¡ntico avanzado con FAISS para reutilizar respuestas y optimizar costos
- ğŸ“Š **Ejemplos PrÃ¡cticos**: Notebooks interactivos con casos de uso reales
- ğŸŒ **DocumentaciÃ³n en EspaÃ±ol**: CÃ³digo y comentarios completamente traducidos

## ğŸ› ï¸ InstalaciÃ³n

### Requisitos Previos

- Python 3.12
- pip (gestor de paquetes de Python)
- Git

### Paso 1: Clonar el Repositorio

```bash
git clone <url-del-repositorio>
cd RAGBootcamp
```

### Paso 2: Crear Entorno Virtual

```bash
# Crear entorno virtual
python3 -m venv .venv

# Activar entorno virtual
# En Windows:
.venv\Scripts\activate

# En macOS/Linux:
source .venv/bin/activate
```

### Paso 3: Instalar Dependencias

```bash
pip install -r requirements.txt
```

### Paso 4: Configurar Variables de Entorno

Copia el archivo `.env.example` y renÃ³mbralo a `.env`, luego completa tus claves API:

```bash
# Copiar el template
cp .env.example .env

# Editar el archivo .env con tus claves API
# En Windows puedes usar: notepad .env
# En macOS/Linux: nano .env
```

Tu archivo `.env` debe contener:

```env
OPENAI_API_KEY=tu_clave_openai_aqui
GROQ_API_KEY=tu_clave_groq_aqui
LANGSMITH_API_KEY=tu_clave_langsmith_aqui  # Opcional
```

**Obtener las claves API:**
- ğŸ”‘ **OpenAI**: [https://platform.openai.com/api-keys](https://platform.openai.com/api-keys)
- ğŸ”‘ **Groq**: [https://console.groq.com/keys](https://console.groq.com/keys)
- ğŸ”‘ **LangSmith** (opcional): [https://smith.langchain.com/settings](https://smith.langchain.com/settings)

### Paso 5: Iniciar Jupyter

```bash
jupyter notebook
```

## ğŸ’¾ Bases de Datos Vectoriales (Vector Stores)

Los vector stores son componentes fundamentales en sistemas RAG que permiten almacenar y buscar embeddings de manera eficiente. Este proyecto incluye implementaciones de **5 bases de datos vectoriales diferentes**, cada una con caracterÃ­sticas y casos de uso especÃ­ficos.

### ğŸ“Š Comparativa de Vector Stores

| Vector Store | Tipo | Persistencia | Escalabilidad | Latencia | Costo | Uso Ideal |
|--------------|------|--------------|---------------|----------|-------|-----------|
| **InMemoryVectorStore** | Local | âŒ No | Baja | Ultra-baja | Gratis | Prototipos rÃ¡pidos, demos |
| **FAISS** | Local | âš ï¸ Manual | Media | Muy baja | Gratis | Aplicaciones locales, alta velocidad |
| **ChromaDB** | Local/HÃ­brido | âœ… SÃ­ | Media | Baja | Gratis | Desarrollo, proyectos pequeÃ±os |
| **AstraDB** | Cloud | âœ… SÃ­ | Muy alta | Baja | ğŸ’° Pago | ProducciÃ³n, aplicaciones distribuidas |
| **Pinecone** | Cloud | âœ… SÃ­ | Muy alta | Muy baja | ğŸ’° Pago | ProducciÃ³n a gran escala |

### ğŸ” Detalles por Vector Store

#### 1. **InMemoryVectorStore**

**CaracterÃ­sticas:**
- Almacenamiento completamente en memoria RAM
- Utiliza diccionarios de Python y NumPy para cÃ¡lculos
- MÃ©trica de similitud: coseno

**Ventajas:**
- âš¡ ConfiguraciÃ³n instantÃ¡nea (sin instalaciÃ³n adicional)
- ğŸš€ Velocidad ultra-rÃ¡pida para conjuntos pequeÃ±os
- ğŸ’» No requiere infraestructura externa

**Desventajas:**
- âŒ Los datos se pierden al cerrar la aplicaciÃ³n
- âŒ Limitado por la memoria RAM disponible
- âŒ No escalable para producciÃ³n

**CuÃ¡ndo usar:**
```python
âœ… Prototipos y pruebas rÃ¡pidas
âœ… Demos y presentaciones
âœ… Datasets pequeÃ±os (<1000 documentos)
âœ… Aplicaciones de un solo uso
âŒ ProducciÃ³n
âŒ Datos que necesitan persistir
âŒ Aplicaciones multiusuario
```

**Ejemplo de uso:**
```python
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_openai import OpenAIEmbeddings

vector_store = InMemoryVectorStore(embedding=OpenAIEmbeddings())
vector_store.add_documents(documents)
results = vector_store.similarity_search("consulta", k=5)
```

---

#### 2. **FAISS (Facebook AI Similarity Search)**

**CaracterÃ­sticas:**
- Biblioteca de Facebook para bÃºsqueda de similitud eficiente
- MÃºltiples algoritmos de indexaciÃ³n (Flat, IVF, HNSW)
- Optimizado para CPU y GPU

**Ventajas:**
- âš¡ Extremadamente rÃ¡pido para millones de vectores
- ğŸ”§ Altamente configurable y optimizable
- ğŸ’¾ Puede guardarse y cargarse desde disco
- ğŸ†“ Completamente gratuito y open-source

**Desventajas:**
- âš ï¸ Requiere cÃ³digo manual para persistencia
- âŒ No incluye gestiÃ³n de metadatos por defecto
- ğŸ”§ Curva de aprendizaje para optimizaciÃ³n avanzada

**CuÃ¡ndo usar:**
```python
âœ… Aplicaciones locales con alto rendimiento
âœ… Datasets medianos a grandes (10K-10M vectores)
âœ… Cuando necesitas control total sobre la indexaciÃ³n
âœ… Aplicaciones que requieren baja latencia
âŒ Cuando necesitas sincronizaciÃ³n multi-dispositivo
âŒ Si prefieres una soluciÃ³n managed
âŒ Aplicaciones web sin servidor local
```

**Ejemplo de uso:**
```python
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings

vector_store = FAISS.from_documents(documents, OpenAIEmbeddings())

# Guardar Ã­ndice
vector_store.save_local("faiss_index")

# Cargar Ã­ndice
vector_store = FAISS.load_local("faiss_index", OpenAIEmbeddings())
```

---

#### 3. **ChromaDB**

**CaracterÃ­sticas:**
- Base de datos vectorial open-source diseÃ±ada para IA
- Persistencia automÃ¡tica en SQLite
- Soporte nativo para metadatos y filtros

**Ventajas:**
- ğŸ“¦ FÃ¡cil de configurar (instalaciÃ³n con pip)
- ğŸ’¾ Persistencia automÃ¡tica sin configuraciÃ³n
- ğŸ·ï¸ Excelente manejo de metadatos y filtros
- ğŸ”„ Soporte para actualizaciones y eliminaciones
- ğŸ†“ Completamente gratuito

**Desventajas:**
- âš ï¸ Rendimiento limitado para datasets muy grandes (>1M)
- âŒ No optimizado para entornos distribuidos
- âš ï¸ Puede tener problemas de concurrencia

**CuÃ¡ndo usar:**
```python
âœ… Desarrollo y prototipado con persistencia
âœ… Proyectos personales y pequeÃ±os
âœ… Aplicaciones que requieren filtrado complejo
âœ… Cuando necesitas actualizar/eliminar documentos
âœ… MVPs y proyectos de tamaÃ±o mediano
âŒ ProducciÃ³n a gran escala (>1M documentos)
âŒ Aplicaciones distribuidas geogrÃ¡ficamente
âŒ Sistemas con alta concurrencia
```

**Ejemplo de uso:**
```python
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

vector_store = Chroma.from_documents(
    documents,
    OpenAIEmbeddings(),
    persist_directory="./chroma_db"
)

# BÃºsqueda con filtros
results = vector_store.similarity_search(
    "consulta",
    k=5,
    filter={"source": "news"}
)
```

---

#### 4. **AstraDB (DataStax)**

**CaracterÃ­sticas:**
- Base de datos vectorial serverless en la nube
- Basada en Apache Cassandra
- Multi-regiÃ³n y multi-nube (AWS, GCP, Azure)

**Ventajas:**
- â˜ï¸ Totalmente administrada (managed service)
- ğŸŒ DistribuciÃ³n global y baja latencia
- ğŸ“ˆ Escalabilidad automÃ¡tica
- ğŸ”’ Seguridad y backup integrados
- ğŸ”„ Alta disponibilidad (99.99% SLA)
- ğŸ†“ Free tier generoso para empezar

**Desventajas:**
- ğŸ’° Costo incrementa con el uso
- ğŸŒ Requiere conexiÃ³n a internet
- âš ï¸ Latencia de red vs. soluciones locales

**CuÃ¡ndo usar:**
```python
âœ… Aplicaciones en producciÃ³n
âœ… Sistemas distribuidos y multi-regiÃ³n
âœ… Necesitas alta disponibilidad
âœ… Equipos sin experiencia en DevOps
âœ… Aplicaciones serverless (Lambda, Cloud Functions)
âœ… Escalamiento automÃ¡tico requerido
âŒ Presupuesto muy limitado
âŒ Requisitos de soberanÃ­a de datos estrictos
âŒ Aplicaciones totalmente offline
```

**Ejemplo de uso:**
```python
from langchain_astradb import AstraDBVectorStore
from langchain_openai import OpenAIEmbeddings

vector_store = AstraDBVectorStore(
    embedding=OpenAIEmbeddings(),
    api_endpoint="https://...",
    token="AstraCS:...",
    collection_name="mi_coleccion"
)

# BÃºsqueda con score threshold
retriever = vector_store.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={"k": 3, "score_threshold": 0.7}
)
```

---

#### 5. **Pinecone**

**CaracterÃ­sticas:**
- Base de datos vectorial cloud-native lÃ­der del mercado
- Optimizada especÃ­ficamente para bÃºsqueda vectorial
- API simple y potente

**Ventajas:**
- ğŸš€ Rendimiento excepcional a cualquier escala
- âš¡ Latencia ultra-baja (p95 < 100ms)
- ğŸ”§ FÃ¡cil de integrar y usar
- ğŸ“Š Dashboards y mÃ©tricas avanzadas
- ğŸŒ Infraestructura global
- ğŸ¯ Especializada en ML/AI workloads

**Desventajas:**
- ğŸ’° MÃ¡s costoso que alternativas
- ğŸŒ Requiere conexiÃ³n a internet siempre
- âš ï¸ Vendor lock-in

**CuÃ¡ndo usar:**
```python
âœ… ProducciÃ³n a gran escala (>10M vectores)
âœ… Aplicaciones que requieren latencia mÃ­nima
âœ… Sistemas de recomendaciÃ³n en tiempo real
âœ… BÃºsqueda semÃ¡ntica de alto rendimiento
âœ… Startups con inversiÃ³n que priorizan velocidad
âœ… Empresas que necesitan soporte enterprise
âŒ Proyectos con presupuesto limitado
âŒ Aplicaciones que necesitan funcionar offline
âŒ Requisitos de hosting on-premise
```

**Ejemplo de uso:**
```python
from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone, ServerlessSpec

pc = Pinecone(api_key="...")
index = pc.Index("mi-indice")

vector_store = PineconeVectorStore(
    index=index,
    embedding=OpenAIEmbeddings()
)

# BÃºsqueda MMR (diversidad)
retriever = vector_store.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 5}
)
```

---

### ğŸ¯ GuÃ­a de SelecciÃ³n RÃ¡pida

#### Por Caso de Uso:

**ğŸ§ª ExperimentaciÃ³n y Aprendizaje**
â†’ `InMemoryVectorStore` o `ChromaDB`

**ğŸ’» AplicaciÃ³n Local de Alto Rendimiento**
â†’ `FAISS`

**ğŸ—ï¸ Desarrollo de MVP**
â†’ `ChromaDB`

**ğŸš€ Startup/ProducciÃ³n (presupuesto medio)**
â†’ `AstraDB`

**ğŸ¢ Empresa/ProducciÃ³n (alto rendimiento)**
â†’ `Pinecone`

#### Por TamaÃ±o de Dataset:

- **< 1,000 documentos**: InMemoryVectorStore
- **1K - 100K documentos**: ChromaDB o FAISS
- **100K - 1M documentos**: FAISS o ChromaDB
- **1M - 10M documentos**: AstraDB o Pinecone
- **> 10M documentos**: Pinecone

#### Por Presupuesto:

- **$0/mes**: InMemoryVectorStore, FAISS, ChromaDB
- **$0-50/mes**: AstraDB (free tier + paid)
- **$70+/mes**: Pinecone (standard tier)

## ğŸ” Estrategias de BÃºsqueda HÃ­brida

Las estrategias de bÃºsqueda hÃ­brida son tÃ©cnicas avanzadas que mejoran significativamente la calidad de la recuperaciÃ³n de documentos en sistemas RAG. El mÃ³dulo 004 cubre tres estrategias fundamentales:

### ğŸ“Š Comparativa de Estrategias

| Estrategia | Tipo | Complejidad | Latencia | Mejora en PrecisiÃ³n | Mejora en Diversidad | Uso Ideal |
|------------|------|-------------|----------|---------------------|---------------------|-----------|
| **Dense + Sparse** | HÃ­brida | Media | Media | â­â­â­â­ | â­â­â­ | BÃºsquedas que requieren tanto coincidencia exacta como semÃ¡ntica |
| **Reranking** | Post-procesamiento | Alta | Alta | â­â­â­â­â­ | â­â­ | Cuando la precisiÃ³n es crÃ­tica y hay presupuesto de latencia |
| **MMR** | DiversificaciÃ³n | Baja | Baja | â­â­â­ | â­â­â­â­â­ | Evitar redundancia y cubrir mÃºltiples aspectos de una consulta |

### ğŸ”¹ 1. BÃºsqueda HÃ­brida (Dense + Sparse)

**Â¿QuÃ© es?**
Combina dos enfoques complementarios de recuperaciÃ³n:
- **RecuperaciÃ³n Densa (FAISS)**: Usa embeddings vectoriales para capturar similitud semÃ¡ntica
- **RecuperaciÃ³n Dispersa (BM25)**: Usa coincidencia de palabras clave para tÃ©rminos exactos

**Ventajas:**
- âœ… Captura tanto significado semÃ¡ntico como coincidencias exactas
- âœ… Mejor rendimiento en consultas con tÃ©rminos especÃ­ficos
- âœ… Reduce falsos negativos de cada mÃ©todo individual

**CuÃ¡ndo usar:**
```python
âœ… BÃºsquedas tÃ©cnicas con terminologÃ­a especÃ­fica
âœ… Consultas que mezclan conceptos y nombres propios
âœ… Cuando necesitas balance entre precisiÃ³n y cobertura
âŒ Consultas muy simples donde un mÃ©todo bastarÃ­a
```

**Ejemplo:**
```python
from langchain.retrievers import EnsembleRetriever

hybrid_retriever = EnsembleRetriever(
    retrievers=[dense_retriever, sparse_retriever],
    weights=[0.7, 0.3]  # 70% semÃ¡ntico, 30% palabras clave
)
```

---

### ğŸ”¹ 2. Reranking (Reordenamiento)

**Â¿QuÃ© es?**
Proceso de dos etapas donde primero se recuperan documentos rÃ¡pidamente y luego se reordenan con un modelo mÃ¡s preciso:
1. **Primera etapa**: Recuperador rÃ¡pido obtiene top-k documentos (ej: k=20)
2. **Segunda etapa**: LLM o cross-encoder re-puntÃºa y reordena por relevancia real

**Ventajas:**
- âœ… Mejora significativa en precisiÃ³n sin perder velocidad inicial
- âœ… Los documentos mÃ¡s relevantes aparecen primero
- âœ… Reduce documentos poco relevantes en el contexto del LLM
- âœ… Mejora calidad de respuestas finales

**CuÃ¡ndo usar:**
```python
âœ… Aplicaciones donde la precisiÃ³n es crÃ­tica
âœ… Cuando tienes presupuesto de latencia (aÃ±ade ~500ms-2s)
âœ… Datasets grandes con mucho ruido
âœ… Consultas complejas con mÃºltiples intenciones
âŒ Aplicaciones de latencia ultra-baja
âŒ Datasets pequeÃ±os y bien curados
```

**Ejemplo:**
```python
# 1. Recuperar top-8 documentos
retrieved_docs = retriever.invoke(query)

# 2. Reordenar con LLM
reranking_prompt = """Clasifica estos documentos por relevancia..."""
reranked_docs = llm_rerank(retrieved_docs, query)
```

---

### ğŸ”¹ 3. MMR (Maximal Marginal Relevance)

**Â¿QuÃ© es?**
TÃ©cnica que balancea dos objetivos al seleccionar documentos:
- **Relevancia**: QuÃ© tan relacionado estÃ¡ con la consulta
- **Diversidad**: QuÃ© tan diferente es de documentos ya seleccionados

**Ventajas:**
- âœ… Evita informaciÃ³n repetitiva y redundante
- âœ… Cubre mÃºltiples aspectos de una consulta
- âœ… Perspectiva mÃ¡s amplia del tema
- âœ… Mejora respuestas para preguntas multifacÃ©ticas

**CuÃ¡ndo usar:**
```python
âœ… Consultas amplias con mÃºltiples sub-temas
âœ… Cuando quieres cobertura completa de un tema
âœ… Evitar documentos muy similares entre sÃ­
âœ… Exploratory search (bÃºsqueda exploratoria)
âŒ Consultas muy especÃ­ficas de una sola respuesta
âŒ Cuando necesitas documentos altamente enfocados
```

**Ejemplo:**
```python
retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={
        "k": 5,
        "fetch_k": 20  # Primero obtiene 20, luego MMR selecciona 5 diversos
    }
)
```

---

### ğŸ¯ GuÃ­a de SelecciÃ³n RÃ¡pida

**Por Caso de Uso:**

**ğŸ“š BÃºsqueda de documentaciÃ³n tÃ©cnica**
â†’ `Dense + Sparse` (captura tÃ©rminos tÃ©cnicos exactos + contexto)

**ğŸ¯ Sistema de Q&A de alta precisiÃ³n**
â†’ `Reranking` (mÃ¡xima precisiÃ³n en respuestas)

**ğŸ”¬ InvestigaciÃ³n exploratoria**
â†’ `MMR` (mÃ¡xima diversidad y cobertura)

**ğŸ¢ Chatbot empresarial**
â†’ `Dense + Sparse + Reranking` (combinaciÃ³n de todas)

**Por Prioridad:**

- **Prioridad: PrecisiÃ³n** â†’ Reranking
- **Prioridad: Cobertura** â†’ MMR
- **Prioridad: Balance** â†’ Dense + Sparse
- **Prioridad: Latencia** â†’ MMR (mÃ¡s rÃ¡pido)

### ğŸ”„ Combinando Estrategias

Puedes combinar mÃºltiples estrategias para mejores resultados:

```python
# Pipeline Ã³ptimo: HÃ­brido â†’ Reranking â†’ MMR
# 1. BÃºsqueda hÃ­brida (Dense + Sparse)
hybrid_results = hybrid_retriever.invoke(query, k=20)

# 2. Reranking con LLM
reranked_results = rerank(hybrid_results)

# 3. MMR para diversidad final
final_results = apply_mmr(reranked_results, k=5)
```

Este enfoque multicapa es ideal para aplicaciones de producciÃ³n donde la calidad es crÃ­tica.

---

## ğŸ¯ Mejora de Consultas (Query Enhancement)

Las tÃ©cnicas de mejora de consultas transforman y optimizan las consultas del usuario antes de la recuperaciÃ³n de documentos, mejorando significativamente la calidad de los resultados en sistemas RAG. El mÃ³dulo 005 cubre tres estrategias fundamentales para optimizar consultas.

### ğŸ“Š Comparativa de TÃ©cnicas de Query Enhancement

| TÃ©cnica | Tipo | Complejidad | Latencia | Costo | Mejora en PrecisiÃ³n | Casos de Uso |
|---------|------|-------------|----------|-------|---------------------|--------------|
| **Query Expansion** | Enriquecimiento | Baja | Media (~500ms-1s) | Medio | â­â­â­â­ | Consultas cortas, vagas o tÃ©cnicas |
| **Query Decomposition** | DivisiÃ³n | Alta | Alta (1s-3s) | Alto | â­â­â­â­â­ | Consultas complejas multi-aspecto |
| **HyDE** | TransformaciÃ³n | Media | Alta (~500ms-2s) | Alto | â­â­â­â­â­ | Vocabulary mismatch, consultas cortas |

### ğŸ”¹ 1. Query Expansion (ExpansiÃ³n de Consultas)

**Â¿QuÃ© es?**
TÃ©cnica que enriquece la consulta original agregando sinÃ³nimos, tÃ©rminos tÃ©cnicos, variaciones ortogrÃ¡ficas y contexto adicional usando un LLM.

**Â¿CÃ³mo funciona?**
```
Consulta: "Langchain memory"
    â†“ (LLM expande)
Consulta expandida: "Langchain memory, ConversationBufferMemory, ConversationSummaryMemory,
                     memory management, session context, state management, context storage..."
    â†“ (recuperaciÃ³n)
Documentos relevantes
```

**Ventajas:**
- âœ… Captura variaciones semÃ¡nticas y sinÃ³nimos
- âœ… Mejora recall (recuperaciÃ³n) de documentos relevantes
- âœ… Enriquece consultas vagas con terminologÃ­a especÃ­fica
- âœ… Reduce falsos negativos
- âœ… Especialmente Ãºtil cuando el usuario no conoce tÃ©rminos exactos

**Desventajas:**
- âš ï¸ Agrega latencia (~500ms-1s por llamada al LLM)
- âš ï¸ Puede sobre-expandir consultas ya muy especÃ­ficas
- âš ï¸ Costo de tokens por cada consulta
- âš ï¸ Calidad depende del LLM utilizado

**CuÃ¡ndo usar:**
```python
âœ… Consultas cortas o vagas ("memoria en IA")
âœ… BÃºsquedas tÃ©cnicas que requieren terminologÃ­a especÃ­fica
âœ… Cuando el usuario usa lenguaje coloquial
âœ… Bases de conocimiento con vocabulario diverso
âŒ Consultas ya muy especÃ­ficas y tÃ©cnicas
âŒ Aplicaciones de latencia crÃ­tica
```

**Ejemplo de implementaciÃ³n:**
```python
query_expansion_prompt = PromptTemplate.from_template("""
Expande la siguiente consulta agregando sinÃ³nimos y tÃ©rminos tÃ©cnicos:
Consulta: "{query}"
""")

expansion_chain = query_expansion_prompt | llm | StrOutputParser()
expanded_query = expansion_chain.invoke({"query": "Langchain memory"})
```

---

### ğŸ”¹ 2. Query Decomposition (DescomposiciÃ³n de Consultas)

**Â¿QuÃ© es?**
TÃ©cnica que divide una consulta compleja de mÃºltiples partes en sub-preguntas mÃ¡s simples y atÃ³micas que se procesan individualmente.

**Â¿CÃ³mo funciona?**
```
Consulta compleja: "Â¿CÃ³mo usa LangChain memoria y agentes vs CrewAI?"
    â†“ (LLM descompone)
Sub-pregunta 1: "Â¿QuÃ© mecanismos de memoria ofrece LangChain?"
Sub-pregunta 2: "Â¿CÃ³mo funcionan los agentes en LangChain?"
Sub-pregunta 3: "Â¿QuÃ© mecanismos de memoria ofrece CrewAI?"
Sub-pregunta 4: "Â¿CÃ³mo funcionan los agentes en CrewAI?"
    â†“ (recuperaciÃ³n individual)
Respuestas combinadas
```

**Ventajas:**
- âœ… PrecisiÃ³n excepcional en consultas complejas
- âœ… Permite razonamiento multi-hop (paso a paso)
- âœ… Cada sub-pregunta obtiene contexto especÃ­fico
- âœ… No se pierden aspectos de la pregunta original
- âœ… RecuperaciÃ³n mÃ¡s enfocada por aspecto
- âœ… Respuestas bien estructuradas

**Desventajas:**
- âš ï¸ Alta latencia (mÃºltiples llamadas al LLM)
- âš ï¸ Mayor costo (1 descomposiciÃ³n + N respuestas)
- âš ï¸ Complejidad en el parseo de sub-preguntas
- âš ï¸ Puede ser overkill para consultas simples

**CuÃ¡ndo usar:**
```python
âœ… Preguntas comparativas ("A vs B")
âœ… Consultas multi-aspecto (memoria + agentes + herramientas)
âœ… AnÃ¡lisis complejos con mÃºltiples entidades
âœ… InvestigaciÃ³n exploratoria de temas amplios
âœ… Cuando se necesita razonamiento estructurado
âŒ Consultas simples de un solo aspecto
âŒ Presupuesto de latencia muy limitado
```

**Ejemplo de implementaciÃ³n:**
```python
decomposition_prompt = PromptTemplate.from_template("""
DescompÃ³n esta pregunta en 2-4 sub-preguntas simples:
Pregunta: "{question}"
""")

# Descomponer y procesar cada sub-pregunta
sub_questions = decomposition_chain.invoke({"question": query})
for subq in sub_questions:
    docs = retriever.invoke(subq)
    answer = qa_chain.invoke({"input": subq, "context": docs})
```

---

### ğŸ”¹ 3. HyDE (Hypothetical Document Embeddings)

**Â¿QuÃ© es?**
TÃ©cnica que genera una respuesta hipotÃ©tica a la consulta usando un LLM, y luego busca documentos similares a esa respuesta hipotÃ©tica en lugar de a la consulta original.

**Â¿CÃ³mo funciona?**
```
Consulta: "Â¿CuÃ¡ndo despidieron a Steve Jobs?"
    â†“ (LLM genera respuesta hipotÃ©tica)
Respuesta hipotÃ©tica: "Steve Jobs fue despedido de Apple en septiembre de 1985
                       debido a conflictos con la junta directiva..."
    â†“ (embedding de la respuesta)
    â†“ (bÃºsqueda vectorial)
Documentos similares a la respuesta hipotÃ©tica
```

**Ventajas:**
- âœ… Resuelve vocabulary mismatch (pregunta vs respuesta)
- âœ… Excelente para consultas cortas
- âœ… Alinea formato pregunta-respuesta
- âœ… Captura el estilo de respuestas esperadas
- âœ… Mejora significativa en recall

**Desventajas:**
- âš ï¸ Alta latencia (LLM + bÃºsqueda)
- âš ï¸ Costo adicional por generaciÃ³n
- âš ï¸ Calidad depende del documento hipotÃ©tico generado
- âš ï¸ Puede ser innecesario si documentos estÃ¡n bien alineados

**CuÃ¡ndo usar:**
```python
âœ… Consultas en lenguaje natural vs documentos tÃ©cnicos
âœ… Preguntas cortas que necesitan expansiÃ³n semÃ¡ntica
âœ… Vocabulario diferente entre usuario y documentos
âœ… FAQs donde buscas respuestas, no preguntas
âœ… Dominios especializados (medicina, leyes)
âŒ BÃºsqueda de palabras clave exactas
âŒ Latencia crÃ­tica
âŒ Documentos y consultas ya alineados
```

**Ejemplo de implementaciÃ³n:**
```python
from langchain.chains.hyde.base import HypotheticalDocumentEmbedder

# OpciÃ³n 1: Con prompt predeterminado
hyde_embedder = HypotheticalDocumentEmbedder.from_llm(
    llm=llm,
    base_embeddings=embeddings,
    prompt_key="web_search"  # o "sci_fact", "fiqa", "trec_news"
)

# OpciÃ³n 2: Con prompt personalizado
custom_prompt = PromptTemplate.from_template(
    "Genera una respuesta hipotÃ©tica para: {query}"
)
hyde_embedder = HypotheticalDocumentEmbedder.from_llm(
    llm=llm,
    base_embeddings=embeddings,
    custom_prompt=custom_prompt
)

vectorstore = Chroma.from_documents(docs, hyde_embedder)
```

---

### ğŸ¯ GuÃ­a de SelecciÃ³n RÃ¡pida

**Por Problema:**

**Consulta vaga o corta**
â†’ `Query Expansion` (agrega contexto y sinÃ³nimos)

**Consulta compleja multi-aspecto**
â†’ `Query Decomposition` (divide en sub-preguntas)

**Vocabulary mismatch**
â†’ `HyDE` (genera respuesta hipotÃ©tica)

**Por Prioridad:**

- **Prioridad: MÃ¡xima precisiÃ³n** â†’ Query Decomposition
- **Prioridad: Resolver vocabulary gap** â†’ HyDE
- **Prioridad: Balance costo/beneficio** â†’ Query Expansion
- **Prioridad: Latencia mÃ­nima** â†’ Query Expansion (mÃ¡s rÃ¡pida)

**Por Tipo de Consulta:**

- **"memoria IA"** (vaga) â†’ Query Expansion
- **"Â¿CÃ³mo se compara X con Y en A, B y C?"** (compleja) â†’ Query Decomposition
- **"Â¿CuÃ¡ndo pasÃ³ X?"** (corta, respuesta especÃ­fica) â†’ HyDE

### ğŸ”„ ComparaciÃ³n con Otras Estrategias

| TÃ©cnica | Latencia | Costo | PrecisiÃ³n | Recall | Uso Ideal |
|---------|----------|-------|-----------|--------|-----------|
| **BÃºsqueda Simple** | âš¡ Muy baja | ğŸ’° Bajo | â­â­â­ | â­â­â­ | Consultas bien formadas |
| **Query Expansion** | âš¡ Media | ğŸ’° Medio | â­â­â­â­ | â­â­â­â­ | Consultas vagas |
| **Query Decomposition** | ğŸŒ Alta | ğŸ’°ğŸ’° Alto | â­â­â­â­â­ | â­â­â­â­ | Consultas complejas |
| **HyDE** | ğŸŒ Alta | ğŸ’°ğŸ’° Alto | â­â­â­â­â­ | â­â­â­â­â­ | Vocabulary mismatch |
| **Dense+Sparse** | âš¡ Media | ğŸ’° Bajo | â­â­â­â­ | â­â­â­â­ | Balance semÃ¡ntico/exacto |
| **Reranking** | ğŸŒ Alta | ğŸ’°ğŸ’° Alto | â­â­â­â­â­ | â­â­â­ | PrecisiÃ³n crÃ­tica |

### ğŸ”„ Combinando TÃ©cnicas

Puedes combinar Query Enhancement con otras estrategias para resultados Ã³ptimos:

```python
# Pipeline Ã³ptimo: Decomposition â†’ Expansion â†’ Dense+Sparse â†’ Reranking
# 1. Descomponer consulta compleja
sub_questions = decomposition_chain.invoke({"question": complex_query})

# 2. Expandir cada sub-pregunta
for subq in sub_questions:
    expanded_subq = expansion_chain.invoke({"query": subq})

    # 3. BÃºsqueda hÃ­brida (Dense + Sparse)
    results = hybrid_retriever.invoke(expanded_subq, k=20)

    # 4. Reranking para precisiÃ³n final
    final_docs = rerank(results, subq, k=5)
```

Este enfoque multicapa es ideal para aplicaciones enterprise donde la calidad es crÃ­tica y hay presupuesto de latencia.

---

## ğŸ–¼ï¸ RAG Multimodal

El RAG Multimodal extiende las capacidades de RAG tradicional para procesar y comprender no solo texto, sino tambiÃ©n imÃ¡genes, grÃ¡ficos, diagramas y otros elementos visuales presentes en documentos. Esta tÃ©cnica es fundamental para trabajar con documentos del mundo real que combinan mÃºltiples modalidades de informaciÃ³n.

### ğŸ¯ Â¿QuÃ© es RAG Multimodal?

**RAG Tradicional vs RAG Multimodal:**

| Aspecto | RAG Tradicional | RAG Multimodal |
|---------|----------------|----------------|
| **Entrada** | Solo texto | Texto + ImÃ¡genes + GrÃ¡ficos |
| **Embeddings** | Un modelo de texto | CLIP (unificado texto-imagen) |
| **LLM** | GPT-3.5/4 estÃ¡ndar | GPT-4 Vision / LLaVA |
| **Casos de uso** | Documentos textuales | PDFs con grÃ¡ficos, presentaciones, informes |
| **Complejidad** | Media | Alta |

**Â¿Por quÃ© es importante?**

Los documentos del mundo real raramente son solo texto:
- ğŸ“Š **Informes financieros** con grÃ¡ficos de barras y tendencias
- ğŸ“„ **Documentos tÃ©cnicos** con diagramas y arquitecturas
- ğŸ¥ **Registros mÃ©dicos** con imÃ¡genes de rayos X y escaneos
- ğŸ“‘ **Presentaciones** con infografÃ­as y visualizaciones
- ğŸ“š **Libros educativos** con ilustraciones y esquemas

### ğŸ”§ Componentes Clave

#### 1. **CLIP (Contrastive Language-Image Pre-training)**

**Â¿QuÃ© es CLIP?**
- Modelo de OpenAI entrenado con 400 millones de pares (imagen, texto)
- Genera embeddings en el **mismo espacio vectorial** para texto e imÃ¡genes
- Permite buscar imÃ¡genes con texto y viceversa

**Arquitectura:**
```
Texto: "grÃ¡fico de ventas"  â†’  [Vision Transformer]  â†’  Vector 512D
                                                           â†“ (mismo espacio)
Imagen: [grÃ¡fico.png]       â†’  [Vision Transformer]  â†’  Vector 512D
```

**Ventajas de CLIP:**
- âœ… BÃºsqueda semÃ¡ntica unificada (texto puede encontrar imÃ¡genes)
- âœ… No requiere etiquetado manual de imÃ¡genes
- âœ… Funciona con mÃºltiples idiomas
- âœ… Generaliza bien a conceptos nuevos

**Alternativas a CLIP:**
- **BLIP-2**: Modelo mÃ¡s reciente con mejor comprensiÃ³n
- **LLaVA**: Combina CLIP con LLaMA para generaciÃ³n
- **ALIGN**: Modelo de Google similar a CLIP

#### 2. **GPT-4 Vision (GPT-4V)**

**Capacidades:**
- ğŸ‘ï¸ Interpreta imÃ¡genes, grÃ¡ficos, tablas, diagramas
- ğŸ“Š Extrae datos de visualizaciones
- ğŸ¨ Describe contenido visual en detalle
- ğŸ”— Relaciona informaciÃ³n visual con contexto textual

**Formato de entrada:**
```json
{
  "role": "user",
  "content": [
    {"type": "text", "text": "Â¿QuÃ© muestra este grÃ¡fico?"},
    {"type": "image_url", "image_url": "data:image/png;base64,..."}
  ]
}
```

### ğŸ”„ Flujo del Pipeline Multimodal

```
                    PDF CON TEXTO E IMÃGENES
                             â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â†“                             â†“
         TEXTO                          IMÃGENES
              â†“                             â†“
    [Text Splitter]                  [ExtracciÃ³n]
              â†“                             â†“
      Chunks de Texto              ImÃ¡genes PIL
              â†“                             â†“
    [CLIP Text Encoder]          [CLIP Image Encoder]
              â†“                             â†“
       Vectores 512D                 Vectores 512D
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â†“
                    FAISS VECTOR STORE
                    (Embeddings Unificados)
                             â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â†“                 â†“
              CONSULTA           RECUPERACIÃ“N
            (texto usuario)    (texto + imÃ¡genes)
                    â†“                 â†“
              [CLIP Encoder]    Docs Relevantes
                    â†“                 â†“
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â†“
                      GPT-4 VISION
                   (procesa texto + imÃ¡genes)
                             â†“
                        RESPUESTA
```

### ğŸ“‹ ImplementaciÃ³n Paso a Paso

#### **Paso 1: Extraer Texto e ImÃ¡genes del PDF**

```python
import fitz  # PyMuPDF
from PIL import Image
import io

# Abrir PDF
doc = fitz.open("documento.pdf")

for page in doc:
    # Extraer texto
    text = page.get_text()

    # Extraer imÃ¡genes
    for img in page.get_images():
        xref = img[0]
        base_image = doc.extract_image(xref)
        image_bytes = base_image["image"]
        pil_image = Image.open(io.BytesIO(image_bytes))
```

#### **Paso 2: Generar Embeddings con CLIP**

```python
from transformers import CLIPProcessor, CLIPModel
import torch

# Inicializar CLIP
clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

def embed_image(image):
    """Generar embedding de imagen"""
    inputs = clip_processor(images=image, return_tensors="pt")
    with torch.no_grad():
        features = clip_model.get_image_features(**inputs)
        return features / features.norm(dim=-1, keepdim=True)

def embed_text(text):
    """Generar embedding de texto"""
    inputs = clip_processor(text=text, return_tensors="pt", max_length=77)
    with torch.no_grad():
        features = clip_model.get_text_features(**inputs)
        return features / features.norm(dim=-1, keepdim=True)
```

#### **Paso 3: Almacenar en Vector Store Unificado**

```python
from langchain_community.vectorstores import FAISS

# Crear vector store con embeddings de texto e imÃ¡genes
vector_store = FAISS.from_embeddings(
    text_embeddings=[(doc.page_content, emb) for doc, emb in zip(all_docs, embeddings)],
    embedding=None,  # Ya tenemos embeddings precalculados
    metadatas=[doc.metadata for doc in all_docs]
)
```

#### **Paso 4: RecuperaciÃ³n Multimodal**

```python
def retrieve_multimodal(query, k=5):
    """Recuperar texto e imÃ¡genes relevantes"""
    # Convertir consulta a embedding CLIP
    query_embedding = embed_text(query)

    # Buscar documentos similares (texto e imÃ¡genes)
    results = vector_store.similarity_search_by_vector(
        embedding=query_embedding,
        k=k
    )
    return results
```

#### **Paso 5: Crear Mensaje para GPT-4 Vision**

```python
import base64
from langchain.schema.messages import HumanMessage

def create_multimodal_message(query, retrieved_docs):
    """Crear mensaje con texto e imÃ¡genes para GPT-4V"""
    content = [{"type": "text", "text": f"Pregunta: {query}\n\nContexto:\n"}]

    # Agregar texto recuperado
    for doc in retrieved_docs:
        if doc.metadata["type"] == "text":
            content.append({"type": "text", "text": doc.page_content})

        # Agregar imÃ¡genes en base64
        elif doc.metadata["type"] == "image":
            image_base64 = image_data_store[doc.metadata["image_id"]]
            content.append({
                "type": "image_url",
                "image_url": {"url": f"data:image/png;base64,{image_base64}"}
            })

    return HumanMessage(content=content)
```

#### **Paso 6: Generar Respuesta con GPT-4 Vision**

```python
from langchain.chat_models import init_chat_model

# Inicializar GPT-4 Vision
llm = init_chat_model("openai:gpt-4-vision-preview")

def multimodal_rag_pipeline(query):
    # 1. Recuperar documentos relevantes
    docs = retrieve_multimodal(query, k=5)

    # 2. Crear mensaje multimodal
    message = create_multimodal_message(query, docs)

    # 3. Generar respuesta
    response = llm.invoke([message])
    return response.content
```

### ğŸ¯ Casos de Uso

#### **1. AnÃ¡lisis de Informes Financieros**
```python
query = "Â¿CuÃ¡les fueron las tendencias de ingresos en Q3 segÃºn el grÃ¡fico?"
# Recupera: texto sobre Q3 + grÃ¡fico de barras
# GPT-4V interpreta el grÃ¡fico y combina con texto
```

#### **2. DocumentaciÃ³n TÃ©cnica**
```python
query = "Explica la arquitectura del sistema mostrada en el diagrama"
# Recupera: descripciÃ³n textual + diagrama de arquitectura
# GPT-4V analiza el diagrama y lo relaciona con el texto
```

#### **3. Informes MÃ©dicos**
```python
query = "Â¿QuÃ© anomalÃ­as se observan en la radiografÃ­a?"
# Recupera: notas mÃ©dicas + imagen de rayos X
# GPT-4V examina la imagen y proporciona anÃ¡lisis
```

### âœ… Ventajas del RAG Multimodal

| Ventaja | DescripciÃ³n |
|---------|-------------|
| **ğŸ¯ ComprensiÃ³n Completa** | Procesa toda la informaciÃ³n, no solo texto |
| **ğŸ“Š AnÃ¡lisis Visual** | Interpreta grÃ¡ficos, tablas, diagramas |
| **ğŸ” BÃºsqueda Unificada** | Una consulta encuentra texto e imÃ¡genes relevantes |
| **ğŸ’¡ Contexto Rico** | El LLM ve exactamente lo que ve el usuario |
| **ğŸ¨ Versatilidad** | Funciona con cualquier tipo de documento visual |

### âš ï¸ DesafÃ­os y Consideraciones

| DesafÃ­o | SoluciÃ³n |
|---------|----------|
| **ğŸ’° Costo Alto** | GPT-4 Vision es mÃ¡s costoso que GPT-4 estÃ¡ndar |
| **ğŸŒ Latencia** | Procesamiento de imÃ¡genes aÃ±ade tiempo (~2-5s extra) |
| **ğŸ“¦ TamaÃ±o de Contexto** | ImÃ¡genes consumen muchos tokens (cada imagen â‰ˆ 85-170 tokens) |
| **ğŸ¨ Calidad de Imagen** | ImÃ¡genes de baja resoluciÃ³n o borrosas limitan la comprensiÃ³n |
| **ğŸ’¾ Almacenamiento** | Guardar imÃ¡genes en base64 ocupa mucho espacio |

### ğŸ”„ ComparaciÃ³n con Alternativas

| Enfoque | Ventajas | Desventajas |
|---------|----------|-------------|
| **RAG Multimodal (CLIP + GPT-4V)** | âœ… ComprensiÃ³n visual completa<br>âœ… BÃºsqueda semÃ¡ntica unificada | âŒ Costoso<br>âŒ Alta latencia |
| **OCR + RAG Tradicional** | âœ… MÃ¡s econÃ³mico<br>âœ… MÃ¡s rÃ¡pido | âŒ Pierde informaciÃ³n visual<br>âŒ No interpreta grÃ¡ficos |
| **Image Captioning + RAG** | âœ… Balance costo/beneficio | âŒ Captions pueden ser inexactos<br>âŒ Pierde detalles |
| **Table Extraction + RAG** | âœ… Bueno para tablas | âŒ No funciona con grÃ¡ficos<br>âŒ Limitado a estructuras |

### ğŸ“Š CuÃ¡ndo Usar RAG Multimodal

**âœ… Usa RAG Multimodal cuando:**
- Tus documentos contienen informaciÃ³n visual crÃ­tica (grÃ¡ficos, diagramas)
- Necesitas respuestas que requieren interpretar imÃ¡genes
- El presupuesto y latencia no son limitantes
- La precisiÃ³n visual es mÃ¡s importante que la velocidad

**âŒ NO uses RAG Multimodal cuando:**
- Tus documentos son mayormente texto
- El presupuesto es muy limitado
- La latencia debe ser ultra-baja (<1s)
- Las imÃ¡genes son decorativas, no informativas

### ğŸš€ Mejoras y Optimizaciones

**1. CachÃ© de Embeddings**
```python
# Guardar embeddings CLIP para evitar recalcular
vector_store.save_local("embeddings_cache")
```

**2. CompresiÃ³n de ImÃ¡genes**
```python
# Reducir tamaÃ±o de imÃ¡genes para ahorrar tokens
from PIL import Image
image = image.resize((800, 600), Image.LANCZOS)
```

**3. Filtrado Inteligente**
```python
# Solo enviar imÃ¡genes realmente relevantes a GPT-4V
if doc.metadata["type"] == "image" and similarity_score > 0.8:
    # Incluir imagen
```

**4. Batch Processing**
```python
# Procesar mÃºltiples pÃ¡ginas en paralelo
from concurrent.futures import ThreadPoolExecutor
with ThreadPoolExecutor() as executor:
    embeddings = list(executor.map(embed_image, images))
```

### ğŸ› ï¸ Herramientas y LibrerÃ­as

| Herramienta | PropÃ³sito | Alternativa |
|-------------|-----------|-------------|
| **CLIP** | Embeddings unificados | BLIP-2, ALIGN |
| **GPT-4 Vision** | LLM multimodal | LLaVA, Gemini Pro Vision |
| **PyMuPDF (fitz)** | ExtracciÃ³n de PDF | pdfplumber, PyPDF2 |
| **PIL/Pillow** | Procesamiento de imÃ¡genes | OpenCV |
| **FAISS** | Vector store | ChromaDB, Pinecone |

---

## ğŸ”„ Fundamentos de LangGraph

LangGraph es un framework de LangChain diseÃ±ado para construir aplicaciones con estado usando grafos. Es especialmente Ãºtil para crear agentes, chatbots complejos y flujos de trabajo que requieren gestiÃ³n avanzada de estado y enrutamiento condicional.

### ğŸ¯ Â¿QuÃ© es LangGraph?

**LangGraph** permite construir aplicaciones basadas en grafos donde:
- **Nodos** representan funciones que procesan el estado
- **Aristas** definen el flujo entre nodos
- **Estado** se comparte y actualiza a travÃ©s del grafo
- **Enrutamiento condicional** permite decisiones dinÃ¡micas basadas en el estado

### ğŸ“Š Conceptos Clave

| Concepto | DescripciÃ³n | Ejemplo |
|----------|-------------|---------|
| **State Schema** | Define la estructura de datos del grafo | `TypedDict`, `DataClass`, `Pydantic` |
| **Nodos** | Funciones que procesan y actualizan el estado | `chatbot(state)`, `tool_executor(state)` |
| **Aristas** | Conexiones entre nodos (fijas o condicionales) | `START â†’ chatbot â†’ END` |
| **Reducers** | Funciones que combinan estados (ej: `add_messages`) | Agregar mensajes sin sobrescribir |
| **Herramientas** | Funciones externas que el LLM puede llamar | BÃºsqueda web, cÃ¡lculos, APIs |

### ğŸ”¹ Esquemas de Estado

LangGraph soporta 3 formas de definir el estado del grafo:

#### 1. **TypedDict** (Solo Type Hints)
```python
from typing_extensions import TypedDict
from typing import Annotated
from langgraph.graph.message import add_messages

class State(TypedDict):
    messages: Annotated[list, add_messages]
```

**CaracterÃ­sticas:**
- âœ… Simple y rÃ¡pido
- âœ… IntegraciÃ³n nativa con Python
- âŒ **NO valida en tiempo de ejecuciÃ³n**
- âŒ Los errores de tipo solo se detectan durante la ejecuciÃ³n

**CuÃ¡ndo usar:**
- Prototipos rÃ¡pidos
- Cuando confÃ­as en los datos de entrada
- Testing y desarrollo

#### 2. **DataClass** (Estructura de Datos)
```python
from dataclasses import dataclass

@dataclass
class State:
    name: str
    game: Literal["cricket", "badminton"]
```

**CaracterÃ­sticas:**
- âœ… Sintaxis concisa y limpia
- âœ… Acceso a atributos con notaciÃ³n de punto (`state.name`)
- âœ… MÃ©todos autogenerados (`__init__`, `__repr__`)
- âš ï¸ ValidaciÃ³n bÃ¡sica solo de tipos

**CuÃ¡ndo usar:**
- Cuando necesitas clases de datos estructuradas
- Mejor legibilidad del cÃ³digo
- Proyectos de tamaÃ±o mediano

#### 3. **Pydantic** (ValidaciÃ³n Robusta) â­ Recomendado
```python
from pydantic import BaseModel

class State(BaseModel):
    name: str
    age: int
```

**CaracterÃ­sticas:**
- âœ… **ValidaciÃ³n completa en tiempo de ejecuciÃ³n**
- âœ… Mensajes de error claros y descriptivos
- âœ… ConversiÃ³n automÃ¡tica de tipos cuando es posible
- âœ… Validadores personalizados
- âœ… IntegraciÃ³n perfecta con FastAPI y otros frameworks

**CuÃ¡ndo usar:**
- Aplicaciones de producciÃ³n âœ…
- Cuando necesitas validaciÃ³n robusta
- APIs y servicios web
- Cuando recibes datos de fuentes externas

**Ejemplo de validaciÃ³n:**
```python
# TypedDict: âŒ No valida, error en tiempo de ejecuciÃ³n posterior
graph.invoke({"name": 123})  # Acepta pero falla despuÃ©s

# Pydantic: âœ… Valida inmediatamente
graph.invoke({"name": 123})  # ValidationError: Input should be a valid string
```

### ğŸ› ï¸ Componentes de un Grafo LangGraph

#### **1. Nodos (Nodes)**
Funciones que procesan el estado:

```python
def chatbot(state: State):
    """Nodo que procesa mensajes con un LLM"""
    return {"messages": [llm.invoke(state["messages"])]}
```

#### **2. Aristas (Edges)**
Conexiones entre nodos:

```python
# Arista fija
builder.add_edge(START, "chatbot")
builder.add_edge("chatbot", END)

# Arista condicional
builder.add_conditional_edges(
    "chatbot",
    tools_condition,  # FunciÃ³n que decide la ruta
)
```

#### **3. Reductores (Reducers)**
Controlan cÃ³mo se actualizan los valores del estado:

```python
from langgraph.graph.message import add_messages

# Sin reducer: sobrescribe
messages: list  # Nuevo valor reemplaza el anterior

# Con reducer: agrega
messages: Annotated[list, add_messages]  # Nuevo valor se agrega al anterior
```

### ğŸ”§ Herramientas en LangGraph

LangGraph permite que los LLMs llamen a herramientas externas:

#### **Herramientas Disponibles:**

| Herramienta | DescripciÃ³n | Uso Ideal |
|-------------|-------------|-----------|
| **Arxiv** | BÃºsqueda de papers cientÃ­ficos | InvestigaciÃ³n acadÃ©mica |
| **Wikipedia** | InformaciÃ³n enciclopÃ©dica | Conocimiento general |
| **Tavily** | BÃºsqueda web optimizada para LLMs | Noticias y contenido actual |
| **Custom Functions** | Funciones Python personalizadas | CÃ¡lculos, transformaciones |

#### **Flujo con Herramientas:**

```
Usuario â†’ LLM (con herramientas vinculadas)
           â†“
      Â¿Necesita herramienta?
           â†“
        SÃ â”Œâ”€â”´â”€â” NO
           â†“     â†“
    Ejecutar â†’ END
    herramienta
           â†“
      Respuesta
```

#### **Ejemplo de ImplementaciÃ³n:**

```python
from langchain_groq import ChatGroq
from langchain_community.tools import ArxivQueryRun, WikipediaQueryRun
from langgraph.prebuilt import ToolNode, tools_condition

# 1. Definir herramientas
tools = [arxiv_tool, wiki_tool, tavily_tool]

# 2. Vincular herramientas al LLM
llm_with_tools = llm.bind_tools(tools)

# 3. Crear nodos
def tool_calling_llm(state):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}

# 4. Construir grafo
builder.add_node("llm", tool_calling_llm)
builder.add_node("tools", ToolNode(tools))

# 5. Enrutamiento condicional
builder.add_conditional_edges("llm", tools_condition)
# tools_condition enruta a "tools" si hay llamada a herramienta, sino a END
```

### ğŸ“‹ Notebooks del MÃ³dulo

#### **2-chatbot.ipynb** - Chatbot Simple
- ConstrucciÃ³n de un chatbot bÃ¡sico con LangGraph
- Uso de mensajes como estado
- Reducer `add_messages` para mantener historial
- Streaming de respuestas

#### **3-DataclassStateSchema.ipynb** - Esquemas de Estado
- ComparaciÃ³n entre TypedDict y DataClass
- Nodos y aristas condicionales
- Enrutamiento aleatorio con `decide_play`
- Diferencias en validaciÃ³n y acceso a datos

#### **4-pydantic.ipynb** - ValidaciÃ³n con Pydantic
- Uso de Pydantic para validaciÃ³n robusta
- Ventajas sobre TypedDict
- DetecciÃ³n temprana de errores
- Mejores prÃ¡cticas para producciÃ³n

#### **5-ChainsLangGraph.ipynb** - Cadenas y Herramientas
- IntegraciÃ³n de herramientas con LLMs
- Uso de `bind_tools()` y `ToolNode`
- Enrutamiento condicional con `tools_condition`
- Manejo de mensajes multimodales

#### **6-chatbotswithmultipletools.ipynb** - Chatbot Avanzado
- Chatbot con mÃºltiples herramientas (Arxiv, Wikipedia, Tavily)
- SelecciÃ³n inteligente de herramientas por el LLM
- Pipeline completo: consulta â†’ decisiÃ³n â†’ herramienta â†’ respuesta
- Casos de uso: investigaciÃ³n, enciclopedia, noticias

### âœ… Ventajas de LangGraph

| Ventaja | DescripciÃ³n |
|---------|-------------|
| **ğŸ¯ Control Total** | Defines exactamente el flujo de tu aplicaciÃ³n |
| **ğŸ”„ Estado Compartido** | El estado se propaga automÃ¡ticamente entre nodos |
| **ğŸ›¤ï¸ Enrutamiento Flexible** | Decisiones dinÃ¡micas basadas en el estado actual |
| **ğŸ› ï¸ Herramientas Integradas** | Conecta fÃ¡cilmente APIs, bases de datos, funciones |
| **ğŸ“Š VisualizaciÃ³n** | Genera diagramas del grafo automÃ¡ticamente |
| **ğŸ§© Modular** | Cada nodo es independiente y reutilizable |

### ğŸ¯ Casos de Uso

**ğŸ¤– Chatbots Avanzados**
- Mantener contexto de conversaciÃ³n
- Llamar herramientas cuando sea necesario
- Enrutamiento basado en intenciÃ³n del usuario

**ğŸ” Agentes de InvestigaciÃ³n**
- BÃºsqueda en mÃºltiples fuentes (Arxiv, Wikipedia, Web)
- AgregaciÃ³n de informaciÃ³n
- Razonamiento multi-paso

**ğŸ¢ Flujos de Trabajo Empresariales**
- Aprobaciones multi-nivel
- Procesamiento condicional de documentos
- IntegraciÃ³n con sistemas legacy

**ğŸ§ª Pipelines RAG Complejos**
- Query Enhancement â†’ Retrieval â†’ Reranking â†’ Generation
- Decisiones adaptativas basadas en la calidad de resultados

### ğŸ”„ ComparaciÃ³n con Alternativas

| Framework | Estado | Enrutamiento | Curva Aprendizaje | Uso Ideal |
|-----------|--------|--------------|-------------------|-----------|
| **LangGraph** | âœ… ExplÃ­cito | âœ… Condicional | Media | Agentes complejos, flujos personalizados |
| **LangChain LCEL** | âš ï¸ ImplÃ­cito | âŒ Lineal | Baja | Cadenas simples, pipelines lineales |
| **CrewAI** | âœ… AutomÃ¡tico | âœ… AutomÃ¡tico | Baja | Multi-agentes colaborativos |
| **AutoGen** | âœ… Conversacional | âœ… AutomÃ¡tico | Alta | Agentes autÃ³nomos, investigaciÃ³n |

### ğŸš€ Mejores PrÃ¡cticas

1. **Usa Pydantic para ProducciÃ³n**
   ```python
   # âœ… Bueno
   class State(BaseModel):
       messages: list

   # âŒ Evitar en producciÃ³n
   class State(TypedDict):
       messages: list
   ```

2. **Siempre Usa Reducers para Listas**
   ```python
   # âœ… Bueno - los mensajes se agregan
   messages: Annotated[list, add_messages]

   # âŒ Malo - los mensajes se sobrescriben
   messages: list
   ```

3. **Nombra Nodos Descriptivamente**
   ```python
   # âœ… Bueno
   builder.add_node("validate_input", validate_fn)
   builder.add_node("call_llm", llm_fn)

   # âŒ Malo
   builder.add_node("node1", validate_fn)
   builder.add_node("node2", llm_fn)
   ```

4. **Usa tools_condition para Herramientas**
   ```python
   # âœ… Bueno - enrutamiento automÃ¡tico
   builder.add_conditional_edges("llm", tools_condition)

   # âŒ Malo - lÃ³gica manual propensa a errores
   builder.add_conditional_edges("llm", custom_routing)
   ```

---

## ğŸ¤– Arquitectura de Agentes

Los agentes son sistemas de IA que pueden tomar decisiones, usar herramientas y ejecutar tareas de manera autÃ³noma basÃ¡ndose en las entradas del usuario. El mÃ³dulo 008 se enfoca en la arquitectura de agentes ReAct, una de las arquitecturas mÃ¡s efectivas para construir agentes inteligentes.

### ğŸ¯ Â¿QuÃ© es ReAct?

**ReAct (Reason + Act)** es un paradigma de arquitectura de agentes que combina razonamiento y acciÃ³n de manera iterativa. El agente alterna entre:

1. **Razonar (Reason)**: El LLM piensa sobre quÃ© hacer a continuaciÃ³n
2. **Actuar (Act)**: El agente ejecuta una herramienta especÃ­fica
3. **Observar (Observe)**: El agente recibe los resultados de la herramienta
4. **Repetir**: Vuelve a razonar basÃ¡ndose en la nueva informaciÃ³n

### ğŸ“Š Ciclo ReAct

```
Usuario: "Â¿CuÃ¡les son las Ãºltimas noticias de IA?"
    â†“
[RAZONAR] LLM decide: "Necesito buscar en internet"
    â†“
[ACTUAR] Ejecuta herramienta: tavily.search("noticias IA")
    â†“
[OBSERVAR] Recibe: [lista de artÃ­culos]
    â†“
[RAZONAR] LLM decide: "Tengo suficiente informaciÃ³n"
    â†“
[RESPONDER] Genera respuesta estructurada al usuario
```

### ğŸ› ï¸ Componentes Clave

#### 1. **Herramientas (Tools)**

Las herramientas son funciones que el agente puede invocar:

| Herramienta | Tipo | DescripciÃ³n | Caso de Uso |
|-------------|------|-------------|-------------|
| **Arxiv** | BÃºsqueda acadÃ©mica | Consulta papers cientÃ­ficos | InvestigaciÃ³n, referencias acadÃ©micas |
| **Wikipedia** | Enciclopedia | InformaciÃ³n general y conceptos | Definiciones, contexto histÃ³rico |
| **Tavily** | BÃºsqueda web | Noticias y contenido actualizado | InformaciÃ³n reciente, tendencias |
| **Custom Functions** | MatemÃ¡ticas/LÃ³gica | add(), multiply(), divide() | CÃ¡lculos, transformaciones |

**Ejemplo de implementaciÃ³n:**
```python
from langchain_community.tools import ArxivQueryRun, WikipediaQueryRun
from langchain_community.tools.tavily_search import TavilySearchResults

# Configurar herramientas
arxiv = ArxivQueryRun(api_wrapper=ArxivAPIWrapper(top_k_results=2))
wiki = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(top_k_results=1))
tavily = TavilySearchResults()

# Funciones personalizadas
def add(a: int, b: int) -> int:
    """Suma dos nÃºmeros"""
    return a + b

# Lista de herramientas disponibles
tools = [arxiv, wiki, tavily, add, multiply, divide]
```

#### 2. **LLM con Herramientas Vinculadas**

El modelo de lenguaje debe estar "consciente" de las herramientas disponibles:

```python
from langchain_groq import ChatGroq

llm = ChatGroq(model="llama-3.1-8b-instant")

# Vincular herramientas al LLM
llm_with_tools = llm.bind_tools(tools)
```

Cuando se vinculan herramientas, el LLM:
- âœ… Recibe las descripciones y parÃ¡metros de cada herramienta
- âœ… Puede decidir cuÃ¡ndo y cÃ³mo usar cada herramienta
- âœ… Genera llamadas a herramientas en formato estructurado

#### 3. **Grafo ReAct con LangGraph**

```python
from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import ToolNode, tools_condition

# Definir el nodo del LLM
def tool_calling_llm(state: State):
    return {"messages": [llm_with_tools.invoke(state["messages"])]}

# Construir grafo
builder = StateGraph(State)
builder.add_node("tool_calling_llm", tool_calling_llm)
builder.add_node("tools", ToolNode(tools))

# Flujo del grafo
builder.add_edge(START, "tool_calling_llm")
builder.add_conditional_edges(
    "tool_calling_llm",
    tools_condition  # Enruta a "tools" o END segÃºn si hay llamadas
)
builder.add_edge("tools", "tool_calling_llm")  # Loop de vuelta al LLM

graph = builder.compile()
```

**Flujo del grafo:**
```
START â†’ tool_calling_llm â”€â”€â”
            â†‘               â”‚
            â”‚               â†“
         tools â† [tools_condition] â†’ END
```

### ğŸ’¾ Memoria en Agentes ReAct

Los agentes pueden mantener contexto entre mÃºltiples interacciones usando **checkpointers**:

#### **Sin Memoria:**
```python
# Cada invocaciÃ³n es independiente
graph.invoke({"messages": "Â¿CuÃ¡nto es 5 + 8?"})  # â†’ 13
graph.invoke({"messages": "Divide eso por 5"})   # âŒ No sabe quÃ© es "eso"
```

#### **Con Memoria (MemorySaver):**
```python
from langgraph.checkpoint.memory import MemorySaver

memory = MemorySaver()
graph_memory = builder.compile(checkpointer=memory)

# Configurar thread_id para sesiÃ³n
config = {"configurable": {"thread_id": "1"}}

# Primera interacciÃ³n
graph_memory.invoke({"messages": "Â¿CuÃ¡nto es 5 + 8?"}, config)  # â†’ 13

# Segunda interacciÃ³n - mantiene contexto
graph_memory.invoke({"messages": "Divide eso por 5"}, config)   # âœ… 13 / 5 = 2.6
```

**Ventajas de la memoria:**
- âœ… Conversaciones naturales con referencias contextuales
- âœ… El agente "recuerda" resultados previos
- âœ… Seguimiento de tareas multi-paso
- âœ… PersonalizaciÃ³n basada en interacciones anteriores

### ğŸ”„ Streaming de Respuestas

El mÃ³dulo tambiÃ©n cubre tÃ©cnicas avanzadas de streaming para mejorar la experiencia del usuario:

#### **1. Stream Mode: "updates"**
Solo muestra los cambios incrementales (nuevos mensajes):

```python
for chunk in graph.stream({"messages": "Hola"}, config, stream_mode="updates"):
    print(chunk)  # Solo el mensaje nuevo del bot
```

#### **2. Stream Mode: "values"**
Muestra el estado completo en cada paso:

```python
for chunk in graph.stream({"messages": "Hola"}, config, stream_mode="values"):
    print(chunk)  # Todo el historial + mensaje nuevo
```

#### **3. Async Stream Events (Token por Token)**
Transmite cada token a medida que se genera:

```python
async for event in graph.astream_events({"messages": "Hola"}, config, version="v2"):
    if event["event"] == "on_chat_model_stream":
        print(event["data"]["chunk"].content, end="")
```

**ComparaciÃ³n:**

| Modo | Latencia Percibida | Ancho de Banda | Caso de Uso |
|------|-------------------|----------------|-------------|
| **updates** | Media | Bajo | APIs, procesamiento por lotes |
| **values** | Media | Alto | Debugging, auditorÃ­a completa |
| **astream_events** | Baja | Muy alto | UIs tipo ChatGPT, experiencia fluida |

### ğŸ“‹ Notebooks del MÃ³dulo

#### **1-ReActAgents.ipynb**
- ImplementaciÃ³n completa de agentes ReAct
- IntegraciÃ³n de mÃºltiples herramientas (Arxiv, Wikipedia, Tavily)
- Funciones personalizadas como herramientas
- ConstrucciÃ³n del grafo con enrutamiento condicional
- Agentes sin memoria vs. con memoria (MemorySaver)
- Ejemplos de consultas complejas multi-herramienta

**Temas cubiertos:**
- ConfiguraciÃ³n de herramientas externas
- VinculaciÃ³n de herramientas al LLM con `bind_tools()`
- Estado del grafo con `TypedDict` y `add_messages`
- Uso de `tools_condition` para enrutamiento automÃ¡tico
- ImplementaciÃ³n de memoria con checkpointers
- GestiÃ³n de threads de conversaciÃ³n

#### **2-streaming.ipynb**
- Chatbot simple con LangGraph
- TÃ©cnicas de streaming sÃ­ncronas (.stream())
- Streaming asÃ­ncrono token por token (.astream_events())
- ComparaciÃ³n entre stream_mode="updates" vs "values"
- Casos de uso para cada tipo de streaming

**Temas cubiertos:**
- ConstrucciÃ³n de chatbot bÃ¡sico con un solo nodo
- ConfiguraciÃ³n de memoria con MemorySaver
- MÃ©todos `.stream()` y `.astream_events()`
- Diferencias entre streaming de estado vs tokens
- Aplicaciones prÃ¡cticas de cada modo

### âœ… Ventajas de los Agentes ReAct

| Ventaja | DescripciÃ³n |
|---------|-------------|
| **ğŸ¯ AutonomÃ­a** | El agente decide quÃ© herramientas usar y cuÃ¡ndo |
| **ğŸ”„ Iterativo** | Puede ejecutar mÃºltiples pasos para tareas complejas |
| **ğŸ› ï¸ Extensible** | Agregar nuevas herramientas es trivial |
| **ğŸ’­ Transparente** | El razonamiento del agente es visible (tool_calls) |
| **ğŸ§© Composable** | Combina mÃºltiples herramientas de manera inteligente |

### ğŸ¯ Casos de Uso

**ğŸ”¬ InvestigaciÃ³n AcadÃ©mica**
```python
query = "Explica el paper 'Attention is All You Need' y busca papers relacionados"
# Agente usa: Arxiv â†’ Wikipedia â†’ Genera resumen
```

**ğŸ“° AnÃ¡lisis de Noticias**
```python
query = "Ãšltimas noticias de IA, resume las 5 mÃ¡s importantes"
# Agente usa: Tavily â†’ Procesa resultados â†’ Genera resumen
```

**ğŸ§® Asistente MatemÃ¡tico**
```python
query = "Calcula (12 + 8) * 3 y luego divide el resultado entre 5"
# Agente usa: add() â†’ multiply() â†’ divide() â†’ Responde
```

**ğŸ¤ Consultas Multi-Fuente**
```python
query = "Â¿QuÃ© es machine learning segÃºn Wikipedia y hay papers recientes sobre el tema?"
# Agente usa: Wikipedia â†’ Arxiv â†’ Combina informaciÃ³n
```

### âš ï¸ Consideraciones y Limitaciones

| Aspecto | ConsideraciÃ³n |
|---------|---------------|
| **ğŸ’° Costo** | Cada llamada a herramienta = 1 llamada extra al LLM |
| **ğŸŒ Latencia** | Agentes multi-paso pueden ser lentos (5-10s) |
| **ğŸ” Loops Infinitos** | Necesitas lÃ­mites en el nÃºmero de iteraciones |
| **ğŸ² No Determinismo** | El LLM puede elegir herramientas diferentes en cada ejecuciÃ³n |
| **ğŸ›¡ï¸ Seguridad** | Herramientas externas requieren validaciÃ³n y rate limiting |

### ğŸš€ Mejores PrÃ¡cticas

1. **Limita Iteraciones del Agente**
   ```python
   # Agregar lÃ­mite de recursiÃ³n
   graph = builder.compile(checkpointer=memory, recursion_limit=10)
   ```

2. **Descripciones Claras de Herramientas**
   ```python
   def add(a: int, b: int) -> int:
       """Suma dos nÃºmeros enteros.  # DescripciÃ³n clara

       Args:
           a: Primer nÃºmero
           b: Segundo nÃºmero

       Returns:
           La suma de a + b
       """
       return a + b
   ```

3. **Usa Memoria para Conversaciones**
   ```python
   # Siempre usa checkpointer para agentes conversacionales
   graph = builder.compile(checkpointer=MemorySaver())
   ```

4. **Monitorea con LangSmith**
   ```python
   os.environ["LANGCHAIN_TRACING_V2"] = "true"
   os.environ["LANGCHAIN_PROJECT"] = "mi-agente"
   ```

---

## ğŸ› Debugging con LangGraph Studio

LangGraph Studio es una herramienta de desarrollo y debugging que permite visualizar, inspeccionar y depurar grafos de LangGraph en tiempo real. El mÃ³dulo 009 introduce las configuraciones necesarias para trabajar con LangGraph Studio.

### ğŸ¯ Â¿QuÃ© es LangGraph Studio?

**LangGraph Studio** es una aplicaciÃ³n de escritorio que proporciona:
- ğŸ” VisualizaciÃ³n interactiva de grafos
- ğŸ› Debugging paso a paso de ejecuciones
- ğŸ“Š InspecciÃ³n de estado en cada nodo
- âš¡ EjecuciÃ³n local de agentes
- ğŸ”„ Recarga en caliente (hot reload) de cambios

### ğŸ“‹ ConfiguraciÃ³n con langgraph.json

El archivo `langgraph.json` define la configuraciÃ³n del proyecto para LangGraph Studio:

```json
{
    "dependencies": ["."],
    "graphs": {
      "openai_agent": "./openai_agent.py:agent"
    },
    "env": "../.env"
}
```

**Campos:**

| Campo | DescripciÃ³n | Ejemplo |
|-------|-------------|---------|
| `dependencies` | Directorios con cÃ³digo fuente | `["."]` - directorio actual |
| `graphs` | Mapeo nombre â†’ ruta del grafo | `"openai_agent": "./openai_agent.py:agent"` |
| `env` | Ruta al archivo .env | `"../.env"` |

### ğŸ› ï¸ Archivo openai_agent.py

Este archivo implementa dos grafos para debugging:

#### **1. Grafo BÃ¡sico (make_default_graph)**
```python
def make_default_graph():
    """Grafo simple: consulta â†’ LLM â†’ respuesta"""
    graph_workflow = StateGraph(State)

    def call_model(state):
        return {"messages": [model.invoke(state['messages'])]}

    graph_workflow.add_node("agent", call_model)
    graph_workflow.add_edge(START, "agent")
    graph_workflow.add_edge("agent", END)

    return graph_workflow.compile()
```

**Flujo:**
```
START â†’ agent â†’ END
```

#### **2. Grafo con Herramientas (make_alternative_graph)**
```python
def make_alternative_graph():
    """Grafo con herramienta 'add' y enrutamiento condicional"""

    @tool
    def add(a: float, b: float):
        """Adds two numbers."""
        return a + b

    tool_node = ToolNode([add])
    model_with_tools = model.bind_tools([add])

    def should_continue(state: State):
        """Decide si ejecutar herramienta o terminar"""
        if state["messages"][-1].tool_calls:
            return "tools"
        else:
            return END

    graph_workflow = StateGraph(State)
    graph_workflow.add_node("agent", call_model)
    graph_workflow.add_node("tools", tool_node)
    graph_workflow.add_edge(START, "agent")
    graph_workflow.add_conditional_edges("agent", should_continue)
    graph_workflow.add_edge("tools", "agent")

    return graph_workflow.compile()
```

**Flujo:**
```
START â†’ agent â”€â”€â”
         â†‘      â”‚
         â”‚      â†“
      tools â† [should_continue] â†’ END
```

### ğŸš€ Uso de LangGraph Studio

#### **1. InstalaciÃ³n**
```bash
# Descargar desde: https://studio.langchain.com/
# Disponible para macOS, Windows y Linux
```

#### **2. Abrir Proyecto**
```bash
# Navegar a la carpeta con langgraph.json
cd 009_Debugging

# LangGraph Studio detectarÃ¡ automÃ¡ticamente el proyecto
```

#### **3. Seleccionar Grafo**
En la interfaz de LangGraph Studio:
1. Seleccionar "openai_agent" del dropdown
2. Ver visualizaciÃ³n del grafo
3. Ejecutar con inputs personalizados

#### **4. Debugging Paso a Paso**
```python
# LangGraph Studio permite:
- Ver el estado despuÃ©s de cada nodo
- Inspeccionar mensajes y herramientas invocadas
- Identificar dÃ³nde falla el grafo
- Modificar el cÃ³digo y ver cambios en tiempo real
```

### ğŸ“Š ComparaciÃ³n: Desarrollo vs ProducciÃ³n

| Aspecto | LangGraph Studio | ProducciÃ³n (Python) |
|---------|------------------|---------------------|
| **VisualizaciÃ³n** | âœ… GrÃ¡fica interactiva | âŒ Solo cÃ³digo |
| **Debugging** | âœ… Paso a paso | âš ï¸ Logs/print |
| **Hot Reload** | âœ… AutomÃ¡tico | âŒ Reinicio manual |
| **InspecciÃ³n de Estado** | âœ… UI visual | âš ï¸ Breakpoints |
| **Velocidad de Desarrollo** | ğŸš€ Muy rÃ¡pida | ğŸŒ Media |
| **Deployment** | âŒ Solo desarrollo | âœ… CÃ³digo Python |

### âœ… Ventajas de LangGraph Studio

| Ventaja | DescripciÃ³n |
|---------|-------------|
| **ğŸ‘ï¸ VisualizaciÃ³n** | Ve tu grafo en tiempo real mientras se ejecuta |
| **ğŸ› Debugging Visual** | Identifica problemas rÃ¡pidamente sin print() |
| **âš¡ IteraciÃ³n RÃ¡pida** | Cambios en el cÃ³digo se reflejan inmediatamente |
| **ğŸ“Š InspecciÃ³n de Estado** | Ve exactamente quÃ© datos pasan entre nodos |
| **ğŸ§ª Testing Interactivo** | Prueba diferentes inputs sin escribir tests |

### ğŸ¯ Casos de Uso

**ğŸ” Desarrollar Nuevo Agente**
- Visualizar flujo antes de escribir cÃ³digo complejo
- Verificar que el enrutamiento condicional funcione correctamente

**ğŸ› Depurar Agente Existente**
- Identificar por quÃ© el agente elige herramientas incorrectas
- Ver el estado exacto cuando ocurre un error

**ğŸ§ª Experimentar con Prompts**
- Probar diferentes prompts y ver su efecto inmediatamente
- Comparar comportamiento entre modelos (GPT-4 vs Llama)

**ğŸ“š Aprender LangGraph**
- Entender cÃ³mo fluyen los datos en grafos complejos
- Ver ejemplos funcionando en tiempo real

### ğŸ› ï¸ Estructura del MÃ³dulo

```
009_Debugging/
â”œâ”€â”€ langgraph.json         # ConfiguraciÃ³n de LangGraph Studio
â””â”€â”€ openai_agent.py        # ImplementaciÃ³n de grafos para debugging
    â”œâ”€â”€ make_default_graph()      # Grafo simple
    â””â”€â”€ make_alternative_graph()  # Grafo con herramientas
```

### ğŸš€ Mejores PrÃ¡cticas

1. **Usa langgraph.json para Todos tus Proyectos**
   ```json
   {
       "dependencies": ["."],
       "graphs": {
         "mi_agente": "./agent.py:graph",
         "mi_chatbot": "./chatbot.py:chatbot_graph"
       },
       "env": ".env"
   }
   ```

2. **Implementa MÃºltiples Variantes de Grafos**
   ```python
   # Ãštil para A/B testing y experimentaciÃ³n
   def make_basic_agent(): ...
   def make_agent_with_memory(): ...
   def make_agent_with_tools(): ...
   ```

3. **Usa Nombres Descriptivos**
   ```python
   # âœ… Bueno
   graph_workflow.add_node("validate_user_input", validate_fn)

   # âŒ Malo
   graph_workflow.add_node("node1", validate_fn)
   ```

4. **Documenta Funciones de DecisiÃ³n**
   ```python
   def should_continue(state: State):
       """Decide si continuar con herramientas o terminar.

       Returns:
           "tools" si hay tool_calls pendientes
           END si la respuesta estÃ¡ lista
       """
       ...
   ```

---

## ğŸ¯ RAG AgÃ©ntico

El RAG AgÃ©ntico representa la evoluciÃ³n de los sistemas RAG tradicionales, donde en lugar de un flujo lineal simple (recuperar â†’ generar), el sistema implementa capacidades de razonamiento, evaluaciÃ³n y auto-correcciÃ³n. El mÃ³dulo 010 introduce estos conceptos avanzados usando LangGraph.

### ğŸ¤– Â¿QuÃ© es RAG AgÃ©ntico?

**RAG AgÃ©ntico (Agentic RAG)** es un sistema de RecuperaciÃ³n Aumentada Generativa donde un agente inteligente:

1. **Razona**: Analiza la pregunta y decide quÃ© herramientas usar
2. **Recupera**: Busca informaciÃ³n en mÃºltiples fuentes de conocimiento
3. **EvalÃºa**: Determina si los documentos recuperados son relevantes
4. **Reformula**: Mejora la consulta si los documentos no son adecuados
5. **Genera**: Crea una respuesta fundamentada en evidencia

### ğŸ”„ Flujo de RAG AgÃ©ntico

```
Usuario: "Â¿QuÃ© es LangGraph?"
    â†“
[AGENTE] Analiza la pregunta y decide usar herramienta de recuperaciÃ³n
    â†“
[RECUPERAR] Busca en vectorstore de LangGraph
    â†“
[EVALUAR] Â¿Los documentos son relevantes?
    â”œâ”€ SÃ â†’ [GENERAR] Crea respuesta basada en contexto
    â””â”€ NO â†’ [REFORMULAR] Mejora la pregunta â†’ Vuelve a [AGENTE]
    â†“
[RESPUESTA] Entrega respuesta final al usuario
```

### ğŸ› ï¸ Componentes del Sistema

#### 1. **MÃºltiples Vectorstores**
```python
# Vectorstore para documentaciÃ³n de LangGraph
vectorstore_langgraph = FAISS.from_documents(docs_langgraph, embeddings)

# Vectorstore para documentaciÃ³n de LangChain
vectorstore_langchain = FAISS.from_documents(docs_langchain, embeddings)

# El agente decide cuÃ¡l usar segÃºn la consulta
```

#### 2. **Nodos del Grafo**

- **Nodo Agent**: Razona y decide quÃ© herramienta usar
  ```python
  def agent(state):
      model = ChatGroq(model="qwen-qwq-32b")
      model = model.bind_tools(tools)
      response = model.invoke(state["messages"])
      return {"messages": [response]}
  ```

- **Nodo Retrieve**: Ejecuta herramientas de recuperaciÃ³n
  ```python
  retrieve = ToolNode([retriever_tool_langgraph, retriever_tool_langchain])
  ```

- **Nodo Grade**: EvalÃºa relevancia de documentos
  ```python
  def grade_documents(state) -> Literal["generate", "rewrite"]:
      # Usa un LLM para evaluar si los docs son relevantes
      scored_result = chain.invoke({"question": question, "context": docs})
      return "generate" if scored_result.binary_score == "yes" else "rewrite"
  ```

- **Nodo Rewrite**: Reformula consultas no exitosas
  ```python
  def rewrite(state):
      # Mejora la pregunta basÃ¡ndose en la intenciÃ³n semÃ¡ntica
      msg = HumanMessage(content=f"Formula una pregunta mejorada: {question}")
      response = model.invoke(msg)
      return {"messages": [response]}
  ```

- **Nodo Generate**: Crea respuesta final
  ```python
  def generate(state):
      prompt = hub.pull("rlm/rag-prompt")
      response = rag_chain.invoke({"context": docs, "question": question})
      return {"messages": [response]}
  ```

#### 3. **Aristas Condicionales**

```python
# Desde Agent: Â¿Usar herramientas o terminar?
workflow.add_conditional_edges(
    "agent",
    tools_condition,
    {"tools": "retrieve", END: END}
)

# Desde Retrieve: Â¿Documentos relevantes?
workflow.add_conditional_edges(
    "retrieve",
    grade_documents,  # Retorna "generate" o "rewrite"
)
```

### ğŸ“š Notebooks del MÃ³dulo

#### 1. **1-agenticrag.ipynb**: RAG AgÃ©ntico BÃ¡sico
- ConstrucciÃ³n de grafo simple con StateGraph
- Nodos de recuperaciÃ³n y generaciÃ³n
- Flujo lineal: recuperar â†’ generar

#### 2. **2-ReAct.ipynb**: Framework ReAct
- ImplementaciÃ³n del patrÃ³n Reasoning + Acting
- Agente que decide quÃ© herramientas usar
- MÃºltiples herramientas (RAG, Wikipedia, ArXiv)
- Herramientas personalizadas desde archivos de texto

#### 3. **3-AgenticRAG.ipynb**: Sistema Completo
- MÃºltiples vectorstores (LangGraph y LangChain)
- EvaluaciÃ³n de relevancia con LLM
- ReformulaciÃ³n automÃ¡tica de consultas
- Ciclos en el grafo para auto-correcciÃ³n
- Decisiones inteligentes con aristas condicionales

### ğŸ¯ Diferencias: RAG Tradicional vs RAG AgÃ©ntico

| CaracterÃ­stica | RAG Tradicional | RAG AgÃ©ntico |
|----------------|-----------------|---------------|
| **Flujo** | Lineal (recuperar â†’ generar) | CÃ­clico con decisiones |
| **Herramientas** | Una fuente de datos | MÃºltiples fuentes |
| **EvaluaciÃ³n** | No evalÃºa relevancia | EvalÃºa y decide |
| **Auto-correcciÃ³n** | No | SÃ­ (reformula consultas) |
| **Complejidad** | Baja | Alta |
| **PrecisiÃ³n** | Moderada | Alta |

### ğŸš€ Mejores PrÃ¡cticas

1. **Usa MÃºltiples Fuentes de Conocimiento**
   ```python
   tools = [
       retriever_tool_docs,      # DocumentaciÃ³n
       retriever_tool_research,  # Papers de investigaciÃ³n
       wiki_tool,                # Conocimiento general
   ]
   ```

2. **Implementa EvaluaciÃ³n de Relevancia**
   ```python
   class Grade(BaseModel):
       binary_score: str = Field(description="'yes' or 'no'")

   llm_grader = llm.with_structured_output(Grade)
   ```

3. **Reformula Consultas Fallidas**
   ```python
   if score == "no":
       # Mejora la consulta y reintenta
       improved_query = rewrite_query(original_query)
       return "rewrite"
   ```

4. **Limita Ciclos de ReformulaciÃ³n**
   ```python
   class State(TypedDict):
       messages: list
       retry_count: int  # Evita bucles infinitos

   def should_retry(state):
       return "rewrite" if state["retry_count"] < 3 else END
   ```

---

## ğŸ§  RAG AutÃ³nomo

El RAG AutÃ³nomo lleva los sistemas RAG un paso mÃ¡s allÃ¡ al implementar **Chain-of-Thought (CoT)**, una tÃ©cnica que descompone preguntas complejas en pasos de razonamiento intermedios. Este enfoque permite al sistema "pensar" antes de recuperar informaciÃ³n, similar a cÃ³mo los humanos abordan problemas complejos.

### ğŸ¤” Â¿QuÃ© es Chain-of-Thought (CoT) en RAG?

**Chain-of-Thought (CoT) RAG** es un sistema que descompone preguntas complejas en sub-problemas mÃ¡s manejables, recupera informaciÃ³n relevante para cada sub-problema, y sintetiza una respuesta coherente considerando todo el razonamiento.

**Diferencia clave**: En lugar de una sola recuperaciÃ³n, CoT RAG realiza recuperaciÃ³n multi-paso guiada por razonamiento.

### ğŸ”„ Flujo de CoT RAG

```
Usuario: "Â¿CuÃ¡les son los experimentos adicionales en la evaluaciÃ³n de Transformers?"
    â†“
[PLANNER] Descompone en sub-pasos:
    1. "Identificar Ã¡reas clave de evaluaciÃ³n de Transformers"
    2. "Determinar categorÃ­as de experimentos adicionales"
    3. "Refinar y especificar experimentos por categorÃ­a"
    â†“
[RETRIEVER] Para cada sub-paso:
    - Busca documentos relevantes especÃ­ficos
    - Acumula todos los documentos encontrados
    â†“
[RESPONDER] Sintetiza respuesta:
    - Combina contexto de todos los sub-pasos
    - Genera respuesta razonada y coherente
```

### ğŸ¯ Arquitectura del Sistema

#### **Estado del Grafo**
```python
class RAGCoTState(BaseModel):
    question: str                    # Pregunta original compleja
    sub_steps: List[str] = []       # Sub-pasos de razonamiento
    retrieved_docs: List[Document]  # Documentos de todos los sub-pasos
    answer: str = ""                # Respuesta final sintetizada
```

#### **Nodo 1: Planner (Planificador)**
Descompone la pregunta compleja en 2-3 pasos de razonamiento:

```python
def plan_steps(state: RAGCoTState) -> RAGCoTState:
    prompt = f"Divide la pregunta en 2-3 pasos: {state.question}"
    result = llm.invoke(prompt).content
    sub_steps = [line.strip("- ") for line in result.split("\n") if line.strip()]
    return state.model_copy(update={"sub_steps": sub_steps})
```

**Ejemplo de salida**:
```
Pregunta: "Â¿CÃ³mo optimizar Transformers para producciÃ³n?"
Sub-pasos:
1. Identificar cuellos de botella de rendimiento en Transformers
2. Explorar tÃ©cnicas de optimizaciÃ³n (quantizaciÃ³n, pruning, destilaciÃ³n)
3. Evaluar trade-offs entre velocidad y precisiÃ³n
```

#### **Nodo 2: Retriever (Recuperador Multi-paso)**
Recupera documentos relevantes para cada sub-paso:

```python
def retrieve_per_step(state: RAGCoTState) -> RAGCoTState:
    all_docs = []
    for sub_step in state.sub_steps:
        docs = retriever.invoke(sub_step)  # RecuperaciÃ³n enfocada
        all_docs.extend(docs)
    return state.model_copy(update={"retrieved_docs": all_docs})
```

**Ventaja**: Cada sub-paso recupera documentos especÃ­ficos, evitando ruido.

#### **Nodo 3: Responder (Sintetizador)**
Genera respuesta final considerando todo el razonamiento:

```python
def generate_answer(state: RAGCoTState) -> RAGCoTState:
    context = "\n\n".join([doc.page_content for doc in state.retrieved_docs])
    prompt = f"""
Pregunta: {state.question}
InformaciÃ³n Relevante: {context}
Sintetiza una respuesta bien razonada.
"""
    result = llm.invoke(prompt).content.strip()
    return state.model_copy(update={"answer": result})
```

### ğŸ“Š CoT RAG vs RAG Tradicional vs RAG AgÃ©ntico

| CaracterÃ­stica | RAG Tradicional | RAG AgÃ©ntico | CoT RAG |
|----------------|-----------------|---------------|---------|
| **DescomposiciÃ³n** | No | No | âœ… SÃ­ (2-3 sub-pasos) |
| **RecuperaciÃ³n** | Una sola vez | MÃºltiple (con evaluaciÃ³n) | Multi-paso (guiada) |
| **Razonamiento** | No explÃ­cito | EvaluaciÃ³n de relevancia | âœ… ExplÃ­cito paso a paso |
| **Ciclos** | No | SÃ­ (reescritura) | No (lineal) |
| **Transparencia** | Baja | Media | âœ… Alta (muestra sub-pasos) |
| **Complejidad** | Baja | Alta | Media |
| **Mejor para** | Preguntas simples | MÃºltiples fuentes | âœ… Preguntas complejas |

### ğŸ¯ Casos de Uso Ideales para CoT RAG

1. **Preguntas que requieren mÃºltiples perspectivas**
   - âŒ RAG Tradicional: "Â¿QuÃ© es un Transformer?" (simple)
   - âœ… CoT RAG: "Â¿CÃ³mo han evolucionado los Transformers y quÃ© optimizaciones existen para producciÃ³n?"

2. **AnÃ¡lisis comparativo**
   - âœ… "Compare tÃ©cnicas de optimizaciÃ³n de Transformers: cuantizaciÃ³n, pruning y destilaciÃ³n"

3. **Preguntas con mÃºltiples sub-componentes**
   - âœ… "Â¿CuÃ¡les son las mejores prÃ¡cticas para implementar RAG: desde chunking hasta evaluaciÃ³n?"

4. **InvestigaciÃ³n profunda**
   - âœ… "Â¿QuÃ© experimentos se han realizado en evaluaciÃ³n de Transformers y cuÃ¡les son sus resultados?"

### ğŸš€ Mejores PrÃ¡cticas

1. **Limita los Sub-pasos**
   ```python
   # âœ… Bueno - 2-3 pasos manejables
   prompt = "Divide en 2-3 pasos de razonamiento"

   # âŒ Malo - demasiados pasos
   prompt = "Divide en 10 pasos detallados"
   ```

2. **Valida la Calidad de DescomposiciÃ³n**
   ```python
   def plan_steps(state):
       result = llm.invoke(prompt).content
       sub_steps = parse_steps(result)

       # Validar que hay entre 2 y 4 pasos
       if len(sub_steps) < 2 or len(sub_steps) > 4:
           # Re-intentar o usar pregunta original
           pass
   ```

3. **Evita Redundancia en RecuperaciÃ³n**
   ```python
   # âœ… Bueno - elimina documentos duplicados
   all_docs = []
   seen_ids = set()
   for sub_step in sub_steps:
       docs = retriever.invoke(sub_step)
       for doc in docs:
           if doc.id not in seen_ids:
               all_docs.append(doc)
               seen_ids.add(doc.id)
   ```

4. **Muestra el Razonamiento al Usuario**
   ```python
   print("ğŸªœ Pasos de Razonamiento:")
   for i, step in enumerate(final["sub_steps"], 1):
       print(f"{i}. {step}")
   print("\nâœ… Respuesta Final:", final["answer"])
   ```

### ğŸ’¡ CuÃ¡ndo Usar CoT RAG

**âœ… Usa CoT RAG cuando**:
- La pregunta es genuinamente compleja y multi-facÃ©tica
- Necesitas transparencia en el razonamiento
- Quieres mejorar la calidad de respuestas para preguntas difÃ­ciles
- El usuario valora ver los pasos de pensamiento

**âŒ No uses CoT RAG cuando**:
- La pregunta es simple y directa
- La latencia es crÃ­tica (CoT aÃ±ade overhead)
- No necesitas explicabilidad
- El costo de llamadas LLM es una restricciÃ³n

### ğŸ”— Flujo del Grafo

```
START â†’ planner â†’ retriever â†’ responder â†’ END
         â†“           â†“            â†“
    sub_steps   retrieved_docs  answer
```

**CaracterÃ­sticas**:
- **Lineal**: No hay ciclos (a diferencia de RAG AgÃ©ntico)
- **Determinista**: Siempre ejecuta los 3 nodos en orden
- **Explicable**: Cada paso es visible y auditable

### ğŸ” Auto-ReflexiÃ³n en RAG

**Auto-reflexiÃ³n** es una tÃ©cnica donde el LLM evalÃºa su propia respuesta para determinar si es completa, precisa y satisfactoria. Si la respuesta no cumple con los estÃ¡ndares, el sistema puede refinar la consulta y recuperar informaciÃ³n adicional.

**Combina**: RecuperaciÃ³n Iterativa + Auto-crÃ­tica

#### Flujo de Auto-ReflexiÃ³n

```
Usuario: "Â¿CuÃ¡les son las variantes de transformers en despliegues de producciÃ³n?"
    â†“
[RETRIEVE] Recupera documentos
    â†“
[GENERATE] Genera respuesta inicial
    â†“
[REFLECT] EvalÃºa la respuesta:
    - Â¿Es factualmente suficiente?
    - Â¿Responde completamente la pregunta?
    â†“
SI: Respuesta aprobada â†’ FIN
NO: Refina y vuelve a recuperar (mÃ¡x 2 intentos)
```

#### Estado del Sistema

```python
class RAGReflectionState(BaseModel):
    question: str                    # Pregunta original
    retrieved_docs: List[Document]   # Documentos recuperados
    answer: str = ""                 # Respuesta generada
    reflection: str = ""             # EvaluaciÃ³n de la respuesta
    revised: bool = False            # Â¿Necesita revisiÃ³n?
    attempts: int = 0                # Contador de intentos
```

#### Nodos Clave

**1. Retrieve**: Recupera documentos del vector store
**2. Generate**: Genera respuesta basÃ¡ndose en el contexto
**3. Reflect**: EvalÃºa la calidad de la respuesta (LLM como juez)
**4. Finalize**: Marca el final del proceso

**Flujo del Grafo**:
```
START â†’ retrieve â†’ generate â†’ reflect â†’ [done o retrieve (si necesita revisiÃ³n)]
                                â†“
                              END (si verificado o attempts >= 2)
```

**Ventajas**:
- âœ… Mejora la calidad de respuestas automÃ¡ticamente
- âœ… Detecta respuestas incompletas o imprecisas
- âœ… Proceso de mejora iterativo y controlado
- âœ… LÃ­mite de intentos evita ciclos infinitos

### ğŸ¯ PlanificaciÃ³n y DescomposiciÃ³n de Consultas

**Query Planning and Decomposition** es una tÃ©cnica que divide consultas complejas en sub-preguntas mÃ¡s simples, permitiendo recuperaciÃ³n mÃ¡s precisa y completa de informaciÃ³n.

**Es como**: IngenierÃ­a inversa de una pregunta en pasos manejables antes de responderla.

#### Â¿Por quÃ© es necesario?

En consultas complejas como:
> "Explica cÃ³mo funcionan los bucles de agentes y cuÃ¡les son los desafÃ­os en la generaciÃ³n de video por difusiÃ³n"

Esta pregunta tiene **dos componentes independientes**:
1. Bucles de agentes
2. DesafÃ­os en generaciÃ³n de video por difusiÃ³n

**Problema**: Una sola bÃºsqueda vectorial puede no capturar ambos aspectos adecuadamente.

**SoluciÃ³n**: Descomponer en sub-preguntas y buscar cada una individualmente.

#### Arquitectura del Sistema

```python
class RAGState(BaseModel):
    question: str                    # Pregunta compleja original
    sub_questions: List[str] = []    # 2-3 sub-preguntas generadas
    retrieved_docs: List[Document]   # Todos los docs recuperados
    answer: str = ""                 # Respuesta final consolidada
```

#### Flujo de Trabajo

```
[PLANNER] Descompone la pregunta:
    "Pregunta compleja" â†’ ["Sub-pregunta 1", "Sub-pregunta 2", "Sub-pregunta 3"]
    â†“
[RETRIEVER] Para cada sub-pregunta:
    - Busca documentos especÃ­ficos
    - Acumula todos los documentos
    â†“
[RESPONDER] Sintetiza respuesta final:
    - Combina contexto de todas las sub-preguntas
    - Genera respuesta coherente y completa
```

#### Nodos del Grafo

**1. Planner**: Divide pregunta en 2-3 sub-preguntas usando LLM
**2. Retriever**: Recupera documentos para cada sub-pregunta
**3. Responder**: Sintetiza respuesta final consolidada

**Flujo Secuencial**:
```
START â†’ planner â†’ retriever â†’ responder â†’ END
```

**Ventajas**:
- âœ… Mejor cobertura para preguntas multifacÃ©ticas
- âœ… RecuperaciÃ³n mÃ¡s precisa y completa
- âœ… Reduce el ruido en documentos recuperados
- âœ… Razonamiento paso a paso mÃ¡s claro

**Casos de Uso Ideales**:
- Preguntas con mÃºltiples temas
- Consultas que requieren informaciÃ³n de diferentes dominios
- AnÃ¡lisis comparativos o multi-dimensionales

### ğŸ”„ RecuperaciÃ³n Iterativa

**Iterative Retrieval** combina recuperaciÃ³n iterativa con auto-reflexiÃ³n en un ciclo de retroalimentaciÃ³n continua. El sistema no se conforma con la primera recuperaciÃ³n; evalÃºa, refina y vuelve a buscar hasta obtener informaciÃ³n suficiente.

**Diferencia clave**: Similar a Auto-reflexiÃ³n, pero con **refinamiento de consulta** cuando la respuesta es insuficiente.

#### Â¿CÃ³mo Funciona?

```
Usuario: "bucles de agentes y sistemas basados en transformers?"
    â†“
[RETRIEVE] Recupera con pregunta original
    â†“
[ANSWER] Genera respuesta
    â†“
[REFLECT] EvalÃºa calidad
    â†“
Â¿Verificada? NO â†’ [REFINE] Mejora la consulta
    â†“                       â†“
   SÃ                  Vuelve a RETRIEVE
    â†“
  END
```

#### Estado del Sistema

```python
class IterativeRAGState(BaseModel):
    question: str                    # Pregunta original
    refined_question: str = ""       # VersiÃ³n refinada de la consulta
    retrieved_docs: List[Document]   # Documentos recuperados
    answer: str = ""                 # Respuesta generada
    verified: bool = False           # Â¿Respuesta verificada?
    attempts: int = 0                # Contador de iteraciones
```

#### Nodos del Grafo

**1. Retrieve**: Usa pregunta refinada (si existe) o la original
**2. Answer**: Genera respuesta e incrementa contador
**3. Reflect**: EvalÃºa si la respuesta es suficiente
**4. Refine**: Genera versiÃ³n mejorada de la consulta

**Ciclo Iterativo**:
```
START â†’ retrieve â†’ answer â†’ reflect â†’ refine â†’ retrieve (ciclo)
                               â†“
                              END (si verificado o attempts >= 2)
```

**El "Ciclo MÃ¡gico"**:
- Si `verified=True` O `attempts>=2` â†’ END
- Si `verified=False` Y `attempts<2` â†’ refine â†’ retrieve (reintentar)

**Ventajas**:
- âœ… Mejora automÃ¡tica de consultas vagas o mal formuladas
- âœ… RecuperaciÃ³n adaptativa basada en resultados previos
- âœ… Similar a cÃ³mo un investigador humano refina bÃºsquedas
- âœ… Maximiza calidad de respuesta dentro de lÃ­mites de iteraciones

**Casos de Uso**:
- Consultas inicialmente vagas o imprecisas
- Cuando la primera recuperaciÃ³n no proporciona contexto suficiente
- Temas que requieren refinamiento progresivo de bÃºsqueda

### ğŸ¨ SÃ­ntesis de Respuestas desde MÃºltiples Fuentes

**Answer Synthesis from Multiple Sources** es el proceso donde un agente de IA recopila informaciÃ³n de diferentes herramientas de recuperaciÃ³n o bases de conocimiento, y las fusiona en una Ãºnica respuesta coherente y contextualmente rica.

**Capacidad fundamental** en RAG AgÃ©ntico: El sistema no solo recupera, sino que **planifica, recupera de mÃºltiples fuentes, y sintetiza**.

#### Â¿Por quÃ© es Necesario?

La mayorÃ­a de consultas del mundo real son:
- **MultifacÃ©ticas**: Requieren mÃºltiples tipos de informaciÃ³n
- **Ambiguas**: Necesitan refinamiento contextual
- **Abiertas**: No se mapean a un solo documento

**LimitaciÃ³n de RAG tradicional**: Una sola base de datos vectorial es insuficiente.

**SoluciÃ³n**: Agente que recupera de mÃºltiples fuentes y sintetiza.

#### Fuentes de InformaciÃ³n

Este sistema integra **4 fuentes diferentes**:

1. **ğŸ“„ Documentos Internos** (Vector Store local)
   - Archivos de texto propios de la organizaciÃ³n
   - DocumentaciÃ³n interna y privada

2. **ğŸ¥ YouTube** (Transcripciones)
   - Contenido multimedia/educativo
   - Explicaciones conceptuales en video

3. **ğŸŒ Wikipedia** (API pÃºblica)
   - Conocimiento enciclopÃ©dico general
   - Definiciones y contexto amplio

4. **ğŸ“š ArXiv** (Papers acadÃ©micos)
   - InvestigaciÃ³n cientÃ­fica actualizada
   - Papers y publicaciones acadÃ©micas

#### Arquitectura del Sistema

```python
class MultiSourceRAGState(BaseModel):
    question: str                    # Pregunta del usuario
    text_docs: List[Document] = []   # Docs internos
    yt_docs: List[Document] = []     # Transcripciones YouTube
    wiki_context: str = ""           # Contenido Wikipedia
    arxiv_context: str = ""          # Papers ArXiv
    final_answer: str = ""           # Respuesta sintetizada
```

#### Flujo de Trabajo

```
Usuario: "Â¿QuÃ© son los agentes transformers y cÃ³mo estÃ¡n evolucionando?"
    â†“
[RETRIEVE TEXT] Documentos internos sobre transformers
    â†“
[RETRIEVE YOUTUBE] Videos explicativos sobre agentes
    â†“
[RETRIEVE WIKIPEDIA] ArtÃ­culos sobre transformers y agentes
    â†“
[RETRIEVE ARXIV] Papers recientes sobre transformer agents
    â†“
[SYNTHESIZE] Combina toda la informaciÃ³n:
    - Organiza contexto por fuente
    - Genera respuesta que integra todas las perspectivas
    - Proporciona visiÃ³n completa y multidimensional
```

#### Nodos del Grafo

**Nodos de RecuperaciÃ³n** (4):
1. `retrieve_text`: DocumentaciÃ³n interna
2. `retrieve_yt`: Contenido multimedia
3. `retrieve_wikipedia`: Conocimiento enciclopÃ©dico
4. `retrieve_arxiv`: InvestigaciÃ³n cientÃ­fica

**Nodo de SÃ­ntesis** (1):
5. `synthesize`: Fusiona toda la informaciÃ³n en respuesta coherente

**Flujo Secuencial**:
```
START â†’ retrieve_text â†’ retrieve_yt â†’ retrieve_wiki â†’ retrieve_arxiv â†’ synthesize â†’ END
```

#### Proceso de SÃ­ntesis

El nodo de sÃ­ntesis realiza:

1. **OrganizaciÃ³n del Contexto**:
```python
context = """
[Documentos Internos]
<contenido de text_docs>

[TranscripciÃ³n de YouTube]
<contenido de yt_docs>

[Wikipedia]
<wiki_context>

[ArXiv]
<arxiv_context>
"""
```

2. **Prompt de SÃ­ntesis**:
```python
prompt = f"""Has recuperado contexto de mÃºltiples fuentes.
Sintetiza una respuesta completa y coherente.

Pregunta: {question}
Contexto: {context}
"""
```

3. **GeneraciÃ³n Unificada**: El LLM analiza toda la informaciÃ³n y genera una respuesta que:
   - Combina perspectivas de todas las fuentes
   - Identifica patrones comunes
   - Resuelve contradicciones
   - Proporciona respuesta rica y completa

#### Ventajas

- âœ… **Cobertura Completa**: InformaciÃ³n de mÃºltiples dominios
- âœ… **Perspectivas Diversas**: DocumentaciÃ³n interna + conocimiento pÃºblico + investigaciÃ³n
- âœ… **ActualizaciÃ³n**: Combina conocimiento histÃ³rico (Wikipedia) con investigaciÃ³n reciente (ArXiv)
- âœ… **Flexibilidad**: FÃ¡cil agregar/quitar fuentes segÃºn necesidad
- âœ… **Robustez**: Si una fuente falla, otras compensan

#### Casos de Uso Ideales

1. **InvestigaciÃ³n Profunda**:
   - "Â¿QuÃ© son los agentes transformers y cÃ³mo estÃ¡n evolucionando en la investigaciÃ³n reciente?"

2. **AnÃ¡lisis Multidimensional**:
   - Combinar documentaciÃ³n interna + papers acadÃ©micos + explicaciones pÃºblicas

3. **VerificaciÃ³n Cruzada**:
   - Contrastar informaciÃ³n de mÃºltiples fuentes para mayor confiabilidad

4. **SÃ­ntesis de Conocimiento**:
   - Generar respuestas que integran mÃºltiples perspectivas y tipos de informaciÃ³n

#### Mejores PrÃ¡cticas

1. **Orden de Fuentes**: Prioriza fuentes mÃ¡s especÃ­ficas primero (docs internos) antes que generales (Wikipedia)

2. **Manejo de Errores**: Implementa fallbacks si alguna fuente falla
```python
try:
    wiki_context = wikipedia_search(query)
except Exception:
    wiki_context = "Wikipedia no disponible"
```

3. **LimitaciÃ³n de Contenido**: Para evitar context window overflow, limita documentos por fuente
```python
arxiv_results = arxiv_loader.load()[:2]  # Solo primeros 2 papers
```

4. **IdentificaciÃ³n de Fuentes**: Marca claramente cada fuente en el contexto para trazabilidad

5. **DeduplicaciÃ³n**: Elimina informaciÃ³n redundante entre fuentes

#### CuÃ¡ndo Usar Multi-Source RAG

**âœ… Usa Multi-Source cuando**:
- Necesitas cobertura completa de un tema
- La pregunta requiere mÃºltiples tipos de informaciÃ³n
- Quieres contrastar informaciÃ³n de diferentes fuentes
- Necesitas combinar conocimiento interno y externo

**âŒ No uses Multi-Source cuando**:
- La pregunta es simple y una fuente es suficiente
- La latencia es crÃ­tica (mÃºltiples fuentes aÃ±aden tiempo)
- Costos de API son restricciÃ³n (mÃ¡s llamadas = mÃ¡s costo)
- Solo tienes una fuente de informaciÃ³n confiable

---

## ğŸ¤– Sistemas RAG Multi-Agente

Los **Sistemas RAG Multi-Agente** representan la evoluciÃ³n mÃ¡s avanzada de RAG, donde el pipeline se divide en mÃºltiples agentes especializados que colaboran para resolver tareas complejas. Cada agente tiene un rol especÃ­fico y herramientas dedicadas, permitiendo una divisiÃ³n inteligente del trabajo.

### ğŸ¯ Â¿QuÃ© son los Sistemas Multi-Agente?

Un Sistema RAG Multi-Agente divide el pipeline RAG en mÃºltiples agentes especializados â€” cada uno responsable de un rol especÃ­fico â€” y les permite **colaborar** en una sola consulta o tarea.

**Diferencia clave con RAG tradicional**:
- **RAG Tradicional**: Un solo agente hace todo (recuperaciÃ³n + generaciÃ³n)
- **RAG Multi-Agente**: MÃºltiples agentes especializados colaboran, cada uno con su expertise

### ğŸ“Š Tres Arquitecturas Multi-Agente

Este mÃ³dulo cubre tres arquitecturas progresivamente mÃ¡s complejas:

#### 1ï¸âƒ£ **Sistema Multi-Agente Colaborativo BÃ¡sico**

**Arquitectura**: Dos agentes que se pasan trabajo entre sÃ­.

```
Usuario: "Escribe un blog sobre transformers"
    â†“
[RESEARCHER] Busca informaciÃ³n:
    - Usa bÃºsqueda web (Tavily)
    - Consulta documentos internos (FAISS)
    - Recopila datos relevantes
    â†“
[BLOG GENERATOR] Escribe contenido:
    - Recibe investigaciÃ³n del researcher
    - Genera blog detallado y estructurado
    - AÃ±ade "FINAL ANSWER" al terminar
    â†“
RESULTADO: Blog completo sobre transformers
```

**Agentes**:
- **Researcher**: Especializado en bÃºsqueda y recuperaciÃ³n de informaciÃ³n
  - Herramientas: `internal_tool_1` (FAISS), `tavily_tool` (bÃºsqueda web)
- **Blog Generator**: Especializado en escritura de contenido
  - Herramientas: Ninguna (solo procesa y escribe)

**PatrÃ³n de TerminaciÃ³n**: Cualquier agente puede aÃ±adir "FINAL ANSWER" para indicar que el trabajo estÃ¡ completo.

**Flujo**:
```
START â†’ researcher â†’ blog_generator â†’ END (si "FINAL ANSWER")
                          â†“
                    researcher (si necesita mÃ¡s info)
```

#### 2ï¸âƒ£ **Supervisor Multi-Agente con RAG**

**Arquitectura**: Un supervisor central coordina agentes especializados.

```
Usuario: "Lista transformers del retriever y calcula 5 + 10"
    â†“
[SUPERVISOR] Analiza y delega:
    - Identifica dos tareas diferentes
    - Decide quÃ© agente usar para cada una
    â†“
[RESEARCH AGENT] Tarea 1:
    - Busca en documentos internos
    - Lista variantes de transformers
    - Reporta al supervisor
    â†“
[MATH AGENT] Tarea 2:
    - Usa herramientas matemÃ¡ticas
    - Calcula 5 + 10 = 15
    - Reporta al supervisor
    â†“
[SUPERVISOR] Consolida:
    - Combina resultados de ambos agentes
    - Genera respuesta final unificada
```

**Componentes**:
- **Supervisor**: Agente de coordinaciÃ³n que:
  - Analiza la consulta del usuario
  - Decide quÃ© agente usar para cada tarea
  - Consolida respuestas
  - **Regla importante**: Delega a un agente a la vez (no en paralelo)

- **Research Agent**:
  - Herramientas: `web_search` (Tavily), `internal_tool_1` (docs internos)
  - RestricciÃ³n: **SOLO investigaciÃ³n**, NO matemÃ¡ticas

- **Math Agent**:
  - Herramientas: `add()`, `multiply()`, `divide()`
  - RestricciÃ³n: **SOLO matemÃ¡ticas**, NO investigaciÃ³n

**LibrerÃ­a**: Usa `langgraph_supervisor.create_supervisor()` pre-construido.

**Ventajas**:
- âœ… EspecializaciÃ³n clara de agentes
- âœ… DelegaciÃ³n inteligente de tareas
- âœ… FÃ¡cil agregar nuevos agentes especializados
- âœ… Supervisor maneja la coordinaciÃ³n automÃ¡ticamente

#### 3ï¸âƒ£ **Equipos JerÃ¡rquicos de Agentes con RAG**

**Arquitectura**: JerarquÃ­a de 3 niveles con equipos completos y supervisores anidados.

```
[SUPERVISOR DE EQUIPOS] (Nivel Superior)
    â†“
    â”œâ”€â†’ [EQUIPO DE INVESTIGACIÃ“N] (Nivel Medio)
    â”‚       [Supervisor de InvestigaciÃ³n]
    â”‚           â†“
    â”‚           â”œâ”€â†’ [Search Agent]: BÃºsqueda general (Tavily + docs)
    â”‚           â””â”€â†’ [Web Scraper Agent]: Scraping de URLs especÃ­ficas
    â”‚
    â””â”€â†’ [EQUIPO DE ESCRITURA] (Nivel Medio)
            [Supervisor de Escritura]
                â†“
                â”œâ”€â†’ [Note Taker]: Crea esquemas/outlines
                â”œâ”€â†’ [Doc Writer]: Escribe documentos completos
                â””â”€â†’ [Chart Generator]: Crea visualizaciones con Python
```

**Flujo de Trabajo Completo**:

```
Usuario: "Escribe sobre transformers en producciÃ³n"
    â†“
1. [SUPERVISOR DE EQUIPOS] Analiza y delega a EQUIPO DE INVESTIGACIÃ“N
    â†“
2. [SUPERVISOR DE INVESTIGACIÃ“N] Coordina bÃºsqueda:
   2.1. [SEARCH AGENT] busca informaciÃ³n general
   2.2. [WEB SCRAPER AGENT] obtiene contenido detallado de pÃ¡ginas especÃ­ficas
   2.3. Reportan resultados al supervisor de investigaciÃ³n
   2.4. Supervisor reporta al supervisor de equipos
    â†“
3. [SUPERVISOR DE EQUIPOS] Delega a EQUIPO DE ESCRITURA
    â†“
4. [SUPERVISOR DE ESCRITURA] Coordina creaciÃ³n:
   4.1. [NOTE TAKER] crea outline del documento
   4.2. [DOC WRITER] escribe documento completo basÃ¡ndose en outline
   4.3. [CHART GENERATOR] (opcional) crea grÃ¡ficos si es necesario
   4.4. Reportan al supervisor de escritura
   4.5. Supervisor reporta al supervisor de equipos
    â†“
5. [SUPERVISOR DE EQUIPOS] Confirma finalizaciÃ³n
```

**Herramientas Avanzadas**:

**Equipo de InvestigaciÃ³n**:
- `tavily_tool`: BÃºsqueda web optimizada para IA (mÃ¡x 5 resultados)
- `internal_tool_1`: RecuperaciÃ³n vectorial con FAISS de docs internos
- `scrape_webpages`: Scraping web con BeautifulSoup para contenido detallado

**Equipo de Escritura**:
- `create_outline`: Crea esquemas numerados de documentos
- `write_document`: Escribe documentos completos
- `edit_document`: Inserta texto en lÃ­neas especÃ­ficas
- `read_document`: Lee documentos (completos o rangos de lÃ­neas)
- `python_repl_tool`: Ejecuta cÃ³digo Python para crear visualizaciones

**Directorio de Trabajo**: Usa `TemporaryDirectory` para gestiÃ³n automÃ¡tica de archivos.

### ğŸ¯ ComparaciÃ³n de Arquitecturas

| CaracterÃ­stica | Colaborativo BÃ¡sico | Supervisor | JerÃ¡rquico |
|----------------|---------------------|------------|------------|
| **Niveles** | 1 nivel | 2 niveles | 3 niveles |
| **Agentes** | 2 agentes | 2+ agentes | 6+ agentes |
| **CoordinaciÃ³n** | Auto-coordinaciÃ³n | Supervisor central | Supervisores anidados |
| **Complejidad** | Baja | Media | Alta |
| **Escalabilidad** | Limitada | Buena | Excelente |
| **EspecializaciÃ³n** | BÃ¡sica | Alta | Muy alta |
| **Mejor para** | Tareas simples | Tareas mÃºltiples | Proyectos complejos |

### ğŸ’¡ Casos de Uso por Arquitectura

#### **Colaborativo BÃ¡sico**
âœ… Usa cuando:
- Solo necesitas 2-3 agentes
- La tarea tiene flujo lineal simple
- No hay muchas decisiones de enrutamiento

**Ejemplo**: Investigar + escribir blog

#### **Supervisor**
âœ… Usa cuando:
- Necesitas mÃºltiples agentes especializados
- Las tareas son claramente separables
- Quieres delegaciÃ³n inteligente automÃ¡tica

**Ejemplo**: Combinar investigaciÃ³n + cÃ¡lculos + anÃ¡lisis

#### **JerÃ¡rquico**
âœ… Usa cuando:
- El proyecto es muy complejo
- Necesitas equipos completos trabajando juntos
- Cada equipo tiene mÃºltiples subagentes
- Quieres mÃ¡xima escalabilidad

**Ejemplo**: Investigar en mÃºltiples fuentes + crear documentos complejos + generar visualizaciones

### ğŸ”§ Componentes Clave en Multi-Agente

#### **1. Estado (State)**
```python
class State(MessagesState):
    next: str  # Tracking del siguiente nodo
```
- Mantiene historial de mensajes entre agentes
- Puede extenderse con campos adicionales

#### **2. Nodos (Nodes)**
Cada nodo es una funciÃ³n que:
- Recibe el estado actual
- Ejecuta un agente especÃ­fico
- Retorna `Command` con actualizaciÃ³n y navegaciÃ³n

#### **3. Comando (Command)**
```python
return Command(
    update={"messages": [...]},  # Actualiza estado
    goto="next_node"              # Navega al siguiente nodo
)
```

#### **4. Supervisor Pattern**
```python
def make_supervisor_node(llm, members):
    # Crea supervisor que decide: "Â¿A quiÃ©n delego?"
    # Usa LLM con structured output para decisiones
```

#### **5. Herramientas (Tools)**
- Funciones Python decoradas con `@tool`
- Descripciones claras para que el LLM sepa cuÃ¡ndo usarlas
- Type hints con `Annotated` para documentaciÃ³n

### ğŸš€ Mejores PrÃ¡cticas

#### **1. DiseÃ±o de Agentes**
```python
# âœ… Bueno - Agente con rol y restricciones claras
research_agent = create_react_agent(
    llm,
    tools=[web_search, internal_docs],
    prompt=(
        "Eres un agente de investigaciÃ³n.\n"
        "SOLO asiste con investigaciÃ³n.\n"
        "NO hagas matemÃ¡ticas.\n"
        "Reporta al supervisor cuando termines."
    )
)

# âŒ Malo - Agente con rol ambiguo
general_agent = create_react_agent(
    llm,
    tools=[everything],
    prompt="Haz lo que sea necesario"
)
```

#### **2. DelegaciÃ³n Secuencial**
```python
# âœ… Bueno - Un agente a la vez
prompt = "Asigna trabajo a un agente a la vez, no en paralelo."

# âŒ Malo - Llamadas paralelas sin coordinaciÃ³n
# Puede causar conflictos y duplicaciÃ³n de trabajo
```

#### **3. Mensajes de Reporte**
```python
# âœ… Bueno - Identifica claramente el origen
HumanMessage(
    content=result["messages"][-1].content,
    name="research_agent"  # Identifica quiÃ©n responde
)
```

#### **4. Manejo de Errores en Herramientas**
```python
@tool
def python_repl_tool(code: str):
    try:
        result = repl.run(code)
    except BaseException as e:
        return f"Fallo al ejecutar. Error: {repr(e)}"
    return f"Ejecutado exitosamente:\n{result}"
```

### âš ï¸ Consideraciones Importantes

#### **Costos**
- Sistemas multi-agente hacen **mÃºltiples llamadas al LLM**
- Supervisor tambiÃ©n usa el LLM para decisiones
- Usa modelos econÃ³micos como GPT-4o-mini

#### **Latencia**
- Cada agente aÃ±ade tiempo de procesamiento
- Supervisores aÃ±aden overhead de decisiÃ³n
- Considera si la complejidad justifica el tiempo

#### **Complejidad**
- MÃ¡s agentes = mÃ¡s difÃ­cil de depurar
- Usa LangGraph Studio para visualizaciÃ³n
- Implementa logging detallado

#### **CoordinaciÃ³n**
- Define claramente roles y restricciones
- Evita solapamiento de responsabilidades
- Documenta el flujo esperado

### ğŸ¯ CuÃ¡ndo Usar Multi-Agente RAG

**âœ… Usa Multi-Agente cuando**:
- La tarea requiere mÃºltiples especialidades (investigaciÃ³n + escritura + anÃ¡lisis)
- Necesitas dividir trabajo complejo en subtareas manejables
- Quieres agentes reutilizables con roles claros
- La calidad justifica el costo adicional
- Necesitas escalabilidad y mantenibilidad

**âŒ No uses Multi-Agente cuando**:
- La tarea es simple y un solo agente basta
- Los costos de API son prohibitivos
- La latencia es crÃ­tica
- El overhead de coordinaciÃ³n no vale la pena
- No hay beneficio claro de la especializaciÃ³n

### ğŸ“š Recursos y Referencias

- **LangGraph**: Framework para construir grafos de agentes
- **langgraph_supervisor**: LibrerÃ­a pre-construida para supervisores
- **create_react_agent**: Constructor de agentes ReAct
- **Command Pattern**: Para navegaciÃ³n y actualizaciÃ³n de estado
- **Structured Output**: Para decisiones supervisores con LLM

---

## ğŸ”§ RAG Correctivo (Corrective RAG)

**Corrective RAG (CRAG)** es una tÃ©cnica avanzada que mejora la calidad y confiabilidad de sistemas RAG mediante la **evaluaciÃ³n automÃ¡tica de documentos recuperados** y la **correcciÃ³n adaptativa** del flujo de recuperaciÃ³n. A diferencia del RAG tradicional que asume que todos los documentos recuperados son relevantes, CRAG evalÃºa cada documento y toma decisiones inteligentes sobre cÃ³mo proceder.

### ğŸ¯ Â¿QuÃ© es Corrective RAG?

CRAG introduce un **ciclo de retroalimentaciÃ³n inteligente** en el pipeline RAG:

```
Usuario: "Â¿QuÃ© es la memoria de agentes?"
    â†“
1. [RECUPERAR] Busca en vectorstore local (FAISS)
    â†“
2. [EVALUAR] LLM califica cada documento: Â¿Es relevante?
    â†“
    â”œâ”€â†’ [SI RELEVANTE] â†’ Genera respuesta directamente
    â”‚
    â””â”€â†’ [NO RELEVANTE] â†’ Reescribe consulta â†’ Busca en web â†’ Genera respuesta
```

**Diferencia clave con RAG tradicional**:
- **RAG Tradicional**: Recupera â†’ Genera (asume que los documentos son relevantes)
- **Corrective RAG**: Recupera â†’ **EvalÃºa** â†’ Corrige si es necesario â†’ Genera

### ğŸ§© Componentes de CRAG

#### 1ï¸âƒ£ **Evaluador de Relevancia (Retrieval Grader)**

Un LLM especializado que califica documentos con puntuaciÃ³n binaria (yes/no).

```python
class GradeDocuments(BaseModel):
    binary_score: str = Field(
        description="Documentos son relevantes a la pregunta, 'yes' o 'no'"
    )

# LLM con salida estructurada usando Pydantic
structured_llm_grader = llm.with_structured_output(GradeDocuments)
```

**Â¿CÃ³mo funciona?**
- Recibe: pregunta del usuario + contenido del documento
- Analiza: similitud semÃ¡ntica y presencia de keywords
- Retorna: `{"binary_score": "yes"}` o `{"binary_score": "no"}`

**Ventajas de salida estructurada**:
- âœ… Respuestas determinÃ­sticas y parseables
- âœ… No necesita parsear texto libre
- âœ… ValidaciÃ³n automÃ¡tica con Pydantic
- âœ… IntegraciÃ³n directa en lÃ³gica condicional

#### 2ï¸âƒ£ **Reescritor de Consultas (Query Rewriter)**

Optimiza consultas cuando los documentos locales no son suficientes.

```python
system = """Eres un reescritor de preguntas que convierte una pregunta
de entrada en una mejor versiÃ³n optimizada para bÃºsqueda web."""

question_rewriter = re_write_prompt | llm | StrOutputParser()
```

**Transformaciones tÃ­picas**:
- Vaga: "memoria de agentes" â†’ Precisa: "Â¿CuÃ¡l es el rol de la memoria en agentes de IA?"
- Ambigua: "transformers" â†’ Contextual: "arquitectura transformer en deep learning"
- TÃ©cnica: "RAG chunking" â†’ Buscable: "mejores prÃ¡cticas para dividir documentos en RAG"

**Por quÃ© es importante**:
- Los vectorstores locales pueden no tener informaciÃ³n actualizada
- Las preguntas mal formuladas obtienen resultados pobres
- La web tiene informaciÃ³n mÃ¡s amplia que requiere consultas optimizadas

#### 3ï¸âƒ£ **BÃºsqueda Web Adaptativa (Tavily Integration)**

Cuando los documentos locales fallan, CRAG busca en la web.

```python
web_search_tool = TavilySearchResults(k=3)  # Top 3 resultados

# Integra resultados web con documentos locales
docs = web_search_tool.invoke({"query": better_question})
web_results = "\n".join([d["content"] for d in docs])
documents.append(Document(page_content=web_results))
```

**Tavily vs Google/Bing**:
- âœ… Optimizado para agentes de IA (respuestas estructuradas)
- âœ… Sin lÃ­mites de rate estrictos
- âœ… Resultados limpios sin ads
- âœ… API simple con respuestas JSON

#### 4ï¸âƒ£ **Flujo de DecisiÃ³n con LangGraph**

El cerebro del sistema que toma decisiones basÃ¡ndose en la evaluaciÃ³n.

```python
def decide_to_generate(state):
    """Decide: Â¿Generar directamente o buscar en web?"""
    web_search = state["web_search"]

    if web_search == "Yes":
        # Documentos no relevantes â†’ transformar consulta
        return "transform_query"
    else:
        # Documentos relevantes â†’ generar respuesta
        return "generate"

# Arista condicional en el grafo
workflow.add_conditional_edges(
    "grade_documents",
    decide_to_generate,
    {
        "transform_query": "transform_query",
        "generate": "generate",
    }
)
```

### ğŸ“Š Arquitectura del Flujo CRAG

```
START
  â†“
[retrieve]
  Recupera documentos del vectorstore local (FAISS)
  Retorna: {"documents": [...], "question": "..."}
  â†“
[grade_documents]
  Por cada documento:
    - EvalÃºa relevancia con LLM
    - Si relevante: agrega a filtered_docs
    - Si no relevante: marca web_search = "Yes"
  Retorna: {"documents": filtered_docs, "web_search": "Yes/No"}
  â†“
[decide_to_generate] â† DECISIÃ“N
  â†“
  â”œâ”€â†’ [SI web_search == "No"] â†’ [generate] â†’ END
  â”‚     Documentos relevantes encontrados
  â”‚     Genera respuesta directamente
  â”‚
  â””â”€â†’ [SI web_search == "Yes"] â†’ [transform_query]
        Documentos no relevantes
        â†“
      [transform_query]
        Reescribe la pregunta para bÃºsqueda web
        Retorna: {"question": "mejor_pregunta"}
        â†“
      [web_search_node]
        Busca en web usando Tavily
        Agrega resultados a documents
        Retorna: {"documents": [...incluye web]}
        â†“
      [generate] â†’ END
        Genera respuesta con documentos web
```

### ğŸ¯ Estado del Grafo (GraphState)

```python
class GraphState(TypedDict):
    question: str       # Pregunta original o reescrita
    generation: str     # Respuesta generada final
    web_search: str     # "Yes" o "No" - necesita bÃºsqueda web
    documents: List[str]  # Documentos locales + web (si aplica)
```

**Flujo de datos**:
1. **Entrada**: `{"question": "memoria de agentes"}`
2. **DespuÃ©s de retrieve**: `{"question": "...", "documents": [...]}`
3. **DespuÃ©s de grade**: `{"...", "web_search": "Yes", "documents": filtered}`
4. **DespuÃ©s de transform**: `{"question": "mejor pregunta", ...}`
5. **DespuÃ©s de generate**: `{"...", "generation": "respuesta final"}`

### ğŸ’¡ Casos de Uso de CRAG

#### **âœ… Usa CRAG cuando**:
- Tu vectorstore tiene informaciÃ³n limitada o desactualizada
- Las preguntas de usuarios son impredecibles o mal formuladas
- Necesitas alta confiabilidad (evitar alucinaciones)
- Quieres combinar conocimiento local + web automÃ¡ticamente
- La calidad de respuestas es crÃ­tica (mejor que velocidad)

**Ejemplos**:
- **Soporte tÃ©cnico**: Base de conocimiento interna + Stack Overflow
- **InvestigaciÃ³n**: Papers locales + bÃºsqueda acadÃ©mica web
- **E-commerce**: CatÃ¡logo interno + reviews web
- **Legal/Compliance**: Documentos corporativos + regulaciones pÃºblicas

#### **âŒ No uses CRAG cuando**:
- Tu vectorstore es completo y siempre tiene respuestas
- La latencia es crÃ­tica (CRAG aÃ±ade evaluaciÃ³n + posible bÃºsqueda web)
- Los costos de API son prohibitivos (evaluaciÃ³n = llamada LLM extra)
- Las consultas siempre son relevantes a tus documentos
- No tienes acceso a bÃºsqueda web (Tavily API)

### ğŸ”§ ImplementaciÃ³n Paso a Paso

#### **Paso 1: Construir Vectorstore**
```python
# Cargar documentos web
docs = [WebBaseLoader(url).load() for url in urls]
docs_list = [item for sublist in docs for item in sublist]

# Dividir en chunks
text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=500, chunk_overlap=0
)
doc_splits = text_splitter.split_documents(docs_list)

# Crear vectorstore FAISS
vectorstore = FAISS.from_documents(
    documents=doc_splits,
    embedding=OpenAIEmbeddings()
)
retriever = vectorstore.as_retriever()
```

#### **Paso 2: Crear Evaluador**
```python
# Modelo Pydantic para salida estructurada
class GradeDocuments(BaseModel):
    binary_score: str = Field(description="'yes' o 'no'")

# LLM con structured output
llm = ChatOpenAI(model="gpt-3.5-turbo-0125", temperature=0)
structured_llm_grader = llm.with_structured_output(GradeDocuments)

# Prompt de evaluaciÃ³n
system = """Eres un evaluador que analiza relevancia de documentos.
Si el documento contiene keywords o significado semÃ¡ntico relacionado
con la pregunta, califÃ­calo como relevante. Da 'yes' o 'no'."""

grade_prompt = ChatPromptTemplate.from_messages([
    ("system", system),
    ("human", "Documento: {document}\nPregunta: {question}")
])

retrieval_grader = grade_prompt | structured_llm_grader
```

#### **Paso 3: Definir Nodos del Grafo**
```python
def retrieve(state):
    """Recuperar documentos del vectorstore"""
    documents = retriever.invoke(state["question"])
    return {"documents": documents, "question": state["question"]}

def grade_documents(state):
    """Evaluar relevancia de cada documento"""
    filtered_docs = []
    web_search = "No"

    for doc in state["documents"]:
        score = retrieval_grader.invoke({
            "question": state["question"],
            "document": doc.page_content
        })
        if score.binary_score == "yes":
            filtered_docs.append(doc)
        else:
            web_search = "Yes"

    return {
        "documents": filtered_docs,
        "question": state["question"],
        "web_search": web_search
    }

def transform_query(state):
    """Reescribir pregunta para bÃºsqueda web"""
    better_question = question_rewriter.invoke({
        "question": state["question"]
    })
    return {"documents": state["documents"], "question": better_question}

def web_search(state):
    """Buscar en web usando Tavily"""
    docs = web_search_tool.invoke({"query": state["question"]})
    web_results = "\n".join([d["content"] for d in docs])
    state["documents"].append(Document(page_content=web_results))
    return {"documents": state["documents"], "question": state["question"]}

def generate(state):
    """Generar respuesta final"""
    generation = rag_chain.invoke({
        "context": state["documents"],
        "question": state["question"]
    })
    return {
        "documents": state["documents"],
        "question": state["question"],
        "generation": generation
    }
```

#### **Paso 4: Construir el Grafo**
```python
from langgraph.graph import StateGraph, START, END

workflow = StateGraph(GraphState)

# Agregar nodos
workflow.add_node("retrieve", retrieve)
workflow.add_node("grade_documents", grade_documents)
workflow.add_node("generate", generate)
workflow.add_node("transform_query", transform_query)
workflow.add_node("web_search_node", web_search)

# Agregar aristas
workflow.add_edge(START, "retrieve")
workflow.add_edge("retrieve", "grade_documents")

# Arista condicional: decide segÃºn relevancia
workflow.add_conditional_edges(
    "grade_documents",
    decide_to_generate,
    {
        "transform_query": "transform_query",
        "generate": "generate",
    }
)

workflow.add_edge("transform_query", "web_search_node")
workflow.add_edge("web_search_node", "generate")
workflow.add_edge("generate", END)

# Compilar
app = workflow.compile()
```

#### **Paso 5: Ejecutar el Flujo**
```python
# Ejecutar con una pregunta
result = app.invoke({
    "question": "What are the types of agent memory?"
})

print(result["generation"])
# Output: "Los tipos de memoria de agentes son: memoria a corto plazo
#          (in-context learning) y memoria a largo plazo (almacenamiento
#          externo con recuperaciÃ³n basada en relevancia, recencia e
#          importancia)..."
```

### ğŸš€ Optimizaciones y Mejores PrÃ¡cticas

#### **1. Threshold de Relevancia**
```python
# Ajustar sensibilidad del evaluador
if len(filtered_docs) >= 2:  # Al menos 2 docs relevantes
    web_search = "No"  # No buscar en web
else:
    web_search = "Yes"  # Buscar aunque haya 1 doc relevante
```

#### **2. Cache de Evaluaciones**
```python
# Evitar evaluar el mismo documento mÃºltiples veces
evaluation_cache = {}

def grade_documents_cached(state):
    for doc in state["documents"]:
        doc_hash = hash(doc.page_content)
        if doc_hash not in evaluation_cache:
            evaluation_cache[doc_hash] = retrieval_grader.invoke(...)
        score = evaluation_cache[doc_hash]
```

#### **3. Fallback Strategy**
```python
# Si web search tambiÃ©n falla, usar respuesta genÃ©rica
def generate(state):
    if not state["documents"]:
        return {
            "generation": "No encontrÃ© informaciÃ³n relevante. "
                          "Â¿Puedes reformular tu pregunta?"
        }
    # Generar normalmente...
```

#### **4. Logging y Observabilidad**
```python
def grade_documents(state):
    print(f"---EVALUANDO {len(state['documents'])} DOCUMENTOS---")

    for i, doc in enumerate(state["documents"]):
        score = retrieval_grader.invoke(...)
        if score.binary_score == "yes":
            print(f"  âœ“ Doc {i+1}: RELEVANTE")
            filtered_docs.append(doc)
        else:
            print(f"  âœ— Doc {i+1}: NO RELEVANTE")
            web_search = "Yes"

    print(f"---RESULTADO: {len(filtered_docs)} relevantes, "
          f"bÃºsqueda web = {web_search}---")
```

### âš ï¸ Consideraciones Importantes

#### **Costos**
- Cada documento evaluado = 1 llamada LLM adicional
- 4 documentos recuperados = 4 evaluaciones = costo significativo
- Usa modelos econÃ³micos (gpt-3.5-turbo) para evaluaciÃ³n
- Considera evaluar solo top-k documentos (e.g., top 2)

#### **Latencia**
- EvaluaciÃ³n aÃ±ade ~1-2 segundos por documento
- BÃºsqueda web aÃ±ade ~2-3 segundos adicionales
- Total: puede ser 5-10 segundos vs 2 segundos de RAG tradicional
- Considera evaluaciÃ³n paralela si tienes muchos documentos

#### **PrecisiÃ³n del Evaluador**
- El evaluador puede cometer errores (falsos positivos/negativos)
- Usa temperature=0 para consistencia
- Considera usar un modelo mÃ¡s potente (GPT-4) para evaluaciÃ³n crÃ­tica
- EvalÃºa el evaluador periÃ³dicamente con ground truth

#### **Dependencia de BÃºsqueda Web**
- Requiere Tavily API key (servicio de pago)
- Tiene lÃ­mites de rate (considera caching)
- BÃºsquedas web pueden retornar informaciÃ³n desactualizada o incorrecta
- Considera implementar filtrado adicional de resultados web

### ğŸ“Š MÃ©tricas de Ã‰xito

**ComparaciÃ³n CRAG vs RAG Tradicional**:

| MÃ©trica | RAG Tradicional | CRAG |
|---------|----------------|------|
| **PrecisiÃ³n** | 70-80% | 85-95% |
| **Tasa de alucinaciÃ³n** | 15-25% | 5-10% |
| **Latencia promedio** | 2s | 6s |
| **Costo por consulta** | $0.002 | $0.008 |
| **Cobertura** | Solo docs locales | Local + Web |
| **Adaptabilidad** | Baja | Alta |

### ğŸ¯ CuÃ¡ndo Usar CRAG vs Alternativas

**CRAG** es ideal para:
- Sistemas donde la precisiÃ³n es mÃ¡s importante que la velocidad
- Dominios donde el conocimiento local es limitado
- Aplicaciones que requieren informaciÃ³n actualizada
- Casos donde las alucinaciones son inaceptables

**Alternativas**:
- **RAG Tradicional**: Cuando velocidad > precisiÃ³n
- **RAG AgÃ©ntico**: Cuando necesitas razonamiento complejo
- **Self-RAG**: Cuando necesitas auto-reflexiÃ³n iterativa
- **Adaptive RAG**: Cuando necesitas enrutamiento multi-fuente

### ğŸ“š Recursos y Referencias

- **Paper Original**: "Corrective Retrieval Augmented Generation" (CRAG)
- **LangGraph**: Framework para grafos de estado con nodos condicionales
- **Tavily API**: Motor de bÃºsqueda optimizado para IA
- **Structured Output**: `with_structured_output()` en LangChain
- **Pydantic**: ValidaciÃ³n de datos y esquemas

---

## ğŸ¯ RAG Adaptativo (Adaptive RAG)

**Adaptive RAG** es el patrÃ³n mÃ¡s completo y robusto de RAG, que combina **enrutamiento inteligente**, **evaluaciÃ³n multi-nivel** y **auto-correcciÃ³n automÃ¡tica** en un solo flujo adaptativo. Este es el enfoque ideal para aplicaciones de producciÃ³n donde la calidad y confiabilidad son crÃ­ticas.

### ğŸ¯ Â¿QuÃ© es Adaptive RAG?

Adaptive RAG toma decisiones inteligentes en **mÃºltiples puntos** del flujo, adaptÃ¡ndose dinÃ¡micamente segÃºn la calidad de los documentos y respuestas:

```
INICIO â†’ Router Inteligente
    â†“
    Â¿Vectorstore o Web?
    â†“
â”Œâ”€â”€â”€â”´â”€â”€â”€â”
â”‚  WEB  â”‚ â†’ Generar â†’ Validar â†’ âœ“ FIN
â””â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”
â”‚ VECTOR â”‚ â†’ Evaluar Docs â†’ Â¿Relevantes?
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â†“
              SÃ â†“   NOâ†“
                 â†“    Reescribir â†’ Reintentar
           Generar
                 â†“
           Validar â†’ Â¿Alucinaciones?
                 â†“
              NO â†“   SÃâ†“
                 â†“    Regenerar
           Â¿Contesta pregunta?
                 â†“
              SÃ â†“   NOâ†“
                 â†“    Reescribir â†’ Reintentar
                 âœ“
               FIN
```

**Diferencias clave**:
- **RAG Tradicional**: Recupera â†’ Genera (sin validaciÃ³n)
- **CRAG**: Recupera â†’ EvalÃºa â†’ Corrige si es necesario â†’ Genera
- **Adaptive RAG**: **Enruta** â†’ Recupera â†’ **EvalÃºa** â†’ Genera â†’ **Valida Alucinaciones** â†’ **Valida Respuesta** â†’ **Auto-Corrige** hasta Ã©xito

### ğŸ§© Componentes de Adaptive RAG

#### 1ï¸âƒ£ **Router (Enrutador Inteligente)**

El primer componente que decide la fuente de datos ANTES de recuperar.

```python
class RouteQuery(BaseModel):
    datasource: Literal["vectorstore", "web_search"] = Field(...)

# Router LLM con salida estructurada
question_router = route_prompt | llm.with_structured_output(RouteQuery)
```

**Â¿CÃ³mo funciona?**
- Analiza la pregunta del usuario
- Compara con los temas del vectorstore (agentes, prompts, ataques)
- Decide: "vectorstore" si es un tema local, "web_search" para todo lo demÃ¡s

**Ventajas**:
- âœ… Evita recuperaciÃ³n innecesaria si claramente necesita web
- âœ… Ahorra costos de embedding cuando la respuesta estÃ¡ en web
- âœ… Reduce latencia al evitar bÃºsquedas vectoriales innecesarias

#### 2ï¸âƒ£ **Retrieval Grader (Evaluador de Relevancia)**

Igual que en CRAG: califica cada documento recuperado.

```python
class GradeDocuments(BaseModel):
    binary_score: str = Field(description="'yes' o 'no'")

retrieval_grader = grade_prompt | llm.with_structured_output(GradeDocuments)
```

**Criterio**: La prueba NO debe ser estricta. El objetivo es filtrar solo recuperaciones **errÃ³neas**, no mediocres.

#### 3ï¸âƒ£ **Hallucination Grader (Detector de Alucinaciones)**

**Novedad en Adaptive RAG**: Verifica que la respuesta estÃ© fundamentada en los documentos.

```python
class GradeHallucinations(BaseModel):
    binary_score: str = Field(description="'yes' = grounded, 'no' = hallucination")

hallucination_grader = hallucination_prompt | llm.with_structured_output(GradeHallucinations)
```

**Â¿QuÃ© detecta?**
- InformaciÃ³n NO presente en los documentos
- Hechos inventados por el LLM
- Extrapolaciones no fundamentadas

**Ejemplo**:
- Documentos: "Los agentes tienen memoria a corto plazo"
- Respuesta con alucinaciÃ³n: "Los agentes tienen memoria a corto, largo y ultra-largo plazo"
- Detector: `binary_score='no'` â†’ Regenerar

#### 4ï¸âƒ£ **Answer Grader (Validador de Respuestas)**

**Novedad en Adaptive RAG**: Verifica que la respuesta REALMENTE conteste la pregunta.

```python
class GradeAnswer(BaseModel):
    binary_score: str = Field(description="'yes' = answers question, 'no' = doesn't")

answer_grader = answer_prompt | llm.with_structured_output(GradeAnswer)
```

**Â¿QuÃ© valida?**
- La respuesta ABORDA la pregunta del usuario
- No es solo informaciÃ³n relacionada, sino la RESPUESTA ESPECÃFICA

**Ejemplo**:
- Pregunta: "Â¿CuÃ¡ntos tipos de memoria tienen los agentes?"
- Respuesta inÃºtil: "Los agentes usan memoria para aprender"
- Validador: `binary_score='no'` â†’ Reescribir pregunta y reintentar

#### 5ï¸âƒ£ **Question Rewriter (Reescritor de Preguntas)**

Optimiza la pregunta cuando falla la recuperaciÃ³n o respuesta.

```python
question_rewriter = re_write_prompt | llm | StrOutputParser()
```

**Diferencia vs CRAG**:
- CRAG: Optimiza para **bÃºsqueda web**
- Adaptive RAG: Optimiza para **recuperaciÃ³n en vectorstore**

### ğŸ“Š Arquitectura del Flujo Adaptive RAG

El flujo mÃ¡s complejo con 3 decisiones condicionales:

```
START
  â†“
[route_question] â† DECISIÃ“N 1: Â¿Vectorstore o Web?
  â†“
  â”œâ”€â†’ "web_search"
  â”‚     â†“
  â”‚   [web_search] â†’ [generate]
  â”‚                      â†“
  â”‚                [grade_generation_v_documents_and_question] â† DECISIÃ“N 3
  â”‚                      â†“
  â”‚                      â”œâ”€â†’ "useful" â†’ END âœ“
  â”‚                      â”œâ”€â†’ "not useful" â†’ [transform_query] â†’ [retrieve] (ciclo)
  â”‚                      â””â”€â†’ "not supported" â†’ [generate] (ciclo)
  â”‚
  â””â”€â†’ "vectorstore"
        â†“
      [retrieve]
        â†“
      [grade_documents] â† DECISIÃ“N 2: Â¿Documentos relevantes?
        â†“
        â”œâ”€â†’ "generate"
        â”‚     â†“
        â”‚   [generate]
        â”‚     â†“
        â”‚   [grade_generation_v_documents_and_question] â† DECISIÃ“N 3
        â”‚     â†“
        â”‚     â”œâ”€â†’ "useful" â†’ END âœ“
        â”‚     â”œâ”€â†’ "not useful" â†’ [transform_query] â†’ [retrieve] (ciclo)
        â”‚     â””â”€â†’ "not supported" â†’ [generate] (ciclo)
        â”‚
        â””â”€â†’ "transform_query" â†’ [retrieve] (ciclo)

CICLOS DE AUTO-CORRECCIÃ“N:
1. transform_query â†’ retrieve â†’ grade_documents (si docs no relevantes)
2. generate â†’ grade_generation (si alucinaciones)
3. transform_query â†’ retrieve â†’ generate (si respuesta no Ãºtil)
```

### ğŸ¯ Tres Decisiones Condicionales

**DECISIÃ“N 1: route_question()**
```python
def route_question(state):
    source = question_router.invoke({"question": state["question"]})
    if source.datasource == "web_search":
        return "web_search"  # Ir directo a bÃºsqueda web
    else:
        return "vectorstore"  # Ir a recuperaciÃ³n vectorial
```

**DECISIÃ“N 2: decide_to_generate()**
```python
def decide_to_generate(state):
    if not state["documents"]:  # No hay documentos relevantes
        return "transform_query"  # Reescribir y reintentar
    else:
        return "generate"  # Generar respuesta
```

**DECISIÃ“N 3: grade_generation_v_documents_and_question()**
```python
def grade_generation_v_documents_and_question(state):
    # Paso 1: Â¿Hay alucinaciones?
    if hallucination_grader(...).binary_score == "yes":
        # Paso 2: Â¿Contesta la pregunta?
        if answer_grader(...).binary_score == "yes":
            return "useful"  # âœ“ Todo bien, terminar
        else:
            return "not useful"  # Reescribir pregunta y reintentar
    else:
        return "not supported"  # Regenerar (hay alucinaciones)
```

### ğŸ’¡ Casos de Uso de Adaptive RAG

#### **âœ… Usa Adaptive RAG cuando**:
- Necesitas el **mÃ¡ximo nivel de calidad** y confiabilidad
- Las alucinaciones son **inaceptables** (legal, mÃ©dico, financiero)
- El conocimiento estÃ¡ **distribuido** entre local + web
- Los usuarios hacen **preguntas impredecibles** de mÃºltiples dominios
- Tienes presupuesto para **mÃºltiples llamadas LLM** (evaluadores)
- La latencia es **secundaria** a la calidad

**Ejemplos**:
- **Asistentes legales**: Combina precedentes locales + leyes pÃºblicas
- **Soporte mÃ©dico**: Base de conocimiento interna + investigaciÃ³n web
- **ConsultorÃ­a financiera**: Datos corporativos + mercados en tiempo real
- **InvestigaciÃ³n acadÃ©mica**: Papers locales + bÃºsqueda web actualizada

#### **âŒ No uses Adaptive RAG cuando**:
- Necesitas **baja latencia** (cada evaluador aÃ±ade ~1-2s)
- Presupuesto de API es **limitado** (3-4 evaluaciones por consulta)
- El conocimiento es **completo** en una sola fuente
- RAG tradicional o CRAG ya dan **resultados aceptables**

### ğŸš€ ComparaciÃ³n: RAG Tradicional vs CRAG vs Adaptive RAG

| CaracterÃ­stica | RAG Tradicional | CRAG | Adaptive RAG |
|----------------|----------------|------|--------------|
| **Enrutamiento** | âŒ No | âŒ No | âœ… SÃ­ (Router) |
| **EvaluaciÃ³n de docs** | âŒ No | âœ… SÃ­ | âœ… SÃ­ |
| **BÃºsqueda web** | âŒ No | âœ… Condicional | âœ… Enrutada |
| **DetecciÃ³n alucinaciones** | âŒ No | âŒ No | âœ… SÃ­ |
| **ValidaciÃ³n respuesta** | âŒ No | âŒ No | âœ… SÃ­ |
| **Ciclos auto-correcciÃ³n** | âŒ No | âœ… 1 ciclo | âœ… 3 ciclos |
| **Complejidad** | Baja | Media | Alta |
| **Llamadas LLM promedio** | 1 | 2-4 | 4-8 |
| **Latencia** | ~2s | ~6s | ~10-15s |
| **Costo por consulta** | $0.002 | $0.008 | $0.015 |
| **PrecisiÃ³n** | 70-80% | 85-95% | 95-98% |
| **Tasa alucinaciÃ³n** | 15-25% | 5-10% | 2-5% |
| **Mejor para** | Demos, MVPs | ProducciÃ³n media | ProducciÃ³n crÃ­tica |

### âš ï¸ Consideraciones Importantes

#### **Costos**
- **3-4 evaluadores por consulta**: router + grader + hallucination + answer
- Si hay ciclos: puede llegar a 8-10 llamadas LLM
- Usa `gpt-4o-mini` en vez de `gpt-4` para evaluadores (10x mÃ¡s barato)

#### **Latencia**
- Cada evaluador: ~1-2 segundos
- Flujo completo exitoso: ~10-15 segundos
- Con ciclos de correcciÃ³n: puede llegar a 20-30 segundos
- Considera evaluaciÃ³n paralela si es posible

#### **Ciclos Infinitos**
- Implementa un **lÃ­mite de iteraciones** (max 3 reintentos)
- Tracking de estados visitados para evitar loops
- Fallback a respuesta genÃ©rica si se agotan intentos

#### **Calidad de Evaluadores**
- Los evaluadores pueden cometer errores (falsos positivos/negativos)
- Usa `temperature=0` para consistencia
- Considera GPT-4 para evaluaciÃ³n crÃ­tica (mÃ¡s caro pero mÃ¡s preciso)
- EvalÃºa los evaluadores periÃ³dicamente con ground truth

### ğŸ¯ CuÃ¡ndo Usar QuÃ©

**RAG Tradicional** â†’ Demos, prototipos, latencia crÃ­tica
**CRAG** â†’ ProducciÃ³n con presupuesto medio, necesitas correcciÃ³n bÃ¡sica
**Adaptive RAG** â†’ ProducciÃ³n crÃ­tica, calidad mÃ¡xima, presupuesto holgado

### ğŸ“š Recursos y Referencias

- **LangGraph**: Framework para grafos complejos con mÃºltiples decisiones
- **Structured Output**: `with_structured_output()` con Pydantic para evaluadores
- **Tavily API**: Motor de bÃºsqueda para agentes de IA
- **Literal Types**: Para enrutamiento con opciones limitadas
- **Conditional Edges**: `add_conditional_edges()` para flujos adaptativos

---

## ğŸ’¾ RAG con Memoria Persistente

**RAG con Memoria Persistente** permite que tu sistema RAG **mantenga el contexto completo** de conversaciones entre mÃºltiples interacciones con el usuario. A diferencia del RAG tradicional (sin memoria), este patrÃ³n usa **LangGraph con MemorySaver** para recordar preguntas previas, respuestas anteriores y mantener el contexto conversacional.

### ğŸ§  Â¿QuÃ© es la Memoria Persistente?

La memoria persistente en RAG significa que el sistema puede:
- ğŸ”„ **Recordar conversaciones completas** entre sesiones
- ğŸ’¬ **Entender referencias contextuales** ("Â¿y quÃ© mÃ¡s?", "explica eso mejor")
- ğŸ¯ **Mantener mÃºltiples hilos** independientes con thread IDs Ãºnicos
- ğŸ“ **Reutilizar contexto previo** sin repetir preguntas

```
SIN MEMORIA:
Usuario: Â¿QuÃ© es LangGraph?
Bot: LangGraph es un framework para construir grafos de estado...

Usuario: Â¿QuÃ© ventajas tiene?
Bot: âŒ No sÃ© de quÃ© hablas (no recuerda el contexto)

CON MEMORIA:
Usuario: Â¿QuÃ© es LangGraph?
Bot: LangGraph es un framework para construir grafos de estado...

Usuario: Â¿QuÃ© ventajas tiene?
Bot: âœ… Las ventajas de LangGraph incluyen... (recuerda que hablamos de LangGraph)
```

### ğŸ¯ Componentes Clave

#### 1ï¸âƒ£ **MemorySaver y Checkpoints**

El **MemorySaver** guarda el estado completo del grafo en cada paso.

```python
from langgraph.checkpoint.memory import MemorySaver

# Crear gestor de memoria
memory = MemorySaver()

# Compilar grafo con memoria
graph = graph_builder.compile(checkpointer=memory)
```

**Â¿QuÃ© se guarda?**
- ğŸ“¨ Todos los mensajes (usuario + asistente + herramientas)
- ğŸ”§ Estado del grafo (nodo actual, variables)
- ğŸ• Historial de ejecuciÃ³n completo

#### 2ï¸âƒ£ **Thread IDs para Sesiones**

Cada conversaciÃ³n se identifica con un **thread_id** Ãºnico.

```python
# ConversaciÃ³n del usuario A
config_user_a = {"configurable": {"thread_id": "user-123"}}
graph.invoke({"messages": [...]}, config_user_a)

# ConversaciÃ³n del usuario B (independiente)
config_user_b = {"configurable": {"thread_id": "user-456"}}
graph.invoke({"messages": [...]}, config_user_b)
```

**Ventajas**:
- âœ… MÃºltiples usuarios simultÃ¡neos sin mezclar contextos
- âœ… Retomar conversaciones en cualquier momento
- âœ… Aislar sesiones (web, mÃ³vil, etc.)

#### 3ï¸âƒ£ **MessagesState**

`MessagesState` mantiene automÃ¡ticamente el historial de mensajes.

```python
from langgraph.graph import MessagesState

class RAGState(MessagesState):
    # MessagesState ya incluye:
    # - messages: List[BaseMessage]
    # AÃ±adimos campos adicionales:
    context_docs: List[Document]
    answer: Optional[str]
```

**Tipos de mensajes**:
- `HumanMessage`: Mensajes del usuario
- `AIMessage`: Respuestas del asistente
- `ToolMessage`: Resultados de herramientas
- `SystemMessage`: Instrucciones del sistema

### ğŸ“Š Arquitectura del Flujo con Memoria

```
INICIO
  â†“
[query_or_respond] â†’ Decide si necesita recuperar info
  â†“
  Â¿Necesita herramienta?
  â†“
  SÃ â†’ [tools] â†’ Ejecuta retrieve â†’ [generate]
  NO â†’ Responde directo
  â†“
[generate] â†’ Genera respuesta con contexto + historial
  â†“
END (guarda checkpoint con MemorySaver)

SEGUNDA INTERACCIÃ“N:
INICIO (carga checkpoint previo)
  â†“
[query_or_respond] â†’ Analiza con HISTORIAL completo
  â†“
...continÃºa...
```

### ğŸ”„ Flujo de Trabajo TÃ­pico

**Primera Pregunta** (sin contexto):
```python
# Usuario pregunta sobre Task Decomposition
state = {
    "messages": [HumanMessage("Â¿QuÃ© es Task Decomposition?")]
}
result = graph.invoke(state, config)

# Sistema:
# 1. Invoca retrieve para buscar docs
# 2. Genera respuesta con contexto recuperado
# 3. Guarda: [HumanMessage, AIMessage, ToolMessage]
```

**Pregunta de Seguimiento** (con contexto):
```python
# Usuario hace pregunta vaga que requiere contexto
state = {
    "messages": [HumanMessage("Â¿Puedes darme ejemplos de eso?")]
}
result = graph.invoke(state, config)

# Sistema:
# 1. Carga historial: [mensaje anterior sobre Task Decomposition]
# 2. Entiende que "eso" = Task Decomposition
# 3. Recupera ejemplos especÃ­ficos
# 4. Genera respuesta contextualizada
```

### ğŸ’¡ Casos de Uso

#### âœ… **Usa Memoria Persistente cuando**:
- ğŸ¯ **Conversaciones multi-turno**: Chatbots, asistentes, soporte tÃ©cnico
- ğŸ”„ **Referencia contextual**: Usuarios usan "eso", "aquello", "lo anterior"
- ğŸ“‹ **Tareas secuenciales**: Cada paso depende del anterior
- ğŸ‘¥ **MÃºltiples usuarios**: Necesitas aislar sesiones independientes
- ğŸ• **Sesiones largas**: Conversaciones que duran minutos u horas

**Ejemplos**:
- ğŸ’¬ **Chatbot de Soporte**: Recuerda el problema reportado y el historial de troubleshooting
- ğŸ“ **Tutor Educativo**: Mantiene progreso del estudiante y conceptos ya explicados
- ğŸ“ **Asistente de Escritura**: Recuerda el contexto del documento que se estÃ¡ editando
- ğŸ›’ **E-commerce**: Mantiene preferencias, bÃºsquedas previas, artÃ­culos consultados

#### âŒ **No uses Memoria cuando**:
- âš¡ **Consultas independientes**: Cada pregunta no requiere contexto previo
- ğŸ”’ **Privacidad estricta**: No debes guardar historial de conversaciÃ³n
- ğŸ’¾ **LÃ­mites de almacenamiento**: Memoria crece con el tiempo (puede requerir limpieza)
- ğŸš€ **Latencia crÃ­tica**: Cargar historial aÃ±ade overhead

### ğŸ¨ ImplementaciÃ³n con ReAct

El patrÃ³n **ReAct** (Reasoning + Acting) se puede combinar con memoria:

```python
from langgraph.prebuilt import create_react_agent

# Agente ReAct con memoria persistente
agent = create_react_agent(
    llm=llm,
    tools=[retrieve],  # Herramientas disponibles
    checkpointer=memory  # Habilita memoria
)

# ConversaciÃ³n continua
agent.invoke({"messages": [...]}, config)
```

**Ventajas del ReAct con Memoria**:
- âœ… El agente **recuerda decisiones previas**
- âœ… **No repite recuperaciones** innecesarias
- âœ… **Referencia resultados anteriores** sin volver a buscar

### ğŸ“ˆ GestiÃ³n de Memoria

#### **Recuperar Historial**
```python
# Obtener estado guardado
state = graph.get_state(config)
chat_history = state.values["messages"]

for message in chat_history:
    print(f"{message.type}: {message.content}")
```

#### **Limpiar Memoria (opcional)**
```python
# Si necesitas reiniciar conversaciÃ³n
# Simplemente usa un nuevo thread_id
config_new = {"configurable": {"thread_id": "new-conversation"}}
```

#### **LÃ­mites de Contexto**
- âš ï¸ **Crecimiento ilimitado**: El historial puede crecer indefinidamente
- ğŸ”§ **SoluciÃ³n**: Implementa **ventana deslizante** (Ãºltimos N mensajes)
- ğŸ’¡ **Alternativa**: **Resumir historial antiguo** periÃ³dicamente

### ğŸš€ ComparaciÃ³n: Sin Memoria vs Con Memoria

| CaracterÃ­stica | Sin Memoria | Con Memoria |
|----------------|-------------|-------------|
| **Contexto conversacional** | âŒ No | âœ… SÃ­ |
| **Referencias ("eso", "aquello")** | âŒ No entiende | âœ… Entiende |
| **MÃºltiples turnos** | âŒ Cada pregunta aislada | âœ… ConversaciÃ³n fluida |
| **Sesiones independientes** | âŒ No aplica | âœ… Thread IDs |
| **Overhead de memoria** | âœ… Ninguno | âš ï¸ Crece con el tiempo |
| **Latencia** | âœ… Baja | âš ï¸ +100-300ms (carga historial) |
| **Complejidad** | âœ… Simple | âš ï¸ Media |
| **Mejor para** | APIs stateless | Chatbots, asistentes |

---

## âš¡ Cache-Augmented Generation (CAG)

**Cache-Augmented Generation (CAG)** es una tÃ©cnica de optimizaciÃ³n que **reutiliza respuestas previas** cuando detecta preguntas similares, reduciendo drÃ¡sticamente costos de API y latencia. A diferencia del RAG tradicional que siempre recupera y genera, CAG **usa cachÃ© semÃ¡ntico** para evitar llamadas innecesarias al LLM.

### ğŸ¯ Â¿QuÃ© es CAG?

CAG **precomputa y almacena respuestas** para reutilizarlas cuando aparecen preguntas semÃ¡nticamente similares:

```
SIN CACHÃ‰ (RAG Tradicional):
Usuario: Â¿QuÃ© es LangGraph?
Sistema: Retrieve â†’ Generate (17 segundos, $0.005)

Usuario: Â¿QuÃ© es Langgraph?  (casi idÃ©ntica)
Sistema: Retrieve â†’ Generate (17 segundos, $0.005)  â† INEFICIENTE

CON CACHÃ‰ (CAG):
Usuario: Â¿QuÃ© es LangGraph?
Sistema: Retrieve â†’ Generate â†’ Cache (17 segundos, $0.005)

Usuario: Â¿QuÃ© es Langgraph?  (detecta similitud)
Sistema: Cache Hit! (0.01 segundos, $0.000)  â† 1700x MÃS RÃPIDO
```

### ğŸ”‘ Tipos de CachÃ©

#### 1ï¸âƒ£ **CachÃ© Simple (Diccionario)**

Coincidencia **exacta** de strings:

```python
cache = {}

def cache_model(query):
    if query in cache:
        return cache[query]  # âœ… Cache hit
    else:
        response = llm.invoke(query)  # âŒ Cache miss
        cache[query] = response
        return response
```

**Problema**: Solo funciona con strings **idÃ©nticos**:
- âœ… "Â¿QuÃ© es LangGraph?" â†’ âœ… "Â¿QuÃ© es LangGraph?"
- âŒ "Â¿QuÃ© es LangGraph?" â†’ âŒ "Â¿QuÃ© es Langgraph?" (falla por mayÃºscula)

#### 2ï¸âƒ£ **CachÃ© SemÃ¡ntico (FAISS)**

Usa **embeddings vectoriales** para detectar similitud semÃ¡ntica:

```python
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings

# Crear Ã­ndice FAISS para cachÃ©
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
cache_index = faiss.IndexFlatL2(384)  # 384 dimensiones

qa_cache = FAISS(
    embedding_function=embeddings,
    index=cache_index,
    docstore=InMemoryDocstore({}),
    index_to_docstore_id={}
)
```

**Ventajas**:
- âœ… Detecta preguntas **parafraseadas**: "Â¿QuÃ© es X?" â‰ˆ "ExplÃ­came X" â‰ˆ "Define X"
- âœ… Tolerante a **errores tipogrÃ¡ficos**: "LangGraph" â‰ˆ "Langgraph"
- âœ… Funciona en **mÃºltiples idiomas** (si el modelo lo soporta)

### ğŸ“Š Arquitectura del Flujo CAG

```
INICIO
  â†“
[normalize_query] â†’ Convierte a minÃºsculas, limpia espacios
  â†“
[semantic_cache_lookup] â†’ Busca en FAISS con umbral de similitud
  â†“
  Â¿Cache hit?
  â†“
  SÃ â†’ [respond_from_cache] â†’ Retorna respuesta guardada â†’ END
  NO â†’ [retrieve] â†’ Recupera docs del vector store
          â†“
        [generate] â†’ Genera respuesta con LLM
          â†“
        [cache_write] â†’ Guarda en cachÃ© con metadatos
          â†“
        END
```

### ğŸ§© Componentes de CAG

#### 1ï¸âƒ£ **NormalizaciÃ³n de Consultas**

```python
def normalize_query(state):
    q = state["question"].strip().lower()
    state["normalized_question"] = q
    return state
```

**Â¿Por quÃ© normalizar?**
- ğŸ”„ Reduce duplicados: "LangGraph" = "langgraph"
- ğŸ“ Mejora coincidencias: "  QuÃ© es X  " = "QuÃ© es X"

#### 2ï¸âƒ£ **BÃºsqueda SemÃ¡ntica en CachÃ©**

```python
def semantic_cache_lookup(state):
    q = state["normalized_question"]

    # Buscar preguntas similares en cachÃ©
    hits = qa_cache.similarity_search_with_score(q, k=3)

    if hits:
        doc, distance = hits[0]

        # Si la distancia L2 es menor al umbral â†’ cache hit
        if distance <= CACHE_DISTANCE_THRESHOLD:
            state["answer"] = doc.metadata["answer"]
            state["cache_hit"] = True

    return state
```

**ParÃ¡metros clave**:
- `CACHE_DISTANCE_THRESHOLD`: Umbral de similitud (ej: 0.45)
  - Menor = **mÃ¡s estricto** (solo preguntas muy similares)
  - Mayor = **mÃ¡s permisivo** (preguntas menos similares)
- Distancia L2: **0.0** = idÃ©ntico, **>1.0** = muy diferente

#### 3ï¸âƒ£ **Escritura en CachÃ©**

```python
def cache_write(state):
    q = state["normalized_question"]
    a = state["answer"]

    # Guardar pregunta + respuesta + timestamp
    qa_cache.add_texts(
        texts=[q],
        metadatas=[{
            "answer": a,
            "ts": time.time()  # Para TTL
        }]
    )
    return state
```

**Metadatos guardados**:
- `answer`: La respuesta generada
- `ts`: Timestamp para Time-To-Live (opcional)

### â° Time-To-Live (TTL)

Opcional: Las entradas caducadas se ignoran.

```python
CACHE_TTL_SEC = 3600  # 1 hora

# En semantic_cache_lookup:
ts = doc.metadata.get("ts")
if time.time() - ts > CACHE_TTL_SEC:
    # Cache expirado, ignorar
    return state
```

**Casos de uso**:
- ğŸ“° **Noticias**: CachÃ© de 5 minutos (informaciÃ³n cambia rÃ¡pido)
- ğŸ“š **DocumentaciÃ³n tÃ©cnica**: CachÃ© de dÃ­as/semanas (informaciÃ³n estable)
- ğŸ”„ **Sin TTL**: Para informaciÃ³n que nunca cambia

### ğŸ’¡ Casos de Uso de CAG

#### âœ… **Usa CAG cuando**:
- ğŸ” **Preguntas repetitivas**: FAQ, soporte tÃ©cnico, educaciÃ³n
- ğŸ’° **OptimizaciÃ³n de costos**: Reduce llamadas a GPT-4 (caro)
- âš¡ **Baja latencia**: Respuestas instantÃ¡neas para usuarios
- ğŸ“ˆ **Alto volumen**: Miles de usuarios con preguntas similares
- ğŸ“ **Onboarding**: Nuevos usuarios hacen las mismas preguntas bÃ¡sicas

**Ejemplos**:
- ğŸ“ **Chatbot Educativo**: "Â¿QuÃ© es X?" preguntado por cientos de estudiantes
- ğŸ› ï¸ **Soporte TÃ©cnico**: "Â¿CÃ³mo reseteo mi contraseÃ±a?" (top FAQ)
- ğŸ“„ **DocumentaciÃ³n**: "Â¿CÃ³mo instalo X?" (pregunta muy comÃºn)
- ğŸ¢ **Onboarding Corporativo**: PolÃ­ticas, beneficios (preguntas repetidas)

#### âŒ **No uses CAG cuando**:
- ğŸ”„ **InformaciÃ³n en tiempo real**: Precios de bolsa, clima actual
- ğŸ¨ **Respuestas personalizadas**: Cada usuario necesita respuesta Ãºnica
- ğŸ—£ï¸ **Conversaciones Ãºnicas**: Pocas o ninguna pregunta repetida
- ğŸš« **Privacidad sensible**: No debes guardar preguntas de usuarios

### ğŸš€ MÃ©tricas de OptimizaciÃ³n

#### **Ahorro de Tiempo**
```
Sin CachÃ©:     17s por consulta
Con CachÃ©:     0.01s por consulta
ReducciÃ³n:     1700x mÃ¡s rÃ¡pido
```

#### **Ahorro de Costos (GPT-4o-mini)**
```
Sin CachÃ©:     $0.005 por consulta
Con CachÃ©:     $0.000 (solo embedding: ~$0.00001)
ReducciÃ³n:     500x mÃ¡s barato
```

#### **Tasa de Acierto de CachÃ©**
```
Cache Hit Rate = (Cache Hits / Total Queries) Ã— 100%

Ejemplo con 1000 consultas:
- 400 cache hits
- 600 cache misses
Cache Hit Rate = 40%

Ahorro: 400 Ã— ($0.005 - $0.00001) = $1.996
```

### âš–ï¸ Umbral de Distancia

El umbral controla quÃ© tan similares deben ser las preguntas:

| Umbral | Comportamiento | Ejemplo |
|--------|---------------|---------|
| **0.0** | Solo idÃ©nticos | "Â¿QuÃ© es X?" = "Â¿QuÃ© es X?" |
| **0.3** | Muy estricto | "Â¿QuÃ© es LangGraph?" â‰ˆ "Â¿QuÃ© es Langgraph?" |
| **0.45** | **Recomendado** | "Â¿QuÃ© es LangGraph?" â‰ˆ "Explica LangGraph" |
| **0.7** | Permisivo | "Â¿QuÃ© es LangGraph?" â‰ˆ "Â¿CÃ³mo funciona LangGraph?" |
| **>1.0** | Demasiado permisivo | Preguntas diferentes se tratan como iguales âŒ |

**RecomendaciÃ³n**: Empieza con `0.45` y ajusta segÃºn tus mÃ©tricas:
- â¬‡ï¸ Si muchos **falsos positivos** (respuestas incorrectas) â†’ Reduce umbral
- â¬†ï¸ Si pocos **cache hits** (bajo aprovechamiento) â†’ Aumenta umbral

### ğŸ› ï¸ Limpieza y Mantenimiento

#### **LÃ­mite de TamaÃ±o del CachÃ©**
```python
MAX_CACHE_SIZE = 10000

if qa_cache.index.ntotal > MAX_CACHE_SIZE:
    # Eliminar entradas mÃ¡s antiguas
    # Estrategia: FIFO, LRU, o por timestamp
```

#### **EvaluaciÃ³n de Calidad**
```python
# Monitorear mÃ©tricas
metrics = {
    "cache_hits": 0,
    "cache_misses": 0,
    "false_positives": 0  # Cache hit con respuesta incorrecta
}

# Logging
if state["cache_hit"]:
    logger.info(f"Cache hit: {query} â†’ {answer}")
```

### ğŸ“ˆ ComparaciÃ³n: Sin CachÃ© vs CAG

| CaracterÃ­stica | Sin CachÃ© | CAG |
|----------------|-----------|-----|
| **Latencia promedio** | ~15s | ~2s (85% hits) |
| **Costo por 1000 queries** | $5.00 | $1.50 (70% savings) |
| **Escalabilidad** | âš ï¸ Crece linealmente | âœ… Mejor con mÃ¡s trÃ¡fico |
| **Complejidad** | âœ… Simple | âš ï¸ Media |
| **Almacenamiento** | âœ… Ninguno | âš ï¸ Crece con el tiempo |
| **Mejor para** | Prototipado | ProducciÃ³n con trÃ¡fico alto |

---

## ğŸ“ Estructura del Proyecto

```
RAGBootcamp/
â”‚
â”œâ”€â”€ 000_DataIngestParsing/          # MÃ³dulo 1: Ingesta de Datos
â”‚   â”œâ”€â”€ 1-dataingestion.ipynb       # Carga de archivos de texto
â”‚   â”œâ”€â”€ 2-dataparsingpdf.ipynb      # Parseo de PDFs (PyPDF, PyMuPDF)
â”‚   â”œâ”€â”€ 3-dataparsingdoc.ipynb      # Parseo de documentos Word
â”‚   â”œâ”€â”€ 4-csvexcelparsing.ipynb     # Datos estructurados (CSV/Excel)
â”‚   â”œâ”€â”€ 5-jsonparsing.ipynb         # Manejo de archivos JSON
â”‚   â”œâ”€â”€ 6-databaseparsing.ipynb     # ConexiÃ³n a bases de datos
â”‚   â””â”€â”€ data/                        # Datasets de ejemplo
â”‚       â”œâ”€â”€ text_files/
â”‚       â”œâ”€â”€ pdf/
â”‚       â”œâ”€â”€ word_files/
â”‚       â”œâ”€â”€ structured_files/
â”‚       â”œâ”€â”€ json_files/
â”‚       â””â”€â”€ databases/
â”‚
â”œâ”€â”€ 001_VectorEmbeddingAndDatabases/ # MÃ³dulo 2: Embeddings
â”‚   â”œâ”€â”€ 1-embedding.ipynb            # Conceptos de embeddings
â”‚   â””â”€â”€ 2-openaiembeddings.ipynb     # Embeddings de OpenAI
â”‚
â”œâ”€â”€ 002_VectorStores/                # MÃ³dulo 3: Bases de Datos Vectoriales
â”‚   â”œâ”€â”€ 1-chromadb.ipynb             # ChromaDB
â”‚   â”œâ”€â”€ 2-faiss.ipynb                # FAISS
â”‚   â”œâ”€â”€ 3-Othervectorstores.ipynb    # InMemoryVectorStore
â”‚   â”œâ”€â”€ 4-Datastaxdb.ipynb           # AstraDB
â”‚   â”œâ”€â”€ 5-PineconeVectorDB.ipynb     # Pinecone
â”‚   â”œâ”€â”€ chroma_db/                   # Almacenamiento ChromaDB
â”‚   â””â”€â”€ faiss_index/                 # Ãndices FAISS guardados
â”‚
â”œâ”€â”€ 003_AdvancedChuking/             # MÃ³dulo 4: TÃ©cnicas Avanzadas de Chunking
â”‚   â””â”€â”€ 1-semantichunking.ipynb      # Semantic Chunking
â”‚
â”œâ”€â”€ 004_HybridSearchStrategies/      # MÃ³dulo 5: Estrategias de BÃºsqueda HÃ­brida
â”‚   â”œâ”€â”€ 1-densesparse.ipynb          # BÃºsqueda HÃ­brida (Dense + Sparse)
â”‚   â”œâ”€â”€ 2-reranking.ipynb            # Reranking con LLM
â”‚   â””â”€â”€ 3-mmr.ipynb                  # MMR (Maximal Marginal Relevance)
â”‚
â”œâ”€â”€ 005_QueryEnhancement/            # MÃ³dulo 6: Mejora de Consultas
â”‚   â”œâ”€â”€ 1-queryexpansion.ipynb       # Query Expansion (ExpansiÃ³n de Consultas)
â”‚   â”œâ”€â”€ 2-querydecomposition.ipynb   # Query Decomposition (DescomposiciÃ³n)
â”‚   â””â”€â”€ 3-HyDE.ipynb                 # HyDE (Hypothetical Document Embeddings)
â”‚
â”œâ”€â”€ 006_MultimodalRag/               # MÃ³dulo 7: RAG Multimodal
â”‚   â””â”€â”€ 1-multimodalopenai.ipynb     # RAG con CLIP + GPT-4 Vision
â”‚
â”œâ”€â”€ 007_LanggraphBasics/             # MÃ³dulo 8: Fundamentos de LangGraph
â”‚   â”œâ”€â”€ 2-chatbot.ipynb              # Chatbot simple con LangGraph
â”‚   â”œâ”€â”€ 3-DataclassStateSchema.ipynb # Esquemas de estado (TypedDict vs DataClass)
â”‚   â”œâ”€â”€ 4-pydantic.ipynb             # ValidaciÃ³n de datos con Pydantic
â”‚   â”œâ”€â”€ 5-ChainsLangGraph.ipynb      # Cadenas y herramientas en LangGraph
â”‚   â””â”€â”€ 6-chatbotswithmultipletools.ipynb # Chatbot con mÃºltiples herramientas
â”‚
â”œâ”€â”€ 008_AgentsArchitecture/          # MÃ³dulo 9: Arquitectura de Agentes
â”‚   â”œâ”€â”€ 1-ReActAgents.ipynb          # Agentes ReAct con herramientas y memoria
â”‚   â””â”€â”€ 2-streaming.ipynb            # Streaming de respuestas en tiempo real
â”‚
â”œâ”€â”€ 009_Debugging/                   # MÃ³dulo 10: Debugging y LangGraph Studio
â”‚   â”œâ”€â”€ langgraph.json               # ConfiguraciÃ³n de LangGraph Studio
â”‚   â””â”€â”€ openai_agent.py              # Agente con herramientas para debugging
â”‚
â”œâ”€â”€ 010_AgenticRag/                  # MÃ³dulo 11: RAG AgÃ©ntico
â”‚   â”œâ”€â”€ 1-agenticrag.ipynb           # IntroducciÃ³n a RAG AgÃ©ntico con LangGraph
â”‚   â”œâ”€â”€ 2-ReAct.ipynb                # Framework ReAct: Reasoning + Acting
â”‚   â”œâ”€â”€ 3-AgenticRAG.ipynb           # Sistema RAG AgÃ©ntico completo con evaluaciÃ³n
â”‚   â”œâ”€â”€ internal_docs.txt            # Documentos internos de ejemplo
â”‚   â””â”€â”€ research_notes.txt           # Notas de investigaciÃ³n de ejemplo
â”‚
â”œâ”€â”€ 011_AutonomousRag/               # MÃ³dulo 12: RAG AutÃ³nomo
â”‚   â”œâ”€â”€ 1-COTRag.ipynb               # Chain-of-Thought RAG (Razonamiento paso a paso)
â”‚   â”œâ”€â”€ 2-Selfreflection.ipynb       # Auto-reflexiÃ³n: El LLM evalÃºa su propia respuesta
â”‚   â”œâ”€â”€ 3-QueryPlanningdecomposition.ipynb  # DescomposiciÃ³n de consultas complejas
â”‚   â”œâ”€â”€ 4-Iterativeretrieval.ipynb   # RecuperaciÃ³n iterativa con refinamiento de consultas
â”‚   â”œâ”€â”€ 5-answersynthesis.ipynb      # SÃ­ntesis de respuestas desde mÃºltiples fuentes
â”‚   â”œâ”€â”€ internal_docs.txt            # Documentos internos de ejemplo
â”‚   â””â”€â”€ research_notes.txt           # Notas de investigaciÃ³n de ejemplo
â”‚
â”œâ”€â”€ 012_MultiAgentsRags/             # MÃ³dulo 13: Sistemas RAG Multi-Agente
â”‚   â””â”€â”€ 1-multiagent.ipynb           # Sistema multi-agente: Colaborativo, Supervisor y JerÃ¡rquico
â”‚
â”œâ”€â”€ 013_CorrectiveRag/               # MÃ³dulo 14: RAG Correctivo (Corrective RAG - CRAG)
â”‚   â””â”€â”€ 1-CorrectiveRAG.ipynb        # Sistema CRAG con evaluaciÃ³n de relevancia y bÃºsqueda web
â”‚
â”œâ”€â”€ 014_AdaptiveRag/                 # MÃ³dulo 15: RAG Adaptativo (Adaptive RAG)
â”‚   â””â”€â”€ 1-AdaptiveRAG.ipynb          # Sistema completo con enrutamiento, evaluadores y auto-correcciÃ³n
â”‚
â”œâ”€â”€ 015_RagMemory/                   # MÃ³dulo 16: RAG con Memoria Persistente
â”‚   â””â”€â”€ 1-ragmemory.ipynb            # Sistema RAG con memoria conversacional usando LangGraph y MemorySaver
â”‚
â”œâ”€â”€ 016_CacheRagLangGraph/           # MÃ³dulo 17: Cache-Augmented Generation (CAG)
â”‚   â””â”€â”€ 1-cache_augment_generation.ipynb  # Sistema de cachÃ© semÃ¡ntico con FAISS para optimizar respuestas
â”‚
â”œâ”€â”€ .env                             # Variables de entorno (API keys)
â”œâ”€â”€ .gitignore                       # Archivos ignorados por Git
â”œâ”€â”€ .python-version                  # VersiÃ³n de Python (3.12)
â”œâ”€â”€ requirements.txt                 # Dependencias del proyecto
â”œâ”€â”€ CLAUDE.md                        # GuÃ­a para Claude Code
â””â”€â”€ README.md                        # Este archivo
```

## ğŸ“ GuÃ­a de Uso

### Orden de Aprendizaje Recomendado

1. **MÃ³dulo 000: Ingesta de Datos** (4-6 horas)
   - Comienza con `1-dataingestion.ipynb`
   - Aprende diferentes tÃ©cnicas de carga de documentos
   - Explora estrategias de text splitting

2. **MÃ³dulo 001: Embeddings** (2-3 horas)
   - Comprende quÃ© son los embeddings
   - Compara diferentes modelos de embeddings
   - Aprende similitud del coseno

3. **MÃ³dulo 002: Vector Stores** (4-5 horas)
   - Experimenta con cada base de datos vectorial
   - Compara rendimiento y caracterÃ­sticas
   - Implementa bÃºsquedas avanzadas con filtros

4. **MÃ³dulo 003: Advanced Chunking** (2-3 horas)
   - Aprende tÃ©cnicas avanzadas de chunking semÃ¡ntico
   - Optimiza la divisiÃ³n de documentos para mejor recuperaciÃ³n
   - Implementa semantic chunking con embeddings

5. **MÃ³dulo 004: Estrategias de BÃºsqueda HÃ­brida** (3-4 horas)
   - Combina recuperaciÃ³n densa y dispersa (Dense + Sparse)
   - Implementa reranking con LLM para mayor precisiÃ³n
   - Usa MMR para diversidad en resultados
   - Aprende cuÃ¡ndo aplicar cada estrategia

6. **MÃ³dulo 005: Mejora de Consultas** (3-4 horas)
   - Implementa Query Expansion para enriquecer consultas vagas
   - Descompone consultas complejas con Query Decomposition
   - Usa HyDE para resolver vocabulary mismatch
   - Aprende a combinar tÃ©cnicas de mejora de consultas
   - Optimiza el balance entre precisiÃ³n, recall y latencia

7. **MÃ³dulo 006: RAG Multimodal** (4-5 horas)
   - Comprende cÃ³mo procesar documentos con texto e imÃ¡genes
   - Implementa embeddings unificados con CLIP
   - Integra GPT-4 Vision para anÃ¡lisis multimodal
   - Construye pipelines que recuperan y procesan imÃ¡genes y texto
   - Aprende optimizaciones para reducir costos y latencia

8. **MÃ³dulo 007: LangGraph Basics** (4-6 horas)
   - Comprende los conceptos fundamentales de grafos de estado
   - Aprende las diferencias entre TypedDict, DataClass y Pydantic
   - Construye chatbots simples y avanzados con LangGraph
   - Integra mÃºltiples herramientas (Arxiv, Wikipedia, Tavily)
   - Implementa enrutamiento condicional y gestiÃ³n de estado
   - Domina el uso de reductores (reducers) y mensajes
   - Crea agentes que toman decisiones inteligentes

9. **MÃ³dulo 008: Arquitectura de Agentes** (4-5 horas)
   - Comprende la arquitectura ReAct (Reason + Act)
   - Implementa agentes que razonan y actÃºan iterativamente
   - Integra herramientas de bÃºsqueda (Arxiv, Wikipedia, Tavily)
   - Crea funciones personalizadas como herramientas
   - Implementa memoria conversacional con MemorySaver
   - Aprende tÃ©cnicas de streaming de respuestas
   - Domina stream_mode="updates" vs "values"
   - Implementa streaming token por token con astream_events()

10. **MÃ³dulo 009: Debugging con LangGraph Studio** (2-3 horas)
   - Configura LangGraph Studio para desarrollo local
   - Visualiza grafos de agentes en tiempo real
   - Depura agentes paso a paso con inspecciÃ³n de estado
   - Experimenta con diferentes configuraciones de grafos
   - Aprende el flujo de desarrollo con hot reload
   - Implementa grafos bÃ¡sicos y con herramientas
   - Domina el archivo langgraph.json para configuraciÃ³n

11. **MÃ³dulo 010: RAG AgÃ©ntico** (5-6 horas)
   - Comprende quÃ© es RAG AgÃ©ntico y cÃ³mo difiere del RAG tradicional
   - Construye un sistema RAG bÃ¡sico con LangGraph y StateGraph
   - Implementa el framework ReAct (Reasoning + Acting)
   - Crea agentes que razonan, recuperan, evalÃºan y reformulan consultas
   - Implementa mÃºltiples fuentes de conocimiento (vectorstores separados)
   - Aprende a evaluar relevancia de documentos con LLM
   - Implementa nodos condicionales para flujo inteligente
   - Domina la reformulaciÃ³n automÃ¡tica de consultas (query rewriting)
   - Crea herramientas personalizadas de recuperaciÃ³n con metadatos
   - Construye grafos complejos con ciclos y toma de decisiones

12. **MÃ³dulo 011: RAG AutÃ³nomo** (6-8 horas)
   - **Chain-of-Thought (CoT)**: Descompone preguntas complejas en sub-pasos razonados
   - **Auto-ReflexiÃ³n**: El LLM evalÃºa su propia respuesta y mejora iterativamente
   - **PlanificaciÃ³n de Consultas**: Divide consultas en sub-preguntas para bÃºsqueda precisa
   - **RecuperaciÃ³n Iterativa**: Ciclo de retroalimentaciÃ³n con refinamiento de consultas
   - **SÃ­ntesis Multi-Fuente**: Combina informaciÃ³n de 4 fuentes (docs, YouTube, Wikipedia, ArXiv)
   - Implementa sistemas que razonan antes de recuperar informaciÃ³n
   - Domina ciclos de retroalimentaciÃ³n con lÃ­mites de iteraciones
   - Construye flujos transparentes y explicables paso a paso
   - Aprende cuÃ¡ndo usar cada patrÃ³n segÃºn el caso de uso
   - Integra mÃºltiples fuentes de conocimiento en respuestas coherentes

13. **MÃ³dulo 012: Sistemas RAG Multi-Agente** (8-10 horas)
   - **Arquitectura Colaborativa**: Dos agentes (researcher + blog writer) colaboran
   - **PatrÃ³n Supervisor**: Supervisor central coordina agentes especializados (research + math)
   - **JerarquÃ­a de Equipos**: 3 niveles de supervisiÃ³n con equipos completos
   - Construye equipos de investigaciÃ³n (search + web scraper)
   - Implementa equipos de escritura (note taker + doc writer + chart generator)
   - Usa `langgraph_supervisor` para coordinaciÃ³n automÃ¡tica
   - Crea herramientas personalizadas con `@tool` decorator
   - Implementa scraping web con BeautifulSoup
   - Ejecuta cÃ³digo Python dinÃ¡mico con REPL para visualizaciones
   - Domina el patrÃ³n Command para navegaciÃ³n y actualizaciÃ³n de estado
   - Aprende cuÃ¡ndo usar cada arquitectura segÃºn complejidad del proyecto
   - Gestiona mÃºltiples agentes especializados con roles claros

14. **MÃ³dulo 013: RAG Correctivo (Corrective RAG - CRAG)** (4-5 horas)
   - **EvaluaciÃ³n AutomÃ¡tica de Relevancia**: Usa un LLM para calificar documentos recuperados
   - **Reescritura Inteligente de Consultas**: Optimiza preguntas cuando los documentos no son relevantes
   - **BÃºsqueda Web Adaptativa**: Integra Tavily para buscar informaciÃ³n externa cuando es necesario
   - **Flujo de DecisiÃ³n con LangGraph**: Implementa lÃ³gica condicional basada en relevancia
   - Construye un evaluador binario (yes/no) con salida estructurada usando Pydantic
   - Aprende a usar `with_structured_output()` para respuestas determinÃ­sticas
   - Implementa nodos de decisiÃ³n con `add_conditional_edges()`
   - Crea flujos adaptativos que se auto-corrigen cuando fallan
   - Integra mÃºltiples fuentes: vectorstore local + bÃºsqueda web
   - Reduce alucinaciones filtrando documentos irrelevantes
   - Domina el patrÃ³n CRAG para sistemas RAG mÃ¡s robustos y confiables
   - Aprende cuÃ¡ndo usar CRAG vs RAG tradicional segÃºn el caso de uso

15. **MÃ³dulo 014: RAG Adaptativo (Adaptive RAG)** (5-6 horas)
   - **Enrutamiento Inteligente**: Router LLM decide automÃ¡ticamente entre vectorstore local y bÃºsqueda web
   - **EvaluaciÃ³n de Relevancia**: Califica cada documento recuperado del vectorstore
   - **DetecciÃ³n de Alucinaciones**: Verifica que la respuesta estÃ© fundamentada en los documentos
   - **ValidaciÃ³n de Respuestas**: Confirma que la respuesta realmente conteste la pregunta
   - **Auto-CorrecciÃ³n con Ciclos**: Reescribe consultas y reintenta hasta obtener una respuesta de calidad
   - Implementa un router con `Literal["vectorstore", "web_search"]` y Pydantic
   - Crea tres evaluadores independientes (relevancia, alucinaciones, respuestas)
   - Construye flujos con mÃºltiples decisiones condicionales anidadas
   - Implementa ciclos de retroalimentaciÃ³n (transform_query â†’ retrieve â†’ generate)
   - Aprende a usar `add_conditional_edges()` con 3 opciones de salida
   - Maneja el flujo complejo: enrutamiento â†’ recuperaciÃ³n â†’ evaluaciÃ³n â†’ generaciÃ³n â†’ validaciÃ³n
   - Domina el patrÃ³n mÃ¡s completo y robusto de RAG para producciÃ³n
   - Aprende cuÃ¡ndo usar Adaptive RAG vs CRAG vs RAG tradicional

16. **MÃ³dulo 015: RAG con Memoria Persistente** (4-5 horas)
   - **Memoria Conversacional con LangGraph**: Mantiene el historial completo de interacciones entre el usuario y el agente
   - **MemorySaver y Checkpoints**: Implementa persistencia de estado usando checkpointers
   - **Thread IDs para Sesiones**: Gestiona mÃºltiples conversaciones independientes con identificadores Ãºnicos
   - **Arquitectura de Grafo Personalizada**: Construye flujos con query_or_respond, retrieve, generate y cache_write
   - **Agente ReAct con Memoria**: Implementa el patrÃ³n ReAct usando `create_react_agent` con memoria persistente
   - **Contexto entre Mensajes**: El sistema recuerda preguntas previas y puede responder seguimientos
   - Implementa herramientas personalizadas de recuperaciÃ³n con el decorador `@tool`
   - Construye nodos de generaciÃ³n que inyectan contexto recuperado en prompts
   - Aprende a usar `MessagesState` para mantener el historial de mensajes
   - Domina el flujo: normalizar â†’ buscar cachÃ© â†’ recuperar â†’ generar â†’ escribir cachÃ©
   - Implementa respuestas que hacen referencia a conversaciones previas
   - Aprende cuÃ¡ndo usar memoria persistente vs conversaciones stateless

17. **MÃ³dulo 016: Cache-Augmented Generation (CAG)** (5-6 horas)
   - **CachÃ© Simple con Diccionarios**: Implementa cachÃ© bÃ¡sico basado en coincidencia exacta de strings
   - **CachÃ© SemÃ¡ntico con FAISS**: Usa embeddings vectoriales para detectar preguntas similares, no solo idÃ©nticas
   - **Similitud L2 y Umbrales**: Configura distancia L2 y thresholds para determinar aciertos de cachÃ©
   - **OptimizaciÃ³n de Costos**: Reduce llamadas a API reutilizando respuestas previas (0.00s vs 12-17s)
   - **TTL (Time To Live)**: Implementa expiraciÃ³n automÃ¡tica de entradas de cachÃ©
   - **Arquitectura Avanzada con LangGraph**: Construye flujo con normalizaciÃ³n, bÃºsqueda semÃ¡ntica y escritura de cachÃ©
   - Implementa funciones de nodo: `normalize_query`, `semantic_cache_lookup`, `retrieve`, `generate`, `cache_write`
   - Usa HuggingFace embeddings (sentence-transformers) para vectorizaciÃ³n
   - Crea Ã­ndices FAISS con `IndexFlatL2` para bÃºsqueda eficiente
   - Aprende edges condicionales: si cache_hit â†’ responder, si no â†’ RAG completo
   - Integra metadatos (respuesta, timestamp) en documentos cacheados
   - Domina el patrÃ³n CAG para sistemas RAG optimizados en producciÃ³n
   - Comprende cuÃ¡ndo usar cachÃ© exacto vs cachÃ© semÃ¡ntico segÃºn el caso de uso

### Ejecutar un Notebook

```bash
# OpciÃ³n 1: Jupyter Notebook
jupyter notebook 000_DataIngestParsing/1-dataingestion.ipynb

# OpciÃ³n 2: Jupyter Lab (interfaz moderna)
jupyter lab

# OpciÃ³n 3: VS Code con extensiÃ³n Jupyter
code .
```

### Ejemplos de CÃ³digo RÃ¡pido

#### Ejemplo 1: Cargar y Dividir Documento

```python
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Cargar documento
loader = TextLoader("documento.txt")
documents = loader.load()

# Dividir en chunks
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
chunks = text_splitter.split_documents(documents)
```

#### Ejemplo 2: Crear Vector Store y Buscar

```python
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

# Crear vector store
vector_store = Chroma.from_documents(
    documents=chunks,
    embedding=OpenAIEmbeddings(),
    persist_directory="./mi_db"
)

# BÃºsqueda semÃ¡ntica
results = vector_store.similarity_search(
    "Â¿CuÃ¡l es la capital de Francia?",
    k=3
)

for doc in results:
    print(doc.page_content)
```

#### Ejemplo 3: Retriever con Filtros

```python
# Convertir a retriever
retriever = vector_store.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={
        "k": 5,
        "score_threshold": 0.7,
        "filter": {"source": "wikipedia"}
    }
)

# Usar en cadena RAG
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI

qa_chain = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(model="gpt-4"),
    retriever=retriever
)

respuesta = qa_chain.invoke("Â¿QuÃ© dice el documento sobre...?")
```

## ğŸ› ï¸ TecnologÃ­as Utilizadas

### Core Framework
- **LangChain 0.3**: Framework principal para RAG
- **LangChain Community**: Extensiones y utilidades
- **LangChain OpenAI**: IntegraciÃ³n con modelos OpenAI

### Embeddings
- **OpenAI Embeddings**: text-embedding-3-small, text-embedding-ada-002
- **HuggingFace Transformers**: Modelos open-source
- **Sentence Transformers**: all-MiniLM-L6-v2, all-mpnet-base-v2

### Vector Databases
- **ChromaDB**: Base de datos vectorial local
- **FAISS**: Biblioteca de Facebook para bÃºsqueda de similitud
- **Pinecone**: Servicio cloud de bases de datos vectoriales
- **AstraDB**: Base de datos vectorial serverless de DataStax

### Document Processing
- **PyPDF / PyMuPDF**: Procesamiento de PDFs
- **python-docx / docx2txt**: Documentos de Word
- **Pandas / OpenPyXL**: Datos estructurados
- **Unstructured**: Parser avanzado de documentos

### Utilities
- **tiktoken**: TokenizaciÃ³n para OpenAI
- **python-dotenv**: GestiÃ³n de variables de entorno
- **Jupyter**: Notebooks interactivos

## ğŸ“š Recursos Adicionales

### DocumentaciÃ³n Oficial
- [LangChain Documentation](https://python.langchain.com/)
- [ChromaDB Docs](https://docs.trychroma.com/)
- [FAISS Wiki](https://github.com/facebookresearch/faiss/wiki)
- [Pinecone Docs](https://docs.pinecone.io/)
- [AstraDB Docs](https://docs.datastax.com/en/astra-serverless/docs/)

### Tutoriales Relacionados
- [LangChain RAG Tutorial](https://python.langchain.com/docs/use_cases/question_answering/)
- [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)
- [Vector Database Comparison](https://www.datastax.com/guides/what-is-a-vector-database)

## ğŸ¤ ContribuciÃ³n

Â¡Las contribuciones son bienvenidas! Si deseas mejorar este proyecto:

1. Fork el repositorio
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

### Ãreas de ContribuciÃ³n
- ğŸ“ Mejorar documentaciÃ³n
- ğŸ› Reportar y corregir bugs
- âœ¨ Agregar nuevos ejemplos
- ğŸŒ Traducciones a otros idiomas
- ğŸ§ª Agregar tests
- ğŸ“Š Benchmarks de rendimiento

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver el archivo `LICENSE` para mÃ¡s detalles.

## ğŸ‘¤ Autor

**RAG Bootcamp Team**

## ğŸ™ Agradecimientos

- A la comunidad de LangChain por el framework excepcional
- A OpenAI por los modelos de embeddings
- A todos los contribuidores de las bibliotecas open-source utilizadas
- A la comunidad de desarrolladores de IA que comparten conocimiento

---

<div align="center">

**Â¿Te gustÃ³ este proyecto? Dale una â­ en GitHub!**

[â¬† Volver arriba](#-rag-bootcamp)

</div>
