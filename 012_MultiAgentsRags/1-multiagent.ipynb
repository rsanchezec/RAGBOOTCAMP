{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ü§ñ ¬øQu√© son los Sistemas RAG Multi-Agente?\n",
        "Un Sistema RAG Multi-Agente divide el pipeline RAG en m√∫ltiples agentes especializados ‚Äî cada uno responsable de un rol espec√≠fico ‚Äî y les permite colaborar en una sola consulta o tarea.\n",
        "\n",
        "#### 1. üìã Sistema de Red Multi-Agente RAG con LangGraph\n",
        "Descripci√≥n del Proyecto\n",
        "\n",
        "Un sistema de Generaci√≥n Aumentada por Recuperaci√≥n (RAG) amigable para principiantes que utiliza una arquitectura multi-agente para responder preguntas inteligentemente desde tus documentos. Construido con LangGraph v0.3 para orquestaci√≥n de flujos de trabajo y OpenAI para comprensi√≥n del lenguaje.\n",
        "\n",
        "Qu√© Hace\n",
        "\n",
        "Transforma tus documentos (PDFs, archivos de texto) en una base de conocimiento buscable que puede responder preguntas inteligentemente usando IA. Simplemente carga documentos y haz preguntas en lenguaje natural - el sistema encuentra informaci√≥n relevante y genera respuestas completas.\n",
        "\n",
        "Caracter√≠sticas Clave\n",
        "\n",
        "- üìö Soporte Multi-Formato: Maneja documentos PDF y de texto\n",
        "- ü§ñ Arquitectura de 3 Agentes: Agentes especializados para procesamiento de documentos, recuperaci√≥n y generaci√≥n de respuestas\n",
        "- üîç B√∫squeda Inteligente: B√∫squeda sem√°ntica basada en vectores encuentra informaci√≥n relevante\n",
        "- üí¨ Preguntas y Respuestas en Lenguaje Natural: Haz preguntas en espa√±ol simple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importaci√≥n del m√≥dulo os para interactuar con variables de entorno del sistema\n",
        "import os\n",
        "\n",
        "# Importaci√≥n de load_dotenv para cargar variables de entorno desde archivo .env\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Carga las variables de entorno desde el archivo .env en el directorio actual\n",
        "# Esto permite mantener las API keys seguras fuera del c√≥digo\n",
        "load_dotenv()\n",
        "\n",
        "# Importaci√≥n de init_chat_model para inicializar modelos de chat de diferentes proveedores\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "# Configuraci√≥n de la API key de Tavily (motor de b√∫squeda web para IA)\n",
        "# Tavily proporciona resultados de b√∫squeda optimizados para LLMs\n",
        "os.environ[\"TAVILY_API_KEY\"]=os.getenv(\"TAVILY_API_KEY\")\n",
        "\n",
        "# Configuraci√≥n de la API key de OpenAI para acceder a modelos GPT\n",
        "# Esta key permite autenticarse con los servicios de OpenAI\n",
        "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inicializaci√≥n del modelo de lenguaje usando GPT-4o-mini de OpenAI\n",
        "# GPT-4o-mini es una versi√≥n m√°s econ√≥mica y r√°pida de GPT-4\n",
        "# Ideal para sistemas multi-agente donde se hacen m√∫ltiples llamadas al LLM\n",
        "llm=init_chat_model(\"openai:gpt-4o-mini\")\n",
        "\n",
        "# Muestra el objeto LLM configurado con sus par√°metros\n",
        "llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importaci√≥n de Annotated para agregar metadatos a tipos en type hints\n",
        "from typing import Annotated\n",
        "\n",
        "# Importaci√≥n de TavilySearch para b√∫squedas web optimizadas para IA\n",
        "from langchain_tavily import TavilySearch\n",
        "\n",
        "# Importaci√≥n de tool decorator para convertir funciones en herramientas de LangChain\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "# Importaci√≥n de WikipediaQueryRun para ejecutar b√∫squedas en Wikipedia\n",
        "from langchain.tools import WikipediaQueryRun\n",
        "\n",
        "# Importaci√≥n de WikipediaAPIWrapper, wrapper que encapsula la API de Wikipedia\n",
        "from langchain.utilities import WikipediaAPIWrapper\n",
        "\n",
        "# Importaci√≥n de TextLoader para cargar archivos de texto plano\n",
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "# Importaci√≥n de FAISS para b√∫squeda vectorial eficiente\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Importaci√≥n de OpenAIEmbeddings para generar vectores de embeddings\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Importaci√≥n de RecursiveCharacterTextSplitter para dividir textos en chunks\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creaci√≥n de una instancia de TavilySearch configurada para b√∫squeda web\n",
        "# max_results=5: limita los resultados a los 5 m√°s relevantes\n",
        "# Tavily es un motor de b√∫squeda dise√±ado espec√≠ficamente para aplicaciones de IA\n",
        "# Proporciona resultados limpios y optimizados para consumo por LLMs\n",
        "tavily_tool=TavilySearch(max_results=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Funci√≥n gen√©rica para crear una herramienta de recuperaci√≥n desde texto\n",
        "\n",
        "# Importaci√≥n de Tool para crear herramientas personalizadas de LangChain\n",
        "from langchain.agents import Tool\n",
        "\n",
        "def make_retriever_tool_from_text(file, name, desc):\n",
        "    \"\"\"\n",
        "    Crea una herramienta de recuperaci√≥n personalizada desde un archivo de texto.\n",
        "    \n",
        "    Par√°metros:\n",
        "    - file: ruta del archivo de texto a cargar\n",
        "    - name: nombre de la herramienta\n",
        "    - desc: descripci√≥n de lo que hace la herramienta\n",
        "    \n",
        "    Retorna: objeto Tool configurado con recuperaci√≥n vectorial\n",
        "    \"\"\"\n",
        "    \n",
        "    # Carga del archivo de texto usando TextLoader con codificaci√≥n UTF-8\n",
        "    # Esto asegura la correcta lectura de caracteres especiales y acentos\n",
        "    docs=TextLoader(file, encoding=\"utf-8\").load()\n",
        "    \n",
        "    # Divisi√≥n de los documentos en chunks de 500 caracteres\n",
        "    # chunk_overlap=50: superposici√≥n de 50 caracteres entre chunks para mantener contexto\n",
        "    chunks = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50).split_documents(docs)\n",
        "    \n",
        "    # Creaci√≥n de un vector store FAISS con los chunks\n",
        "    # FAISS indexa los embeddings para b√∫squedas r√°pidas de similitud\n",
        "    vs = FAISS.from_documents(chunks, OpenAIEmbeddings())\n",
        "    \n",
        "    # Conversi√≥n del vector store en retriever para b√∫squedas\n",
        "    retriever = vs.as_retriever()\n",
        "\n",
        "    # Definici√≥n de la funci√≥n interna que ejecutar√° la herramienta\n",
        "    def tool_func(query: str) -> str:\n",
        "        \"\"\"\n",
        "        Funci√≥n que realiza la b√∫squeda vectorial.\n",
        "        \n",
        "        Par√°metros:\n",
        "        - query: consulta de b√∫squeda\n",
        "        \n",
        "        Retorna: contenido de los documentos encontrados concatenados\n",
        "        \"\"\"\n",
        "        # Imprime mensaje indicando qu√© herramienta se est√° usando\n",
        "        print(f\"üìö Usando herramienta: {name}\")\n",
        "        \n",
        "        # Invoca el retriever para buscar documentos relevantes\n",
        "        results = retriever.invoke(query)\n",
        "        \n",
        "        # Concatena el contenido de todos los documentos encontrados\n",
        "        # Separados por doble salto de l√≠nea para mejor legibilidad\n",
        "        return \"\\n\\n\".join(doc.page_content for doc in results)\n",
        "    \n",
        "    # Retorna un objeto Tool de LangChain con la funci√≥n configurada\n",
        "    return Tool(name=name, description=desc, func=tool_func)\n",
        "\n",
        "\n",
        "# Creaci√≥n de una herramienta de recuperaci√≥n para notas de investigaci√≥n internas\n",
        "# Esta herramienta busca en el archivo internal_docs.txt\n",
        "internal_tool_1=make_retriever_tool_from_text(\n",
        "    \"internal_docs.txt\",  # Archivo de documentos internos\n",
        "    \"InternalResearchNotes\",  # Nombre de la herramienta\n",
        "    \"Buscar en notas de investigaci√≥n internas para resultados experimentales\"  # Descripci√≥n\n",
        ")\n",
        "\n",
        "# Muestra la herramienta creada\n",
        "internal_tool_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importaciones necesarias para crear agentes multi-agente\n",
        "\n",
        "# BaseMessage: clase base para todos los mensajes en LangChain\n",
        "# HumanMessage: representa mensajes del usuario/humano\n",
        "from langchain_core.messages import BaseMessage, HumanMessage\n",
        "\n",
        "# create_react_agent: crea agentes con patr√≥n ReAct (Reason + Act)\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "# MessagesState: estado que contiene historial de mensajes\n",
        "# END: marcador especial que indica el final del grafo\n",
        "from langgraph.graph import MessagesState, END\n",
        "\n",
        "# Command: clase para enviar comandos de actualizaci√≥n y navegaci√≥n en el grafo\n",
        "from langgraph.types import Command\n",
        "\n",
        "def get_next_node(last_message: BaseMessage, goto: str):\n",
        "    \"\"\"\n",
        "    Determina el siguiente nodo a ejecutar en el grafo multi-agente.\n",
        "    \n",
        "    Esta funci√≥n implementa la l√≥gica de terminaci√≥n:\n",
        "    - Si el mensaje contiene \"FINAL ANSWER\", termina el flujo\n",
        "    - Si no, contin√∫a al siguiente nodo especificado\n",
        "    \n",
        "    Par√°metros:\n",
        "    - last_message: √∫ltimo mensaje generado por un agente\n",
        "    - goto: nombre del siguiente nodo a ejecutar\n",
        "    \n",
        "    Retorna: END si el trabajo est√° completo, o el nombre del siguiente nodo\n",
        "    \"\"\"\n",
        "    # Verifica si el mensaje contiene \"FINAL ANSWER\" (en may√∫sculas)\n",
        "    # Este es el indicador de que alg√∫n agente ha completado el trabajo\n",
        "    if \"FINAL ANSWER\" in last_message.content:\n",
        "        # Cualquier agente decidi√≥ que el trabajo est√° completo\n",
        "        return END\n",
        "    \n",
        "    # Si no hay \"FINAL ANSWER\", contin√∫a al siguiente nodo\n",
        "    return goto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_system_prompt(suffix: str) -> str:\n",
        "    \"\"\"\n",
        "    Crea un prompt del sistema para agentes colaborativos.\n",
        "    \n",
        "    Este prompt establece las reglas de colaboraci√≥n entre agentes:\n",
        "    - Cada agente es parte de un equipo\n",
        "    - Pueden usar herramientas espec√≠ficas\n",
        "    - Deben progresar hacia la respuesta final\n",
        "    - Pueden pasar el trabajo a otro agente\n",
        "    - Deben indicar cuando tienen la respuesta final\n",
        "    \n",
        "    Par√°metros:\n",
        "    - suffix: instrucciones adicionales espec√≠ficas para cada agente\n",
        "    \n",
        "    Retorna: string con el prompt completo del sistema\n",
        "    \"\"\"\n",
        "    return (\n",
        "        # Establece el rol del agente: asistente colaborativo\n",
        "        \"Eres un asistente de IA √∫til, colaborando con otros asistentes.\"\n",
        "        \n",
        "        # Instrucci√≥n para usar herramientas disponibles\n",
        "        \" Usa las herramientas proporcionadas para progresar hacia responder la pregunta.\"\n",
        "        \n",
        "        # Permite que el agente no complete toda la tarea solo\n",
        "        \" Si no puedes responder completamente, est√° bien, otro asistente con diferentes herramientas\"\n",
        "        \" ayudar√° donde lo dejaste. Ejecuta lo que puedas para hacer progreso.\"\n",
        "        \n",
        "        # Instrucci√≥n crucial: c√≥mo indicar que el trabajo est√° completo\n",
        "        \" Si t√∫ o cualquiera de los otros asistentes tienen la respuesta final o el entregable,\"\n",
        "        \" prefija tu respuesta con RESPUESTA FINAL para que el equipo sepa que debe detenerse.\"\n",
        "        \n",
        "        # Agrega instrucciones espec√≠ficas del agente\n",
        "        f\"\\n{suffix}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Agente de Investigaci√≥n y nodo\n",
        "\n",
        "# Creaci√≥n del agente de investigaci√≥n usando el patr√≥n ReAct\n",
        "# Este agente es especializado en hacer investigaci√≥n usando herramientas espec√≠ficas\n",
        "research_agent=create_react_agent(\n",
        "    llm,  # Modelo de lenguaje GPT-4o-mini configurado anteriormente\n",
        "    \n",
        "    # Lista de herramientas disponibles para este agente:\n",
        "    tools=[\n",
        "        internal_tool_1,  # Herramienta para buscar en documentos internos\n",
        "        tavily_tool  # Herramienta para b√∫squeda web con Tavily\n",
        "    ],\n",
        "    \n",
        "    # Prompt del sistema que define el rol y comportamiento del agente\n",
        "    prompt=make_system_prompt(\n",
        "        # Instrucciones espec√≠ficas para el agente de investigaci√≥n\n",
        "        \"Solo puedes hacer investigaci√≥n. Usa la herramienta con la que est√°s vinculado, puedes usar ambas.\"\n",
        "        \" Est√°s trabajando con un colega escritor de contenido.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Muestra el agente de investigaci√≥n configurado\n",
        "research_agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Nodo de Investigaci√≥n\n",
        "\n",
        "# Importaci√≥n de Literal para type hints con valores literales espec√≠ficos\n",
        "from typing import Literal\n",
        "\n",
        "def research_node(state: MessagesState) -> Command[Literal[\"blog_generator\", END]]:\n",
        "    \"\"\"\n",
        "    Nodo que ejecuta el agente de investigaci√≥n y decide el siguiente paso.\n",
        "    \n",
        "    Flujo:\n",
        "    1. Invoca el agente de investigaci√≥n con el estado actual\n",
        "    2. Determina si el trabajo est√° completo o debe pasar a otro agente\n",
        "    3. Convierte el mensaje del agente a HumanMessage para compatibilidad\n",
        "    4. Retorna comando con actualizaci√≥n de estado y siguiente nodo\n",
        "    \n",
        "    Par√°metros:\n",
        "    - state: estado actual con historial de mensajes\n",
        "    \n",
        "    Retorna: Command dirigiendo al generador de blogs o END\n",
        "    \"\"\"\n",
        "    \n",
        "    # Invoca el agente de investigaci√≥n con el estado actual\n",
        "    # El agente procesa los mensajes y usa sus herramientas (internal_tool_1, tavily_tool)\n",
        "    result = research_agent.invoke(state)\n",
        "    \n",
        "    # Determina el siguiente nodo bas√°ndose en el √∫ltimo mensaje\n",
        "    # Si contiene \"FINAL ANSWER\" ‚Üí END, si no ‚Üí \"blog_generator\"\n",
        "    goto = get_next_node(result[\"messages\"][-1], \"blog_generator\")\n",
        "\n",
        "    # Envuelve el √∫ltimo mensaje en un HumanMessage\n",
        "    # Esto es necesario porque no todos los proveedores de LLM permiten\n",
        "    # mensajes de IA en la √∫ltima posici√≥n de la lista de mensajes de entrada\n",
        "    result[\"messages\"][-1] = HumanMessage(\n",
        "        content=result[\"messages\"][-1].content,  # Contenido del mensaje del agente\n",
        "        name=\"researcher\"  # Identifica que este mensaje viene del investigador\n",
        "    )\n",
        "    \n",
        "    # Retorna un Command que:\n",
        "    # 1. Actualiza el estado con el historial de mensajes del agente de investigaci√≥n\n",
        "    # 2. Navega al siguiente nodo (blog_generator o END)\n",
        "    return Command(\n",
        "        update={\n",
        "            # Comparte el historial de mensajes interno del agente de investigaci√≥n\n",
        "            # con otros agentes para mantener el contexto\n",
        "            \"messages\": result[\"messages\"],\n",
        "        },\n",
        "        goto=goto,  # Siguiente nodo a ejecutar\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Agente de Escritura de Blog\n",
        "\n",
        "# Creaci√≥n del agente de escritura de blogs usando el patr√≥n ReAct\n",
        "# Este agente se especializa en escribir contenido detallado bas√°ndose en investigaci√≥n\n",
        "blog_agent=create_react_agent(\n",
        "    llm,  # Modelo de lenguaje GPT-4o-mini\n",
        "    \n",
        "    # Lista de herramientas: vac√≠a porque este agente no usa herramientas externas\n",
        "    # Su funci√≥n es procesar informaci√≥n y escribir, no buscar\n",
        "    tools=[],\n",
        "    \n",
        "    # Prompt del sistema que define el rol espec√≠fico del escritor\n",
        "    prompt=make_system_prompt(\n",
        "        # Instrucciones espec√≠ficas: solo escribir, no investigar\n",
        "        \"Solo puedes escribir un blog detallado. Est√°s trabajando con un colega investigador.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "def blog_node(state: MessagesState) -> Command[Literal[\"researcher\", END]]:\n",
        "    \"\"\"\n",
        "    Nodo que ejecuta el agente de escritura de blogs.\n",
        "    \n",
        "    Flujo:\n",
        "    1. Invoca el agente de blog con el estado (que incluye la investigaci√≥n)\n",
        "    2. Determina si el blog est√° completo o necesita m√°s investigaci√≥n\n",
        "    3. Convierte el mensaje a HumanMessage para compatibilidad\n",
        "    4. Retorna comando con actualizaci√≥n y siguiente paso\n",
        "    \n",
        "    Par√°metros:\n",
        "    - state: estado actual con historial de mensajes (incluye investigaci√≥n)\n",
        "    \n",
        "    Retorna: Command dirigiendo al investigador o END\n",
        "    \"\"\"\n",
        "    \n",
        "    # Invoca el agente de blog con el estado actual\n",
        "    # El agente procesa la informaci√≥n de investigaci√≥n y genera contenido escrito\n",
        "    result = blog_agent.invoke(state)\n",
        "    \n",
        "    # Determina el siguiente nodo\n",
        "    # Si el blog est√° completo (contiene \"FINAL ANSWER\") ‚Üí END\n",
        "    # Si necesita m√°s informaci√≥n ‚Üí vuelve al \"researcher\"\n",
        "    goto = get_next_node(result[\"messages\"][-1], \"researcher\")\n",
        "    \n",
        "    # Envuelve el √∫ltimo mensaje en un HumanMessage\n",
        "    # Marca el mensaje como proveniente del \"blog_generator\"\n",
        "    result[\"messages\"][-1] = HumanMessage(\n",
        "        content=result[\"messages\"][-1].content,  # Contenido del blog generado\n",
        "        name=\"blog_generator\"  # Identifica el autor del mensaje\n",
        "    )\n",
        "    \n",
        "    # Retorna Command con actualizaci√≥n de estado y navegaci√≥n\n",
        "    return Command(\n",
        "        update={\n",
        "            # Comparte el historial de mensajes interno del agente de blog\n",
        "            # con otros agentes\n",
        "            \"messages\": result[\"messages\"],\n",
        "        },\n",
        "        goto=goto,  # Siguiente nodo (researcher o END)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importaciones para construcci√≥n del grafo\n",
        "from langgraph.graph import StateGraph, START\n",
        "\n",
        "# Creaci√≥n del grafo de estado con MessagesState\n",
        "# MessagesState mantiene el historial de mensajes entre agentes\n",
        "workflow = StateGraph(MessagesState)\n",
        "\n",
        "# Agregado de nodos al grafo\n",
        "# Cada nodo representa un agente especializado\n",
        "\n",
        "# Nodo del investigador: busca informaci√≥n usando herramientas\n",
        "workflow.add_node(\"researcher\", research_node)\n",
        "\n",
        "# Nodo del generador de blogs: escribe contenido bas√°ndose en la investigaci√≥n\n",
        "workflow.add_node(\"blog_generator\", blog_node)\n",
        "\n",
        "# Define el punto de entrada del grafo\n",
        "# El flujo comienza con el investigador\n",
        "workflow.add_edge(START, \"researcher\")\n",
        "\n",
        "# Compilaci√≥n del grafo para hacerlo ejecutable\n",
        "# .compile() valida la estructura y crea el grafo final\n",
        "graph = workflow.compile()\n",
        "\n",
        "# Muestra el grafo compilado (en Jupyter puede renderizar una visualizaci√≥n)\n",
        "graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Invocaci√≥n del grafo multi-agente con una consulta\n",
        "# Este es el punto de entrada para usar el sistema\n",
        "\n",
        "response=graph.invoke({\n",
        "    # El mensaje inicial que dispara todo el flujo de trabajo\n",
        "    # \"messages\" es el campo requerido por MessagesState\n",
        "    \"messages\": \"Escribe un blog detallado sobre variantes de transformers en despliegues de producci√≥n\"\n",
        "})\n",
        "\n",
        "# Flujo de ejecuci√≥n esperado:\n",
        "# 1. START ‚Üí researcher (nodo de investigaci√≥n)\n",
        "# 2. researcher busca informaci√≥n usando internal_tool_1 y tavily_tool\n",
        "# 3. researcher ‚Üí blog_generator (pasa la investigaci√≥n al escritor)\n",
        "# 4. blog_generator escribe el blog usando la informaci√≥n recopilada\n",
        "# 5. Si est√° completo: a√±ade \"FINAL ANSWER\" y termina (END)\n",
        "# 6. Si necesita m√°s info: vuelve a researcher\n",
        "\n",
        "# La salida mostrar√°: \"üìö Usando herramienta: InternalResearchNotes\"\n",
        "# cuando el investigador use la herramienta de documentos internos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extrae y muestra el contenido del √∫ltimo mensaje\n",
        "# Este es el blog final generado por el sistema multi-agente\n",
        "# [-1] accede al √∫ltimo elemento de la lista de mensajes\n",
        "# .content obtiene el texto del mensaje\n",
        "response[\"messages\"][-1].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Supervisor Multi-Agente con RAG\n",
        "El Supervisor es una arquitectura multi-agente donde agentes especializados son coordinados por un agente supervisor central. El agente supervisor controla todo el flujo de comunicaci√≥n y delegaci√≥n de tareas, tomando decisiones sobre qu√© agente invocar bas√°ndose en el contexto actual y los requisitos de la tarea.\n",
        "\n",
        "En este tutorial, construir√°s un sistema supervisor con dos agentes ‚Äî un experto en investigaci√≥n y matem√°ticas. Al final del tutorial podr√°s:\n",
        "\n",
        "1. Construir agentes especializados de investigaci√≥n y matem√°ticas\n",
        "2. Construir un supervisor para orquestarlos con langgraph-supervisor pre-construido\n",
        "3. Construir un supervisor personalizado\n",
        "4. Implementar delegaci√≥n avanzada de tareas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Muestra la herramienta de recuperaci√≥n interna creada anteriormente\n",
        "# Esta herramienta ser√° usada por el agente de investigaci√≥n en el sistema supervisor\n",
        "internal_tool_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importaci√≥n de TavilySearch para b√∫squedas web\n",
        "from langchain_tavily import TavilySearch\n",
        "\n",
        "# Creaci√≥n de herramienta de b√∫squeda web con l√≠mite de 3 resultados\n",
        "# Limitamos a 3 para reducir el contexto y el costo de tokens\n",
        "web_search = TavilySearch(max_results=3)\n",
        "\n",
        "# Muestra la herramienta de b√∫squeda web configurada\n",
        "web_search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importaci√≥n de create_react_agent para crear agentes con patr√≥n ReAct\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "# Creaci√≥n del agente de investigaci√≥n para el sistema supervisor\n",
        "# Este agente tiene capacidades m√°s restringidas que en el ejemplo anterior\n",
        "research_agent=create_react_agent(\n",
        "    model=llm,  # Modelo GPT-4o-mini\n",
        "    \n",
        "    # Herramientas disponibles: b√∫squeda web y documentos internos\n",
        "    tools=[web_search, internal_tool_1],\n",
        "    \n",
        "    # Prompt m√°s espec√≠fico para este contexto\n",
        "    prompt=(\n",
        "        \"Eres un agente de investigaci√≥n.\\n\\n\"\n",
        "        \"INSTRUCCIONES:\\n\"\n",
        "        \n",
        "        # Restricci√≥n clara: SOLO investigaci√≥n, NO matem√°ticas\n",
        "        \"- Asiste SOLO con tareas relacionadas con investigaci√≥n, NO hagas ninguna matem√°tica\\n\"\n",
        "        \n",
        "        # Despu√©s de terminar, debe reportar directamente al supervisor\n",
        "        \"- Despu√©s de que termines con tus tareas, responde al supervisor directamente\\n\"\n",
        "        \n",
        "        # Solo debe proporcionar resultados, sin texto adicional\n",
        "        \"- Responde SOLO con los resultados de tu trabajo, NO incluyas NING√öN otro texto.\"\n",
        "    ),\n",
        "    \n",
        "    # Nombre del agente para identificaci√≥n en mensajes\n",
        "    name=\"research_agent\"\n",
        ")\n",
        "\n",
        "# Muestra el agente de investigaci√≥n configurado\n",
        "research_agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definici√≥n de funciones matem√°ticas b√°sicas como herramientas\n",
        "# Estas funciones ser√°n usadas por el agente de matem√°ticas\n",
        "\n",
        "def add(a: float, b: float):\n",
        "    \"\"\"\n",
        "    Suma dos n√∫meros.\n",
        "    \n",
        "    Par√°metros:\n",
        "    - a: primer n√∫mero (float)\n",
        "    - b: segundo n√∫mero (float)\n",
        "    \n",
        "    Retorna: la suma de a y b\n",
        "    \"\"\"\n",
        "    return a + b\n",
        "\n",
        "\n",
        "def multiply(a: float, b: float):\n",
        "    \"\"\"\n",
        "    Multiplica dos n√∫meros.\n",
        "    \n",
        "    Par√°metros:\n",
        "    - a: primer n√∫mero (float)\n",
        "    - b: segundo n√∫mero (float)\n",
        "    \n",
        "    Retorna: el producto de a y b\n",
        "    \"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "def divide(a: float, b: float):\n",
        "    \"\"\"\n",
        "    Divide dos n√∫meros.\n",
        "    \n",
        "    Par√°metros:\n",
        "    - a: numerador (float)\n",
        "    - b: denominador (float)\n",
        "    \n",
        "    Retorna: el cociente de a dividido por b\n",
        "    \n",
        "    Nota: No maneja divisi√≥n por cero expl√≠citamente\n",
        "    \"\"\"\n",
        "    return a / b\n",
        "\n",
        "\n",
        "# Creaci√≥n del agente de matem√°ticas\n",
        "# Este agente se especializa en operaciones matem√°ticas\n",
        "math_agent=create_react_agent(\n",
        "    model=llm,  # Modelo GPT-4o-mini\n",
        "    \n",
        "    # Herramientas matem√°ticas disponibles\n",
        "    tools=[add, multiply, divide],\n",
        "    \n",
        "    # Prompt con instrucciones espec√≠ficas para el agente matem√°tico\n",
        "    prompt=(\n",
        "        \"Eres un agente de matem√°ticas.\\n\\n\"\n",
        "        \"INSTRUCCIONES:\\n\"\n",
        "        \n",
        "        # Restricci√≥n: SOLO matem√°ticas\n",
        "        \"- Asiste SOLO con tareas relacionadas con matem√°ticas\\n\"\n",
        "        \n",
        "        # Reportar al supervisor cuando termine\n",
        "        \"- Despu√©s de que termines con tus tareas, responde al supervisor directamente\\n\"\n",
        "        \n",
        "        # Solo resultados, sin texto adicional\n",
        "        \"- Responde SOLO con los resultados de tu trabajo, NO incluyas NING√öN otro texto.\"\n",
        "    ),\n",
        "    \n",
        "    # Nombre del agente\n",
        "    name=\"math_agent\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Crear agente supervisor\n",
        "\n",
        "# Importaci√≥n de create_supervisor desde langgraph_supervisor\n",
        "# Esta es una funci√≥n pre-construida que facilita la creaci√≥n de supervisores\n",
        "from langgraph_supervisor import create_supervisor\n",
        "\n",
        "# Creaci√≥n del supervisor que coordina a los agentes\n",
        "supervisor=create_supervisor(\n",
        "    model=llm,  # Modelo LLM para el supervisor (GPT-4o-mini)\n",
        "    \n",
        "    # Lista de agentes que el supervisor puede delegar trabajo\n",
        "    agents=[research_agent, math_agent],\n",
        "    \n",
        "    # Prompt que define el comportamiento del supervisor\n",
        "    prompt=(\n",
        "        \"Eres un supervisor gestionando dos agentes:\\n\"\n",
        "        \n",
        "        # Describe cada agente y su especialidad\n",
        "        \"- un agente de investigaci√≥n. Asigna tareas relacionadas con investigaci√≥n a este agente\\n\"\n",
        "        \"- un agente de matem√°ticas. Asigna tareas relacionadas con matem√°ticas a este agente\\n\"\n",
        "        \n",
        "        # Regla importante: un agente a la vez (no en paralelo)\n",
        "        \"Asigna trabajo a un agente a la vez, no llames agentes en paralelo.\\n\"\n",
        "        \n",
        "        # El supervisor solo coordina, no hace el trabajo\n",
        "        \"No hagas ning√∫n trabajo t√∫ mismo.\"\n",
        "    ),\n",
        "    \n",
        "    # Agrega mensajes de \"handoff\" (transferencia) de vuelta al supervisor\n",
        "    # Esto permite que los agentes \"reporten\" al supervisor cuando terminen\n",
        "    add_handoff_back_messages=True,\n",
        "    \n",
        "    # Modo de salida: devuelve el historial completo de mensajes\n",
        "    # Alternativas: \"final_answer\" (solo la respuesta final)\n",
        "    output_mode=\"full_history\"\n",
        ").compile()  # Compila el supervisor en un grafo ejecutable\n",
        "\n",
        "# Muestra el supervisor compilado\n",
        "supervisor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Invocaci√≥n del sistema supervisor con una consulta compleja\n",
        "# Esta consulta requiere AMBOS agentes: investigaci√≥n Y matem√°ticas\n",
        "\n",
        "response=supervisor.invoke({\n",
        "    # Mensaje que combina dos tareas diferentes:\n",
        "    # 1. Investigaci√≥n: listar variantes de transformers (research_agent)\n",
        "    # 2. Matem√°ticas: calcular 5 + 10 (math_agent)\n",
        "    \"messages\": \"lista todas las variantes de transformers en despliegues de producci√≥n del retriever y luego dime cu√°nto es 5 m√°s 10\"\n",
        "})\n",
        "\n",
        "# Flujo de ejecuci√≥n esperado:\n",
        "# 1. Supervisor analiza la consulta\n",
        "# 2. Supervisor delega a research_agent (primera tarea)\n",
        "# 3. research_agent usa internal_tool_1 para buscar transformers\n",
        "# 4. research_agent reporta resultados al supervisor\n",
        "# 5. Supervisor delega a math_agent (segunda tarea)\n",
        "# 6. math_agent usa la funci√≥n add(5, 10)\n",
        "# 7. math_agent reporta resultado al supervisor\n",
        "# 8. Supervisor consolida ambas respuestas\n",
        "\n",
        "# La salida mostrar√°: \"üìö Usando herramienta: InternalResearchNotes\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Muestra la respuesta completa del supervisor\n",
        "# Incluye todo el historial de mensajes entre supervisor y agentes\n",
        "# √ötil para ver el flujo completo de delegaci√≥n y respuestas\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extrae y muestra el contenido del √∫ltimo mensaje\n",
        "# Este es el resultado final consolidado por el supervisor\n",
        "# Deber√≠a contener:\n",
        "# 1. Lista de variantes de transformers (de research_agent)\n",
        "# 2. El resultado de 5 + 10 = 15 (de math_agent)\n",
        "response[\"messages\"][-1].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Equipos Jer√°rquicos de Agentes con RAG\n",
        "En nuestro ejemplo anterior (Supervisor de Agentes), introdujimos el concepto de un solo nodo supervisor para enrutar trabajo entre diferentes nodos trabajadores.\n",
        "\n",
        "Pero ¬øqu√© pasa si el trabajo para un solo trabajador se vuelve demasiado complejo? ¬øQu√© pasa si el n√∫mero de trabajadores se vuelve demasiado grande?\n",
        "\n",
        "Para algunas aplicaciones, el sistema puede ser m√°s efectivo si el trabajo se distribuye jer√°rquicamente.\n",
        "\n",
        "Puedes hacer esto componiendo diferentes subgrafos y creando un supervisor de nivel superior, junto con supervisores de nivel medio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importaciones para el sistema jer√°rquico avanzado\n",
        "from typing import Annotated, List\n",
        "\n",
        "# WebBaseLoader: carga contenido desde URLs web\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "# TavilySearch: b√∫squeda web optimizada para IA\n",
        "from langchain_tavily import TavilySearch\n",
        "\n",
        "# tool decorator: convierte funciones Python en herramientas de LangChain\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "# Creaci√≥n de herramienta Tavily con l√≠mite de 5 resultados\n",
        "tavily_tool = TavilySearch(max_results=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definici√≥n de herramienta personalizada para scraping web\n",
        "@tool\n",
        "def scrape_webpages(urls: List[str]) -> str:\n",
        "    \"\"\"\n",
        "    Usa requests y bs4 para scrapear las p√°ginas web proporcionadas para obtener informaci√≥n detallada.\n",
        "    \n",
        "    Esta herramienta permite al agente acceder a contenido espec√≠fico de URLs,\n",
        "    complementando la b√∫squeda web general con informaci√≥n detallada de p√°ginas espec√≠ficas.\n",
        "    \n",
        "    Par√°metros:\n",
        "    - urls: lista de URLs a scrapear\n",
        "    \n",
        "    Retorna: contenido de todas las p√°ginas concatenado y formateado\n",
        "    \"\"\"\n",
        "    \n",
        "    # Carga el contenido de las URLs usando WebBaseLoader\n",
        "    # WebBaseLoader maneja autom√°ticamente requests y BeautifulSoup\n",
        "    loader = WebBaseLoader(urls)\n",
        "    \n",
        "    # .load() descarga y procesa todas las p√°ginas\n",
        "    docs = loader.load()\n",
        "    \n",
        "    # Formatea cada documento con su t√≠tulo y contenido\n",
        "    # Usa tags XML-like para estructura clara\n",
        "    return \"\\n\\n\".join(\n",
        "        [\n",
        "            # Para cada documento, crea un bloque estructurado\n",
        "            f'<Document name=\"{doc.metadata.get(\"title\", \"\")}\">' +\n",
        "            f'\\n{doc.page_content}\\n</Document>'\n",
        "            for doc in docs\n",
        "        ]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importaciones para herramientas de escritura de documentos\n",
        "from pathlib import Path\n",
        "from tempfile import TemporaryDirectory\n",
        "from typing import Dict, Optional\n",
        "\n",
        "# PythonREPL: permite ejecutar c√≥digo Python din√°micamente\n",
        "from langchain_experimental.utilities import PythonREPL\n",
        "\n",
        "# TypedDict: para type hints de diccionarios con estructura espec√≠fica\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "# Creaci√≥n de directorio temporal para guardar documentos\n",
        "# Esto mantiene los archivos aislados y se limpian autom√°ticamente\n",
        "_TEMP_DIRECTORY = TemporaryDirectory()\n",
        "WORKING_DIRECTORY = Path(_TEMP_DIRECTORY.name)\n",
        "\n",
        "# Herramienta 1: Crear esquemas/outlines\n",
        "@tool\n",
        "def create_outline(\n",
        "    points: Annotated[List[str], \"Lista de puntos principales o secciones.\"],\n",
        "    file_name: Annotated[str, \"Ruta del archivo para guardar el esquema.\"],\n",
        ") -> Annotated[str, \"Ruta del archivo de esquema guardado.\"]:\n",
        "    \"\"\"\n",
        "    Crea y guarda un esquema/outline de documento.\n",
        "    \n",
        "    √ötil para el agente note_taker que crea estructuras de documentos.\n",
        "    \"\"\"\n",
        "    # Abre archivo en modo escritura\n",
        "    with (WORKING_DIRECTORY / file_name).open(\"w\") as file:\n",
        "        # Enumera cada punto con formato \"1. punto\"\n",
        "        for i, point in enumerate(points):\n",
        "            file.write(f\"{i + 1}. {point}\\n\")\n",
        "    return f\"Esquema guardado en {file_name}\"\n",
        "\n",
        "# Herramienta 2: Escribir documentos completos\n",
        "@tool\n",
        "def write_document(\n",
        "    content: Annotated[str, \"Contenido de texto a escribir en el documento.\"],\n",
        "    file_name: Annotated[str, \"Ruta del archivo para guardar el documento.\"],\n",
        ") -> Annotated[str, \"Ruta del archivo de documento guardado.\"]:\n",
        "    \"\"\"\n",
        "    Crea y guarda un documento de texto.\n",
        "    \n",
        "    √ötil para el agente doc_writer que escribe contenido completo.\n",
        "    \"\"\"\n",
        "    # Escribe el contenido completo en el archivo\n",
        "    with (WORKING_DIRECTORY / file_name).open(\"w\") as file:\n",
        "        file.write(content)\n",
        "    return f\"Documento guardado en {file_name}\"\n",
        "\n",
        "# Herramienta 3: Editar documentos existentes\n",
        "@tool\n",
        "def edit_document(\n",
        "    file_name: Annotated[str, \"Ruta del documento a editar.\"],\n",
        "    inserts: Annotated[\n",
        "        Dict[int, str],\n",
        "        \"Diccionario donde la clave es el n√∫mero de l√≠nea (indexado desde 1) y el valor es el texto a insertar en esa l√≠nea.\",\n",
        "    ],\n",
        ") -> Annotated[str, \"Ruta del archivo de documento editado.\"]:\n",
        "    \"\"\"\n",
        "    Edita un documento insertando texto en n√∫meros de l√≠nea espec√≠ficos.\n",
        "    \n",
        "    √ötil para hacer correcciones o agregar contenido sin reescribir todo.\n",
        "    \"\"\"\n",
        "    # Lee todas las l√≠neas del archivo existente\n",
        "    with (WORKING_DIRECTORY / file_name).open(\"r\") as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    # Ordena las inserciones por n√∫mero de l√≠nea\n",
        "    sorted_inserts = sorted(inserts.items())\n",
        "\n",
        "    # Procesa cada inserci√≥n\n",
        "    for line_number, text in sorted_inserts:\n",
        "        # Verifica que el n√∫mero de l√≠nea sea v√°lido (1-indexado)\n",
        "        if 1 <= line_number <= len(lines) + 1:\n",
        "            # Inserta el texto en la posici√≥n especificada (convertido a 0-indexed)\n",
        "            lines.insert(line_number - 1, text + \"\\n\")\n",
        "        else:\n",
        "            # Retorna error si el n√∫mero de l√≠nea est√° fuera de rango\n",
        "            return f\"Error: N√∫mero de l√≠nea {line_number} est√° fuera de rango.\"\n",
        "\n",
        "    # Escribe las l√≠neas modificadas de vuelta al archivo\n",
        "    with (WORKING_DIRECTORY / file_name).open(\"w\") as file:\n",
        "        file.writelines(lines)\n",
        "\n",
        "    return f\"Documento editado y guardado en {file_name}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Muestra el directorio de trabajo temporal donde se guardan los documentos\n",
        "# Este es un directorio temporal que se limpia autom√°ticamente\n",
        "WORKING_DIRECTORY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advertencia: Este c√≥digo ejecuta c√≥digo Python localmente,\n",
        "# lo cual puede ser inseguro cuando no est√° en sandbox\n",
        "\n",
        "# Creaci√≥n de un int√©rprete Python REPL\n",
        "# REPL = Read-Eval-Print Loop (Leer-Evaluar-Imprimir-Repetir)\n",
        "repl = PythonREPL()\n",
        "\n",
        "\n",
        "@tool\n",
        "def python_repl_tool(\n",
        "    code: Annotated[str, \"El c√≥digo Python a ejecutar para generar tu gr√°fico.\"],\n",
        "):\n",
        "    \"\"\"\n",
        "    Usa esto para ejecutar c√≥digo Python. Si quieres ver la salida de un valor,\n",
        "    debes imprimirlo con `print(...)`. Esto es visible para el usuario.\n",
        "    \n",
        "    Esta herramienta permite al agente chart_generator crear visualizaciones\n",
        "    y realizar an√°lisis de datos din√°micamente.\n",
        "    \n",
        "    Par√°metros:\n",
        "    - code: c√≥digo Python a ejecutar (como string)\n",
        "    \n",
        "    Retorna: resultado de la ejecuci√≥n o mensaje de error\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Intenta ejecutar el c√≥digo usando el REPL\n",
        "        result = repl.run(code)\n",
        "    except BaseException as e:\n",
        "        # Captura cualquier error durante la ejecuci√≥n\n",
        "        return f\"Fallo al ejecutar. Error: {repr(e)}\"\n",
        "    \n",
        "    # Retorna un mensaje formateado con el c√≥digo y su salida\n",
        "    return f\"Ejecutado exitosamente:\\n```python\\n{code}\\n```\\nStdout: {result}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importaciones para el sistema jer√°rquico\n",
        "from typing import List, Optional, Literal\n",
        "from langchain_core.language_models.chat_models import BaseChatModel\n",
        "\n",
        "from langgraph.graph import StateGraph, MessagesState, START, END\n",
        "from langgraph.types import Command\n",
        "from langchain_core.messages import HumanMessage, trim_messages\n",
        "\n",
        "\n",
        "# Definici√≥n del estado personalizado que extiende MessagesState\n",
        "class State(MessagesState):\n",
        "    \"\"\"\n",
        "    Estado extendido que incluye campo 'next' para tracking.\n",
        "    \n",
        "    Campos:\n",
        "    - messages: heredado de MessagesState (historial de mensajes)\n",
        "    - next: indica el siguiente nodo a ejecutar\n",
        "    \"\"\"\n",
        "    next: str  # Nombre del siguiente nodo/agente a ejecutar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_supervisor_node(llm: BaseChatModel, members: list[str]) -> str:\n",
        "    \"\"\"\n",
        "    Crea un nodo supervisor que enruta a trabajadores espec√≠ficos.\n",
        "    \n",
        "    Esta funci√≥n es el n√∫cleo del patr√≥n jer√°rquico. Crea un supervisor\n",
        "    que puede delegar trabajo a m√∫ltiples agentes trabajadores.\n",
        "    \n",
        "    Par√°metros:\n",
        "    - llm: modelo de lenguaje para el supervisor\n",
        "    - members: lista de nombres de agentes trabajadores disponibles\n",
        "    \n",
        "    Retorna: funci√≥n del nodo supervisor configurada\n",
        "    \"\"\"\n",
        "    \n",
        "    # Opciones incluye \"FINISH\" m√°s todos los trabajadores\n",
        "    # \"FINISH\" indica que el trabajo est√° completo\n",
        "    options = [\"FINISH\"] + members\n",
        "    \n",
        "    # Prompt del sistema para el supervisor\n",
        "    system_prompt = (\n",
        "        \"Eres un supervisor encargado de gestionar una conversaci√≥n entre los\"\n",
        "        f\" siguientes trabajadores: {members}. Dada la siguiente solicitud del usuario,\"\n",
        "        \" responde con el trabajador que debe actuar a continuaci√≥n. Cada trabajador realizar√° una\"\n",
        "        \" tarea y responder√° con sus resultados y estado. Cuando termines,\"\n",
        "        \" responde con FINISH.\"\n",
        "    )\n",
        "\n",
        "    # Definici√≥n de TypedDict para validaci√≥n de respuesta estructurada\n",
        "    class Router(TypedDict):\n",
        "        \"\"\"Trabajador al que enrutar a continuaci√≥n. Si no se necesitan trabajadores, enrutar a FINISH.\"\"\"\n",
        "        next: Literal[*options]  # next debe ser uno de los valores en options\n",
        "\n",
        "    def supervisor_node(state: State) -> Command[Literal[*members, \"__end__\"]]:\n",
        "        \"\"\"\n",
        "        Un enrutador basado en LLM.\n",
        "        \n",
        "        Este nodo:\n",
        "        1. Recibe el estado actual\n",
        "        2. Usa el LLM para decidir el siguiente trabajador\n",
        "        3. Retorna comando para navegar al siguiente nodo\n",
        "        \"\"\"\n",
        "        # Construye lista de mensajes con prompt del sistema + historial\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "        ] + state[\"messages\"]\n",
        "        \n",
        "        # Usa LLM con salida estructurada (Router TypedDict)\n",
        "        # Esto fuerza al LLM a responder con formato: {\"next\": \"nombre_trabajador\"}\n",
        "        response = llm.with_structured_output(Router).invoke(messages)\n",
        "        \n",
        "        # Extrae la decisi√≥n del siguiente nodo\n",
        "        goto = response[\"next\"]\n",
        "        \n",
        "        # Si el supervisor dice \"FINISH\", termina el grafo\n",
        "        if goto == \"FINISH\":\n",
        "            goto = END\n",
        "\n",
        "        # Retorna comando con navegaci√≥n al siguiente nodo\n",
        "        return Command(goto=goto, update={\"next\": goto})\n",
        "\n",
        "    # Retorna la funci√≥n del nodo supervisor configurada\n",
        "    return supervisor_node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Muestra la herramienta de recuperaci√≥n interna\n",
        "# Esta ser√° usada por el equipo de investigaci√≥n\n",
        "internal_tool_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importaciones necesarias\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "\n",
        "\n",
        "# === EQUIPO DE INVESTIGACI√ìN ===\n",
        "\n",
        "# Agente 1: search_agent (b√∫squeda general)\n",
        "# Este agente busca informaci√≥n usando Tavily y documentos internos\n",
        "search_agent = create_react_agent(llm, tools=[tavily_tool, internal_tool_1])\n",
        "search_agent\n",
        "\n",
        "def search_node(state: State) -> Command[Literal[\"supervisor\"]]:\n",
        "    \"\"\"\n",
        "    Nodo que ejecuta el agente de b√∫squeda.\n",
        "    Siempre reporta de vuelta al supervisor del equipo de investigaci√≥n.\n",
        "    \"\"\"\n",
        "    # Invoca el agente de b√∫squeda con el estado actual\n",
        "    result = search_agent.invoke(state)\n",
        "    \n",
        "    # Retorna comando con actualizaci√≥n y reporte al supervisor\n",
        "    return Command(\n",
        "        update={\n",
        "            # Envuelve el mensaje en HumanMessage con nombre \"search\"\n",
        "            \"messages\": [\n",
        "                HumanMessage(content=result[\"messages\"][-1].content, name=\"search\")\n",
        "            ]\n",
        "        },\n",
        "        # Queremos que nuestros trabajadores SIEMPRE \"reporten\" al supervisor cuando terminen\n",
        "        goto=\"supervisor\",\n",
        "    )\n",
        "\n",
        "### Subagente: web_scraper\n",
        "# Agente 2: web_scraper_agent (scraping de p√°ginas espec√≠ficas)\n",
        "# Este agente scrapea contenido detallado de URLs espec√≠ficas\n",
        "web_scraper_agent = create_react_agent(llm, tools=[scrape_webpages])\n",
        "\n",
        "\n",
        "def web_scraper_node(state: State) -> Command[Literal[\"supervisor\"]]:\n",
        "    \"\"\"\n",
        "    Nodo que ejecuta el agente de scraping web.\n",
        "    Siempre reporta de vuelta al supervisor del equipo de investigaci√≥n.\n",
        "    \"\"\"\n",
        "    # Invoca el agente de scraping\n",
        "    result = web_scraper_agent.invoke(state)\n",
        "    \n",
        "    # Retorna comando con actualizaci√≥n y reporte al supervisor\n",
        "    return Command(\n",
        "        update={\n",
        "            # Envuelve el mensaje con nombre \"web_scraper\"\n",
        "            \"messages\": [\n",
        "                HumanMessage(content=result[\"messages\"][-1].content, name=\"web_scraper\")\n",
        "            ]\n",
        "        },\n",
        "        # Reporta al supervisor cuando termine\n",
        "        goto=\"supervisor\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creaci√≥n del supervisor del equipo de investigaci√≥n\n",
        "# Este supervisor coordina a search_agent y web_scraper_agent\n",
        "research_supervisor_node = make_supervisor_node(llm, [\"search\", \"web_scraper\"])\n",
        "\n",
        "# Muestra la funci√≥n del supervisor de investigaci√≥n\n",
        "research_supervisor_node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === CONSTRUCCI√ìN DEL GRAFO DEL EQUIPO DE INVESTIGACI√ìN ===\n",
        "\n",
        "# Creaci√≥n del constructor del grafo para el equipo de investigaci√≥n\n",
        "research_builder = StateGraph(State)\n",
        "\n",
        "# Agregado de nodos del equipo de investigaci√≥n\n",
        "# El supervisor coordina a dos trabajadores especializados\n",
        "research_builder.add_node(\"supervisor\", research_supervisor_node)  # Supervisor del equipo\n",
        "research_builder.add_node(\"search\", search_node)  # Trabajador de b√∫squeda\n",
        "research_builder.add_node(\"web_scraper\", web_scraper_node)  # Trabajador de scraping\n",
        "\n",
        "# El flujo comienza con el supervisor del equipo de investigaci√≥n\n",
        "research_builder.add_edge(START, \"supervisor\")\n",
        "\n",
        "# Compilaci√≥n del subgrafo del equipo de investigaci√≥n\n",
        "research_graph = research_builder.compile()\n",
        "\n",
        "# Muestra el grafo del equipo de investigaci√≥n\n",
        "research_graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === HERRAMIENTA PARA LEER DOCUMENTOS ===\n",
        "\n",
        "@tool\n",
        "def read_document(\n",
        "    file_name: Annotated[str, \"Ruta del archivo para leer el documento.\"],\n",
        "    start: Annotated[Optional[int], \"La l√≠nea de inicio. Por defecto es 0\"] = None,\n",
        "    end: Annotated[Optional[int], \"La l√≠nea final. Por defecto es None\"] = None,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Lee el documento especificado.\n",
        "    \n",
        "    Permite leer todo el documento o solo un rango de l√≠neas espec√≠fico.\n",
        "    √ötil para que los agentes revisen documentos antes de editarlos.\n",
        "    \n",
        "    Par√°metros:\n",
        "    - file_name: nombre del archivo a leer\n",
        "    - start: l√≠nea de inicio (opcional, por defecto 0)\n",
        "    - end: l√≠nea final (opcional, por defecto None = hasta el final)\n",
        "    \n",
        "    Retorna: contenido del documento (completo o rango especificado)\n",
        "    \"\"\"\n",
        "    # Abre y lee todas las l√≠neas del archivo\n",
        "    with (WORKING_DIRECTORY / file_name).open(\"r\") as file:\n",
        "        lines = file.readlines()\n",
        "    \n",
        "    # Si no se especifica start, comienza desde el principio\n",
        "    if start is None:\n",
        "        start = 0\n",
        "    \n",
        "    # Retorna las l√≠neas desde start hasta end (o hasta el final si end es None)\n",
        "    return \"\\n\".join(lines[start:end])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === EQUIPO DE ESCRITURA DE DOCUMENTOS ===\n",
        "\n",
        "# Agente 1: doc_writer_agent (escritor de documentos)\n",
        "# Este agente puede leer, escribir y editar documentos completos\n",
        "doc_writer_agent = create_react_agent(\n",
        "    llm,\n",
        "    tools=[write_document, edit_document, read_document],  # Herramientas de gesti√≥n de documentos\n",
        "    prompt=(\n",
        "        \"Puedes leer, escribir y editar documentos bas√°ndote en los esquemas del tomador de notas. \"\n",
        "        \"No hagas preguntas de seguimiento.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "\n",
        "def doc_writing_node(state: State) -> Command[Literal[\"supervisor\"]]:\n",
        "    \"\"\"\n",
        "    Nodo que ejecuta el agente escritor de documentos.\n",
        "    Reporta al supervisor del equipo de escritura cuando termine.\n",
        "    \"\"\"\n",
        "    result = doc_writer_agent.invoke(state)\n",
        "    return Command(\n",
        "        update={\n",
        "            \"messages\": [\n",
        "                HumanMessage(content=result[\"messages\"][-1].content, name=\"doc_writer\")\n",
        "            ]\n",
        "        },\n",
        "        goto=\"supervisor\",\n",
        "    )\n",
        "\n",
        "# Agente 2: note_taking_agent (tomador de notas / creador de esquemas)\n",
        "# Este agente crea outlines/esquemas que gu√≠an al escritor\n",
        "note_taking_agent = create_react_agent(\n",
        "    llm,\n",
        "    tools=[create_outline, read_document],  # Herramientas de organizaci√≥n\n",
        "    prompt=(\n",
        "        \"Puedes leer documentos y crear esquemas para el escritor de documentos. \"\n",
        "        \"No hagas preguntas de seguimiento.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "\n",
        "def note_taking_node(state: State) -> Command[Literal[\"supervisor\"]]:\n",
        "    \"\"\"\n",
        "    Nodo que ejecuta el agente tomador de notas.\n",
        "    Reporta al supervisor del equipo de escritura cuando termine.\n",
        "    \"\"\"\n",
        "    result = note_taking_agent.invoke(state)\n",
        "    return Command(\n",
        "        update={\n",
        "            \"messages\": [\n",
        "                HumanMessage(content=result[\"messages\"][-1].content, name=\"note_taker\")\n",
        "            ]\n",
        "        },\n",
        "        goto=\"supervisor\",\n",
        "    )\n",
        "\n",
        "\n",
        "# Agente 3: chart_generating_agent (generador de gr√°ficos)\n",
        "# Este agente crea visualizaciones y gr√°ficos ejecutando c√≥digo Python\n",
        "chart_generating_agent = create_react_agent(\n",
        "    llm, \n",
        "    tools=[read_document, python_repl_tool]  # Herramientas de an√°lisis y visualizaci√≥n\n",
        ")\n",
        "\n",
        "\n",
        "def chart_generating_node(state: State) -> Command[Literal[\"supervisor\"]]:\n",
        "    \"\"\"\n",
        "    Nodo que ejecuta el agente generador de gr√°ficos.\n",
        "    Reporta al supervisor del equipo de escritura cuando termine.\n",
        "    \"\"\"\n",
        "    result = chart_generating_agent.invoke(state)\n",
        "    return Command(\n",
        "        update={\n",
        "            \"messages\": [\n",
        "                HumanMessage(\n",
        "                    content=result[\"messages\"][-1].content, name=\"chart_generator\"\n",
        "                )\n",
        "            ]\n",
        "        },\n",
        "        goto=\"supervisor\",\n",
        "    )\n",
        "\n",
        "# Creaci√≥n del supervisor del equipo de escritura\n",
        "# Este supervisor coordina a doc_writer, note_taker y chart_generator\n",
        "doc_writing_supervisor_node = make_supervisor_node(\n",
        "    llm, [\"doc_writer\", \"note_taker\", \"chart_generator\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === CONSTRUCCI√ìN DEL GRAFO DEL EQUIPO DE ESCRITURA ===\n",
        "\n",
        "# Creaci√≥n del constructor del grafo para el equipo de escritura de papers\n",
        "paper_writing_builder = StateGraph(State)\n",
        "\n",
        "# Agregado de nodos del equipo de escritura\n",
        "# El supervisor coordina a tres trabajadores especializados\n",
        "paper_writing_builder.add_node(\"supervisor\", doc_writing_supervisor_node)  # Supervisor del equipo\n",
        "paper_writing_builder.add_node(\"doc_writer\", doc_writing_node)  # Escritor de documentos\n",
        "paper_writing_builder.add_node(\"note_taker\", note_taking_node)  # Tomador de notas\n",
        "paper_writing_builder.add_node(\"chart_generator\", chart_generating_node)  # Generador de gr√°ficos\n",
        "\n",
        "# El flujo comienza con el supervisor del equipo de escritura\n",
        "paper_writing_builder.add_edge(START, \"supervisor\")\n",
        "\n",
        "# Compilaci√≥n del subgrafo del equipo de escritura\n",
        "paper_writing_graph = paper_writing_builder.compile()\n",
        "\n",
        "# Muestra el grafo del equipo de escritura\n",
        "paper_writing_graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === SUPERVISOR DE NIVEL SUPERIOR ===\n",
        "\n",
        "# Importaci√≥n de BaseMessage\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "# Creaci√≥n del supervisor de equipos (nivel superior)\n",
        "# Este supervisor coordina a los dos equipos completos:\n",
        "# - research_team (b√∫squeda + scraping)\n",
        "# - writing_team (notas + escritura + gr√°ficos)\n",
        "teams_supervisor_node = make_supervisor_node(llm, [\"research_team\", \"writing_team\"])\n",
        "\n",
        "# Muestra la funci√≥n del supervisor de equipos\n",
        "teams_supervisor_node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === FUNCIONES DE LLAMADA A EQUIPOS ===\n",
        "\n",
        "def call_research_team(state: State) -> Command[Literal[\"supervisor\"]]:\n",
        "    \"\"\"\n",
        "    Invoca el subgrafo completo del equipo de investigaci√≥n.\n",
        "    \n",
        "    Este nodo act√∫a como \"proxy\" que:\n",
        "    1. Toma el √∫ltimo mensaje del estado\n",
        "    2. Lo pasa al equipo de investigaci√≥n completo\n",
        "    3. Espera a que el equipo termine su trabajo\n",
        "    4. Retorna los resultados al supervisor de equipos\n",
        "    \"\"\"\n",
        "    # Invoca el grafo completo del equipo de investigaci√≥n\n",
        "    # Solo pasa el √∫ltimo mensaje para evitar duplicaci√≥n\n",
        "    response = research_graph.invoke({\"messages\": state[\"messages\"][-1]})\n",
        "    \n",
        "    # Retorna los resultados al supervisor de equipos\n",
        "    return Command(\n",
        "        update={\n",
        "            # Marca el mensaje como proveniente del \"research_team\"\n",
        "            \"messages\": [\n",
        "                HumanMessage(\n",
        "                    content=response[\"messages\"][-1].content, name=\"research_team\"\n",
        "                )\n",
        "            ]\n",
        "        },\n",
        "        goto=\"supervisor\",  # Siempre reporta al supervisor de equipos\n",
        "    )\n",
        "\n",
        "\n",
        "def call_paper_writing_team(state: State) -> Command[Literal[\"supervisor\"]]:\n",
        "    \"\"\"\n",
        "    Invoca el subgrafo completo del equipo de escritura.\n",
        "    \n",
        "    Similar a call_research_team, pero para el equipo de escritura.\n",
        "    \"\"\"\n",
        "    # Invoca el grafo completo del equipo de escritura\n",
        "    response = paper_writing_graph.invoke({\"messages\": state[\"messages\"][-1]})\n",
        "    \n",
        "    # Retorna los resultados al supervisor de equipos\n",
        "    return Command(\n",
        "        update={\n",
        "            # Marca el mensaje como proveniente del \"writing_team\"\n",
        "            \"messages\": [\n",
        "                HumanMessage(\n",
        "                    content=response[\"messages\"][-1].content, name=\"writing_team\"\n",
        "                )\n",
        "            ]\n",
        "        },\n",
        "        goto=\"supervisor\",  # Siempre reporta al supervisor de equipos\n",
        "    )\n",
        "\n",
        "\n",
        "# === CONSTRUCCI√ìN DEL GRAFO SUPERIOR JER√ÅRQUICO ===\n",
        "\n",
        "# Creaci√≥n del constructor del grafo de nivel superior\n",
        "super_builder = StateGraph(State)\n",
        "\n",
        "# Agregado de nodos del nivel superior\n",
        "# Este grafo coordina a los dos equipos completos\n",
        "super_builder.add_node(\"supervisor\", teams_supervisor_node)  # Supervisor de equipos\n",
        "super_builder.add_node(\"research_team\", call_research_team)  # Nodo que invoca equipo de investigaci√≥n\n",
        "super_builder.add_node(\"writing_team\", call_paper_writing_team)  # Nodo que invoca equipo de escritura\n",
        "\n",
        "# El flujo comienza con el supervisor de equipos\n",
        "super_builder.add_edge(START, \"supervisor\")\n",
        "\n",
        "# Compilaci√≥n del grafo jer√°rquico completo\n",
        "super_graph = super_builder.compile()\n",
        "\n",
        "# Muestra el grafo jer√°rquico completo\n",
        "super_graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === INVOCACI√ìN DEL SISTEMA JER√ÅRQUICO COMPLETO ===\n",
        "\n",
        "# Invoca el sistema jer√°rquico multi-agente completo con una tarea compleja\n",
        "response=super_graph.invoke(\n",
        "    {\n",
        "        # La consulta solicita escribir sobre variantes de transformers\n",
        "        # Esto requiere:\n",
        "        # 1. Equipo de investigaci√≥n: buscar informaci√≥n (search + web_scraper)\n",
        "        # 2. Equipo de escritura: crear outline (note_taker) ‚Üí escribir documento (doc_writer)\n",
        "        \"messages\": [\n",
        "            (\"user\", \"Escribe sobre variantes de transformers en despliegues de producci√≥n.\")\n",
        "        ],\n",
        "    }\n",
        ")\n",
        "\n",
        "# Flujo de ejecuci√≥n esperado:\n",
        "# 1. Supervisor de equipos analiza la tarea\n",
        "# 2. Delega a research_team\n",
        "#    2.1. Supervisor de investigaci√≥n coordina a search y web_scraper\n",
        "#    2.2. Recopilan informaci√≥n sobre transformers\n",
        "#    2.3. Reportan resultados al supervisor de equipos\n",
        "# 3. Supervisor de equipos delega a writing_team\n",
        "#    3.1. Supervisor de escritura coordina a note_taker, doc_writer y chart_generator\n",
        "#    3.2. note_taker crea outline del documento\n",
        "#    3.3. doc_writer escribe el documento completo\n",
        "#    3.4. Reportan resultados al supervisor de equipos\n",
        "# 4. Supervisor de equipos confirma finalizaci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Muestra la respuesta completa del sistema jer√°rquico\n",
        "# Incluye todo el historial de mensajes entre:\n",
        "# - Supervisor de equipos\n",
        "# - Equipo de investigaci√≥n (y sus subagentes)\n",
        "# - Equipo de escritura (y sus subagentes)\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extrae y muestra el contenido del √∫ltimo mensaje\n",
        "# Este es el mensaje final del equipo de escritura confirmando que:\n",
        "# 1. El documento ha sido creado\n",
        "# 2. El outline ha sido guardado\n",
        "# 3. El trabajo est√° completo\n",
        "response[\"messages\"][-1].content"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
