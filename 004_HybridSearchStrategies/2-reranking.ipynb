{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5a23eff",
   "metadata": {},
   "source": [
    "### Estrategias de Búsqueda Híbrida con Reordenamiento (Reranking)\n",
    "\n",
    "El reordenamiento es una técnica avanzada que mejora significativamente la calidad de los resultados en sistemas RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff5fac5",
   "metadata": {},
   "source": [
    "El **reordenamiento (re-ranking)** es un proceso de filtrado de segunda etapa en sistemas de recuperación, especialmente en pipelines RAG, donde:\n",
    "\n",
    "1. **Primera etapa**: Usamos un recuperador rápido (como BM25, FAISS, o híbrido) para obtener los top-k documentos rápidamente.\n",
    "\n",
    "2. **Segunda etapa**: Usamos un modelo más preciso pero más lento (como un cross-encoder o LLM) para re-puntuar y reordenar esos documentos según su relevancia a la consulta.\n",
    "\n",
    "👉 **Ventaja**: Esto asegura que los documentos más relevantes aparezcan en la parte superior, mejorando significativamente la calidad de la respuesta final del LLM.\n",
    "\n",
    "**¿Por qué es importante?**\n",
    "- Los recuperadores rápidos priorizan velocidad sobre precisión\n",
    "- El reordenamiento añade una capa de precisión sin sacrificar demasiado rendimiento\n",
    "- Solo reordenamos un conjunto pequeño (top-k), no toda la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f5fd53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Udemy\\RAGBootcamp\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importación de librerías necesarias para el sistema de reordenamiento\n",
    "\n",
    "# TextLoader: Para cargar archivos de texto plano\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# RecursiveCharacterTextSplitter: Para dividir texto en fragmentos (chunks) manejables\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# init_chat_model: Para inicializar modelos de chat de diferentes proveedores\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# PromptTemplate: Para crear plantillas de prompts con variables dinámicas\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Document: Clase para representar documentos con contenido y metadatos\n",
    "from langchain.schema import Document\n",
    "\n",
    "# StrOutputParser: Para convertir la salida del LLM a string\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdf8243d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'langchain_sample.txt'}, page_content='LangChain es un framework flexible diseÃ±ado para desarrollar aplicaciones basadas en grandes modelos de lenguaje (LLM). Proporciona herramientas y abstracciones para trabajar con LLM de forma mÃ¡s eficaz e incluye componentes para la gestiÃ³n de indicaciones, cadenas, memoria y agentes.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='LangChain se integra con numerosos servicios de terceros, como OpenAI, Hugging Face y Cohere. Esto permite a los desarrolladores experimentar con diferentes modelos y optimizar el rendimiento para casos de uso especÃ\\xadficos, como la sÃ\\xadntesis, la respuesta a preguntas o la traducciÃ³n.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='La GeneraciÃ³n Aumentada por RecuperaciÃ³n (RAG) es una potente tÃ©cnica que recupera conocimiento externo y lo transmite a la indicaciÃ³n para fundamentar las respuestas de los LLM. LangChain facilita la implementaciÃ³n de RAG utilizando bases de datos vectoriales como FAISS, Chroma y Pinecone.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='BM25 es un mÃ©todo tradicional de recuperaciÃ³n dispersa que puntÃºa los documentos basÃ¡ndose en la coincidencia de palabras clave. Aunque es rÃ¡pido, a menudo presenta dificultades con sinÃ³nimos y similitud semÃ¡ntica.\\nLa recuperaciÃ³n densa utiliza incrustaciones para vincular la consulta y los documentos en un espacio vectorial. Esto permite capturar el significado semÃ¡ntico, lo que lo hace Ãºtil para consultas difusas o en lenguaje natural.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='LangChain admite la recuperaciÃ³n hÃ\\xadbrida combinando BM25 y puntuaciones de similitud densas. Este enfoque mejora tanto la precisiÃ³n como la recuperaciÃ³n en la bÃºsqueda de documentos.\\nFAISS es una biblioteca popular utilizada para la bÃºsqueda rÃ¡pida y aproximada del vecino mÃ¡s cercano en espacios de alta dimensiÃ³n. Admite Ã\\xadndices planos y comprimidos, lo que la hace escalable para grandes almacenes de documentos.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='Los agentes en LangChain son cadenas que utilizan LLM para decidir quÃ© herramientas usar y en quÃ© orden. Esto los hace adecuados para tareas de varios pasos, como la respuesta a preguntas con bÃºsqueda y ejecuciÃ³n de cÃ³digo.\\nLangChain admite la integraciÃ³n de herramientas, como bÃºsqueda web, calculadoras y API, lo que permite que los LLM interactÃºen con sistemas externos y respondan con mayor precisiÃ³n a consultas dinÃ¡micas.'),\n",
       " Document(metadata={'source': 'langchain_sample.txt'}, page_content='La memoria en LangChain permite la retenciÃ³n de contexto en varios pasos de una conversaciÃ³n o tarea, lo que hace que la aplicaciÃ³n sea mÃ¡s coherente y con estado.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paso 1: Cargar el archivo de texto\n",
    "# Cargamos un archivo de muestra que contiene información sobre LangChain\n",
    "loader = TextLoader(\"langchain_sample.txt\")\n",
    "# load() devuelve una lista de documentos (en este caso, solo uno con todo el contenido)\n",
    "raw_docs = loader.load()\n",
    "\n",
    "# Paso 2: Dividir el texto en fragmentos (chunks) más pequeños\n",
    "# RecursiveCharacterTextSplitter divide el texto de forma inteligente\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,      # Tamaño máximo de cada fragmento en caracteres\n",
    "    chunk_overlap=50     # Superposición entre fragmentos para mantener contexto\n",
    ")\n",
    "# split_documents() toma los documentos y los divide en fragmentos más pequeños\n",
    "docs = splitter.split_documents(raw_docs)\n",
    "\n",
    "# Mostrar los documentos fragmentados\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0b203af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la consulta del usuario\n",
    "# Esta es la pregunta que queremos responder usando nuestro sistema RAG con reordenamiento\n",
    "query = \"¿Cómo puedo usar LangChain para construir una aplicación con memoria y herramientas?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9720bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3: Configurar FAISS con embeddings de HuggingFace\n",
    "\n",
    "# Importar las librerías necesarias para el almacén vectorial\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Inicializar el modelo de embeddings de HuggingFace\n",
    "# all-MiniLM-L6-v2 es un modelo compacto y eficiente (384 dimensiones)\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Crear el almacén vectorial FAISS a partir de los documentos fragmentados\n",
    "# FAISS convierte cada documento en un vector numérico para búsqueda de similitud\n",
    "vectorstore = FAISS.from_documents(docs, embedding_model)\n",
    "\n",
    "# Convertir el almacén en un recuperador configurado para devolver los top-8 resultados\n",
    "# k=8 significa que recuperaremos los 8 documentos más similares a la consulta\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "332edeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternativa: Configurar FAISS con embeddings de OpenAI\n",
    "\n",
    "# Importar módulos necesarios para variables de entorno\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Cargar las variables de entorno desde el archivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Establecer la API key de OpenAI desde las variables de entorno\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Importar la clase de embeddings de OpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Inicializar los embeddings de OpenAI\n",
    "# OpenAI usa modelos más potentes pero requiere API key y tiene costo\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Crear un almacén vectorial alternativo usando embeddings de OpenAI\n",
    "# Esto permite comparar el rendimiento entre HuggingFace y OpenAI\n",
    "vectorstore_openai = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# Crear recuperador con OpenAI embeddings, también configurado para top-8\n",
    "retriever_openai = vectorstore_openai.as_retriever(search_kwargs={\"k\": 8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7e52001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001C53FAD8CB0>, search_kwargs={'k': 8})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostrar la configuración del recuperador con HuggingFace embeddings\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49fc0a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001C541CB9DF0>, search_kwargs={'k': 8})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostrar la configuración del recuperador con OpenAI embeddings\n",
    "retriever_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cd8773a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001C5432F5AF0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001C5436C5010>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paso 4: Configurar el modelo de lenguaje para reordenamiento\n",
    "\n",
    "# Importar la función para inicializar modelos de chat\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Cargar la API key de Groq desde las variables de entorno\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Inicializar el LLM usando Groq con el modelo Gemma2-9B\n",
    "# Groq ofrece inferencia rápida y este modelo es eficiente para tareas de ranking\n",
    "llm = init_chat_model(\"groq:llama-3.1-8b-instant\")\n",
    "\n",
    "# Mostrar la configuración del LLM\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2b0c172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 5: Crear el prompt para reordenamiento\n",
    "# Este prompt instruye al LLM a evaluar y clasificar documentos por relevancia\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Eres un asistente útil. Tu tarea es clasificar los siguientes documentos de más a menos relevante según la pregunta del usuario.\n",
    "\n",
    "Pregunta del Usuario: \"{question}\"\n",
    "\n",
    "Documentos:\n",
    "{documents}\n",
    "\n",
    "Instrucciones:\n",
    "- Piensa cuidadosamente sobre la relevancia de cada documento respecto a la pregunta del usuario.\n",
    "- Devuelve una lista de índices de documentos en orden de clasificación, comenzando por el más relevante.\n",
    "- Considera tanto la coincidencia temática como la utilidad práctica para responder la pregunta.\n",
    "\n",
    "Formato de salida: índices de documentos separados por comas (ej: 2,1,3,0,...)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8eed561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='a0c1f7e3-2d65-4691-9640-b7ba176e4efe', metadata={'source': 'langchain_sample.txt'}, page_content='La memoria en LangChain permite la retenciÃ³n de contexto en varios pasos de una conversaciÃ³n o tarea, lo que hace que la aplicaciÃ³n sea mÃ¡s coherente y con estado.'),\n",
       " Document(id='44be4b59-d863-4940-a9e0-4a7f82ddb285', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain es un framework flexible diseÃ±ado para desarrollar aplicaciones basadas en grandes modelos de lenguaje (LLM). Proporciona herramientas y abstracciones para trabajar con LLM de forma mÃ¡s eficaz e incluye componentes para la gestiÃ³n de indicaciones, cadenas, memoria y agentes.'),\n",
       " Document(id='0c9c6910-5387-4e35-b2d7-47f357e7f615', metadata={'source': 'langchain_sample.txt'}, page_content='Los agentes en LangChain son cadenas que utilizan LLM para decidir quÃ© herramientas usar y en quÃ© orden. Esto los hace adecuados para tareas de varios pasos, como la respuesta a preguntas con bÃºsqueda y ejecuciÃ³n de cÃ³digo.\\nLangChain admite la integraciÃ³n de herramientas, como bÃºsqueda web, calculadoras y API, lo que permite que los LLM interactÃºen con sistemas externos y respondan con mayor precisiÃ³n a consultas dinÃ¡micas.'),\n",
       " Document(id='6ccab2bb-2f2c-4861-aba2-6ed82d64a406', metadata={'source': 'langchain_sample.txt'}, page_content='BM25 es un mÃ©todo tradicional de recuperaciÃ³n dispersa que puntÃºa los documentos basÃ¡ndose en la coincidencia de palabras clave. Aunque es rÃ¡pido, a menudo presenta dificultades con sinÃ³nimos y similitud semÃ¡ntica.\\nLa recuperaciÃ³n densa utiliza incrustaciones para vincular la consulta y los documentos en un espacio vectorial. Esto permite capturar el significado semÃ¡ntico, lo que lo hace Ãºtil para consultas difusas o en lenguaje natural.'),\n",
       " Document(id='f9ec19c5-901a-46ad-85ba-3521f0ffb711', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain admite la recuperaciÃ³n hÃ\\xadbrida combinando BM25 y puntuaciones de similitud densas. Este enfoque mejora tanto la precisiÃ³n como la recuperaciÃ³n en la bÃºsqueda de documentos.\\nFAISS es una biblioteca popular utilizada para la bÃºsqueda rÃ¡pida y aproximada del vecino mÃ¡s cercano en espacios de alta dimensiÃ³n. Admite Ã\\xadndices planos y comprimidos, lo que la hace escalable para grandes almacenes de documentos.'),\n",
       " Document(id='7705b287-51b8-4123-8fda-9db579b92dd4', metadata={'source': 'langchain_sample.txt'}, page_content='La GeneraciÃ³n Aumentada por RecuperaciÃ³n (RAG) es una potente tÃ©cnica que recupera conocimiento externo y lo transmite a la indicaciÃ³n para fundamentar las respuestas de los LLM. LangChain facilita la implementaciÃ³n de RAG utilizando bases de datos vectoriales como FAISS, Chroma y Pinecone.'),\n",
       " Document(id='1f3ce853-fafa-4360-b449-0c05198bf00e', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain se integra con numerosos servicios de terceros, como OpenAI, Hugging Face y Cohere. Esto permite a los desarrolladores experimentar con diferentes modelos y optimizar el rendimiento para casos de uso especÃ\\xadficos, como la sÃ\\xadntesis, la respuesta a preguntas o la traducciÃ³n.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paso 6: Recuperar documentos usando el recuperador vectorial\n",
    "# Invocamos el recuperador con nuestra consulta para obtener los documentos más similares\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "# Mostrar los documentos recuperados antes del reordenamiento\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ce0eeb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['documents', 'question'], input_types={}, partial_variables={}, template='\\nEres un asistente útil. Tu tarea es clasificar los siguientes documentos de más a menos relevante según la pregunta del usuario.\\n\\nPregunta del Usuario: \"{question}\"\\n\\nDocumentos:\\n{documents}\\n\\nInstrucciones:\\n- Piensa cuidadosamente sobre la relevancia de cada documento respecto a la pregunta del usuario.\\n- Devuelve una lista de índices de documentos en orden de clasificación, comenzando por el más relevante.\\n- Considera tanto la coincidencia temática como la utilidad práctica para responder la pregunta.\\n\\nFormato de salida: índices de documentos separados por comas (ej: 2,1,3,0,...)\\n')\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001C5432F5AF0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001C5436C5010>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paso 7: Crear la cadena de reordenamiento\n",
    "# Esta cadena combina el prompt, el LLM y el parser de salida\n",
    "# El operador | (pipe) encadena los componentes secuencialmente\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Mostrar la estructura de la cadena\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3278ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 8: Formatear los documentos para el prompt\n",
    "# Creamos una lista numerada de documentos con su contenido\n",
    "# enumerate() nos da el índice (i) y el documento (doc) para cada elemento\n",
    "doc_lines = [f\"{i+1}. {doc.page_content}\" for i, doc in enumerate(retrieved_docs)]\n",
    "\n",
    "# Unir todos los documentos en un solo string, separados por saltos de línea\n",
    "# Esto crea un formato legible para que el LLM evalúe todos los documentos\n",
    "formatted_docs = \"\\n\".join(doc_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ecef652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. La memoria en LangChain permite la retenciÃ³n de contexto en varios pasos de una conversaciÃ³n o tarea, lo que hace que la aplicaciÃ³n sea mÃ¡s coherente y con estado.',\n",
       " '2. LangChain es un framework flexible diseÃ±ado para desarrollar aplicaciones basadas en grandes modelos de lenguaje (LLM). Proporciona herramientas y abstracciones para trabajar con LLM de forma mÃ¡s eficaz e incluye componentes para la gestiÃ³n de indicaciones, cadenas, memoria y agentes.',\n",
       " '3. Los agentes en LangChain son cadenas que utilizan LLM para decidir quÃ© herramientas usar y en quÃ© orden. Esto los hace adecuados para tareas de varios pasos, como la respuesta a preguntas con bÃºsqueda y ejecuciÃ³n de cÃ³digo.\\nLangChain admite la integraciÃ³n de herramientas, como bÃºsqueda web, calculadoras y API, lo que permite que los LLM interactÃºen con sistemas externos y respondan con mayor precisiÃ³n a consultas dinÃ¡micas.',\n",
       " '4. BM25 es un mÃ©todo tradicional de recuperaciÃ³n dispersa que puntÃºa los documentos basÃ¡ndose en la coincidencia de palabras clave. Aunque es rÃ¡pido, a menudo presenta dificultades con sinÃ³nimos y similitud semÃ¡ntica.\\nLa recuperaciÃ³n densa utiliza incrustaciones para vincular la consulta y los documentos en un espacio vectorial. Esto permite capturar el significado semÃ¡ntico, lo que lo hace Ãºtil para consultas difusas o en lenguaje natural.',\n",
       " '5. LangChain admite la recuperaciÃ³n hÃ\\xadbrida combinando BM25 y puntuaciones de similitud densas. Este enfoque mejora tanto la precisiÃ³n como la recuperaciÃ³n en la bÃºsqueda de documentos.\\nFAISS es una biblioteca popular utilizada para la bÃºsqueda rÃ¡pida y aproximada del vecino mÃ¡s cercano en espacios de alta dimensiÃ³n. Admite Ã\\xadndices planos y comprimidos, lo que la hace escalable para grandes almacenes de documentos.',\n",
       " '6. La GeneraciÃ³n Aumentada por RecuperaciÃ³n (RAG) es una potente tÃ©cnica que recupera conocimiento externo y lo transmite a la indicaciÃ³n para fundamentar las respuestas de los LLM. LangChain facilita la implementaciÃ³n de RAG utilizando bases de datos vectoriales como FAISS, Chroma y Pinecone.',\n",
       " '7. LangChain se integra con numerosos servicios de terceros, como OpenAI, Hugging Face y Cohere. Esto permite a los desarrolladores experimentar con diferentes modelos y optimizar el rendimiento para casos de uso especÃ\\xadficos, como la sÃ\\xadntesis, la respuesta a preguntas o la traducciÃ³n.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostrar la lista de documentos formateados antes de unirlos\n",
    "doc_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9882543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. La memoria en LangChain permite la retenciÃ³n de contexto en varios pasos de una conversaciÃ³n o tarea, lo que hace que la aplicaciÃ³n sea mÃ¡s coherente y con estado.\\n2. LangChain es un framework flexible diseÃ±ado para desarrollar aplicaciones basadas en grandes modelos de lenguaje (LLM). Proporciona herramientas y abstracciones para trabajar con LLM de forma mÃ¡s eficaz e incluye componentes para la gestiÃ³n de indicaciones, cadenas, memoria y agentes.\\n3. Los agentes en LangChain son cadenas que utilizan LLM para decidir quÃ© herramientas usar y en quÃ© orden. Esto los hace adecuados para tareas de varios pasos, como la respuesta a preguntas con bÃºsqueda y ejecuciÃ³n de cÃ³digo.\\nLangChain admite la integraciÃ³n de herramientas, como bÃºsqueda web, calculadoras y API, lo que permite que los LLM interactÃºen con sistemas externos y respondan con mayor precisiÃ³n a consultas dinÃ¡micas.\\n4. BM25 es un mÃ©todo tradicional de recuperaciÃ³n dispersa que puntÃºa los documentos basÃ¡ndose en la coincidencia de palabras clave. Aunque es rÃ¡pido, a menudo presenta dificultades con sinÃ³nimos y similitud semÃ¡ntica.\\nLa recuperaciÃ³n densa utiliza incrustaciones para vincular la consulta y los documentos en un espacio vectorial. Esto permite capturar el significado semÃ¡ntico, lo que lo hace Ãºtil para consultas difusas o en lenguaje natural.\\n5. LangChain admite la recuperaciÃ³n hÃ\\xadbrida combinando BM25 y puntuaciones de similitud densas. Este enfoque mejora tanto la precisiÃ³n como la recuperaciÃ³n en la bÃºsqueda de documentos.\\nFAISS es una biblioteca popular utilizada para la bÃºsqueda rÃ¡pida y aproximada del vecino mÃ¡s cercano en espacios de alta dimensiÃ³n. Admite Ã\\xadndices planos y comprimidos, lo que la hace escalable para grandes almacenes de documentos.\\n6. La GeneraciÃ³n Aumentada por RecuperaciÃ³n (RAG) es una potente tÃ©cnica que recupera conocimiento externo y lo transmite a la indicaciÃ³n para fundamentar las respuestas de los LLM. LangChain facilita la implementaciÃ³n de RAG utilizando bases de datos vectoriales como FAISS, Chroma y Pinecone.\\n7. LangChain se integra con numerosos servicios de terceros, como OpenAI, Hugging Face y Cohere. Esto permite a los desarrolladores experimentar con diferentes modelos y optimizar el rendimiento para casos de uso especÃ\\xadficos, como la sÃ\\xadntesis, la respuesta a preguntas o la traducciÃ³n.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostrar todos los documentos concatenados en un solo string\n",
    "# Este es el formato que se enviará al LLM para su evaluación\n",
    "formatted_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7e76228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Después de analizar los documentos y considerar tanto la coincidencia temática como la utilidad práctica para responder la pregunta, clasifico los documentos de la siguiente manera:\\n\\nLa memoria en LangChain permite la retenciÃ³n de contexto en varios pasos de una conversaciÃ³n o tarea, lo que hace que la aplicaciÃ³n sea mÃ¡s coherente y con estado. (Documento 1)\\nLangChain es un framework flexible diseÃ±ado para desarrollar aplicaciones basadas en grandes modelos de lenguaje (LLM). Proporciona herramientas y abstracciones para trabajar con LLM de forma mÃ¡s eficaz e incluye componentes para la gestiÃ³n de indicaciones, cadenas, memoria y agentes. (Documento 2)\\nLos agentes en LangChain son cadenas que utilizan LLM para decidir quÃ© herramientas usar y en quÃ© orden. Esto los hace adecuados para tareas de varios pasos, como la respuesta a preguntas con bÃºsqueda y ejecuciÃ³n de cÃ³digo. (Documento 3)\\nLangChain admite la integraciÃ³n de herramientas, como bÃºsqueda web, calculadoras y API, lo que permite que los LLM interactÃºen con sistemas externos y respondan con mayor precisiÃ³n a consultas dinÃ¡micas. (Documento 4)\\nLangChain se integra con numerosos servicios de terceros, como OpenAI, Hugging Face y Cohere. Esto permite a los desarrolladores experimentar con diferentes modelos y optimizar el rendimiento para casos de uso especÃ\\xadficos, como la sÃ\\xadntesis, la respuesta a preguntas o la traducciÃ³n. (Documento 7)\\nFAISS es una biblioteca popular utilizada para la bÃºsqueda rÃ¡pida y aproximada del vecino mÃ¡s cercano en espacios de alta dimensiÃ³n. Admite Ã\\xadndices planos y comprimidos, lo que la hace escalable para grandes almacenes de documentos. (Documento 5)\\nLa GeneraciÃ³n Aumentada por RecuperaciÃ³n (RAG) es una potente tÃ©cnica que recupera conocimiento externo y lo transmite a la indicaciÃ³n para fundamentar las respuestas de los LLM. LangChain facilita la implementaciÃ³n de RAG utilizando bases de datos vectoriales como FAISS, Chroma y Pinecone. (Documento 6)\\nBM25 es un mÃ©todo tradicional de recuperaciÃ³n dispersa que puntÃºa los documentos basÃ¡ndose en la coincidencia de palabras clave. Aunque es rÃ¡pido, a menudo presenta dificultades con sinÃ³nimos y similitud semÃ¡ntica. (Documento 4)\\nLa recuperaciÃ³n densa utiliza incrustaciones para vincular la consulta y los documentos en un espacio vectorial. Esto permite capturar el significado semÃ¡ntico, lo que lo hace Ãºtil para consultas difusas o en lenguaje natural. (Documento 5)\\nLangChain admite la recuperaciÃ³n hÃ\\xadbrida combinando BM25 y puntuaciones de similitud densas. Este enfoque mejora tanto la precisiÃ³n como la recuperaciÃ³n en la bÃºsqueda de documentos. (Documento 5)\\n\\nClasificación:\\n\\n1, 2, 3, 7, 5, 6, 4, 5, 5\\n\\nNota: Los documentos 5 se repiten debido a que tienen contenido relevante pero en diferentes partes del texto. Si solo se considera el contenido principal del documento 5, la clasificación sería:\\n\\n1, 2, 3, 7, 5, 6, 4'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paso 9: Invocar la cadena de reordenamiento\n",
    "# Enviamos la pregunta y los documentos formateados al LLM\n",
    "# El LLM analizará cada documento y devolverá los índices ordenados por relevancia\n",
    "response = chain.invoke({\"question\": query, \"documents\": formatted_docs})\n",
    "\n",
    "# Mostrar la respuesta del LLM con el ranking de documentos\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3c0699d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 6, 4, 5, 3, 4, 1, 2, 6, 4, 5, 3]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paso 10: Parsear la respuesta del LLM para extraer los índices\n",
    "# El LLM devuelve algo como \"2,1,4,5,6...\" y necesitamos convertirlo a índices numéricos\n",
    "\n",
    "# Dividimos la respuesta por comas, limpiamos espacios y filtramos solo números\n",
    "# Restamos 1 porque el LLM usa numeración 1-based pero Python usa 0-based\n",
    "indices = [int(x.strip()) - 1 for x in response.split(\",\") if x.strip().isdigit()]\n",
    "\n",
    "# Mostrar los índices extraídos (en formato 0-based de Python)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "468339a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='a0c1f7e3-2d65-4691-9640-b7ba176e4efe', metadata={'source': 'langchain_sample.txt'}, page_content='La memoria en LangChain permite la retenciÃ³n de contexto en varios pasos de una conversaciÃ³n o tarea, lo que hace que la aplicaciÃ³n sea mÃ¡s coherente y con estado.'),\n",
       " Document(id='44be4b59-d863-4940-a9e0-4a7f82ddb285', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain es un framework flexible diseÃ±ado para desarrollar aplicaciones basadas en grandes modelos de lenguaje (LLM). Proporciona herramientas y abstracciones para trabajar con LLM de forma mÃ¡s eficaz e incluye componentes para la gestiÃ³n de indicaciones, cadenas, memoria y agentes.'),\n",
       " Document(id='0c9c6910-5387-4e35-b2d7-47f357e7f615', metadata={'source': 'langchain_sample.txt'}, page_content='Los agentes en LangChain son cadenas que utilizan LLM para decidir quÃ© herramientas usar y en quÃ© orden. Esto los hace adecuados para tareas de varios pasos, como la respuesta a preguntas con bÃºsqueda y ejecuciÃ³n de cÃ³digo.\\nLangChain admite la integraciÃ³n de herramientas, como bÃºsqueda web, calculadoras y API, lo que permite que los LLM interactÃºen con sistemas externos y respondan con mayor precisiÃ³n a consultas dinÃ¡micas.'),\n",
       " Document(id='6ccab2bb-2f2c-4861-aba2-6ed82d64a406', metadata={'source': 'langchain_sample.txt'}, page_content='BM25 es un mÃ©todo tradicional de recuperaciÃ³n dispersa que puntÃºa los documentos basÃ¡ndose en la coincidencia de palabras clave. Aunque es rÃ¡pido, a menudo presenta dificultades con sinÃ³nimos y similitud semÃ¡ntica.\\nLa recuperaciÃ³n densa utiliza incrustaciones para vincular la consulta y los documentos en un espacio vectorial. Esto permite capturar el significado semÃ¡ntico, lo que lo hace Ãºtil para consultas difusas o en lenguaje natural.'),\n",
       " Document(id='f9ec19c5-901a-46ad-85ba-3521f0ffb711', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain admite la recuperaciÃ³n hÃ\\xadbrida combinando BM25 y puntuaciones de similitud densas. Este enfoque mejora tanto la precisiÃ³n como la recuperaciÃ³n en la bÃºsqueda de documentos.\\nFAISS es una biblioteca popular utilizada para la bÃºsqueda rÃ¡pida y aproximada del vecino mÃ¡s cercano en espacios de alta dimensiÃ³n. Admite Ã\\xadndices planos y comprimidos, lo que la hace escalable para grandes almacenes de documentos.'),\n",
       " Document(id='7705b287-51b8-4123-8fda-9db579b92dd4', metadata={'source': 'langchain_sample.txt'}, page_content='La GeneraciÃ³n Aumentada por RecuperaciÃ³n (RAG) es una potente tÃ©cnica que recupera conocimiento externo y lo transmite a la indicaciÃ³n para fundamentar las respuestas de los LLM. LangChain facilita la implementaciÃ³n de RAG utilizando bases de datos vectoriales como FAISS, Chroma y Pinecone.'),\n",
       " Document(id='1f3ce853-fafa-4360-b449-0c05198bf00e', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain se integra con numerosos servicios de terceros, como OpenAI, Hugging Face y Cohere. Esto permite a los desarrolladores experimentar con diferentes modelos y optimizar el rendimiento para casos de uso especÃ\\xadficos, como la sÃ\\xadntesis, la respuesta a preguntas o la traducciÃ³n.')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostrar nuevamente los documentos originales recuperados (antes del reordenamiento)\n",
    "# Esto nos permite comparar el orden original vs el orden reordenado\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3d31bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='44be4b59-d863-4940-a9e0-4a7f82ddb285', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain es un framework flexible diseÃ±ado para desarrollar aplicaciones basadas en grandes modelos de lenguaje (LLM). Proporciona herramientas y abstracciones para trabajar con LLM de forma mÃ¡s eficaz e incluye componentes para la gestiÃ³n de indicaciones, cadenas, memoria y agentes.'),\n",
       " Document(id='0c9c6910-5387-4e35-b2d7-47f357e7f615', metadata={'source': 'langchain_sample.txt'}, page_content='Los agentes en LangChain son cadenas que utilizan LLM para decidir quÃ© herramientas usar y en quÃ© orden. Esto los hace adecuados para tareas de varios pasos, como la respuesta a preguntas con bÃºsqueda y ejecuciÃ³n de cÃ³digo.\\nLangChain admite la integraciÃ³n de herramientas, como bÃºsqueda web, calculadoras y API, lo que permite que los LLM interactÃºen con sistemas externos y respondan con mayor precisiÃ³n a consultas dinÃ¡micas.'),\n",
       " Document(id='1f3ce853-fafa-4360-b449-0c05198bf00e', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain se integra con numerosos servicios de terceros, como OpenAI, Hugging Face y Cohere. Esto permite a los desarrolladores experimentar con diferentes modelos y optimizar el rendimiento para casos de uso especÃ\\xadficos, como la sÃ\\xadntesis, la respuesta a preguntas o la traducciÃ³n.'),\n",
       " Document(id='f9ec19c5-901a-46ad-85ba-3521f0ffb711', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain admite la recuperaciÃ³n hÃ\\xadbrida combinando BM25 y puntuaciones de similitud densas. Este enfoque mejora tanto la precisiÃ³n como la recuperaciÃ³n en la bÃºsqueda de documentos.\\nFAISS es una biblioteca popular utilizada para la bÃºsqueda rÃ¡pida y aproximada del vecino mÃ¡s cercano en espacios de alta dimensiÃ³n. Admite Ã\\xadndices planos y comprimidos, lo que la hace escalable para grandes almacenes de documentos.'),\n",
       " Document(id='7705b287-51b8-4123-8fda-9db579b92dd4', metadata={'source': 'langchain_sample.txt'}, page_content='La GeneraciÃ³n Aumentada por RecuperaciÃ³n (RAG) es una potente tÃ©cnica que recupera conocimiento externo y lo transmite a la indicaciÃ³n para fundamentar las respuestas de los LLM. LangChain facilita la implementaciÃ³n de RAG utilizando bases de datos vectoriales como FAISS, Chroma y Pinecone.'),\n",
       " Document(id='6ccab2bb-2f2c-4861-aba2-6ed82d64a406', metadata={'source': 'langchain_sample.txt'}, page_content='BM25 es un mÃ©todo tradicional de recuperaciÃ³n dispersa que puntÃºa los documentos basÃ¡ndose en la coincidencia de palabras clave. Aunque es rÃ¡pido, a menudo presenta dificultades con sinÃ³nimos y similitud semÃ¡ntica.\\nLa recuperaciÃ³n densa utiliza incrustaciones para vincular la consulta y los documentos en un espacio vectorial. Esto permite capturar el significado semÃ¡ntico, lo que lo hace Ãºtil para consultas difusas o en lenguaje natural.'),\n",
       " Document(id='f9ec19c5-901a-46ad-85ba-3521f0ffb711', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain admite la recuperaciÃ³n hÃ\\xadbrida combinando BM25 y puntuaciones de similitud densas. Este enfoque mejora tanto la precisiÃ³n como la recuperaciÃ³n en la bÃºsqueda de documentos.\\nFAISS es una biblioteca popular utilizada para la bÃºsqueda rÃ¡pida y aproximada del vecino mÃ¡s cercano en espacios de alta dimensiÃ³n. Admite Ã\\xadndices planos y comprimidos, lo que la hace escalable para grandes almacenes de documentos.'),\n",
       " Document(id='44be4b59-d863-4940-a9e0-4a7f82ddb285', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain es un framework flexible diseÃ±ado para desarrollar aplicaciones basadas en grandes modelos de lenguaje (LLM). Proporciona herramientas y abstracciones para trabajar con LLM de forma mÃ¡s eficaz e incluye componentes para la gestiÃ³n de indicaciones, cadenas, memoria y agentes.'),\n",
       " Document(id='0c9c6910-5387-4e35-b2d7-47f357e7f615', metadata={'source': 'langchain_sample.txt'}, page_content='Los agentes en LangChain son cadenas que utilizan LLM para decidir quÃ© herramientas usar y en quÃ© orden. Esto los hace adecuados para tareas de varios pasos, como la respuesta a preguntas con bÃºsqueda y ejecuciÃ³n de cÃ³digo.\\nLangChain admite la integraciÃ³n de herramientas, como bÃºsqueda web, calculadoras y API, lo que permite que los LLM interactÃºen con sistemas externos y respondan con mayor precisiÃ³n a consultas dinÃ¡micas.'),\n",
       " Document(id='1f3ce853-fafa-4360-b449-0c05198bf00e', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain se integra con numerosos servicios de terceros, como OpenAI, Hugging Face y Cohere. Esto permite a los desarrolladores experimentar con diferentes modelos y optimizar el rendimiento para casos de uso especÃ\\xadficos, como la sÃ\\xadntesis, la respuesta a preguntas o la traducciÃ³n.'),\n",
       " Document(id='f9ec19c5-901a-46ad-85ba-3521f0ffb711', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain admite la recuperaciÃ³n hÃ\\xadbrida combinando BM25 y puntuaciones de similitud densas. Este enfoque mejora tanto la precisiÃ³n como la recuperaciÃ³n en la bÃºsqueda de documentos.\\nFAISS es una biblioteca popular utilizada para la bÃºsqueda rÃ¡pida y aproximada del vecino mÃ¡s cercano en espacios de alta dimensiÃ³n. Admite Ã\\xadndices planos y comprimidos, lo que la hace escalable para grandes almacenes de documentos.'),\n",
       " Document(id='7705b287-51b8-4123-8fda-9db579b92dd4', metadata={'source': 'langchain_sample.txt'}, page_content='La GeneraciÃ³n Aumentada por RecuperaciÃ³n (RAG) es una potente tÃ©cnica que recupera conocimiento externo y lo transmite a la indicaciÃ³n para fundamentar las respuestas de los LLM. LangChain facilita la implementaciÃ³n de RAG utilizando bases de datos vectoriales como FAISS, Chroma y Pinecone.'),\n",
       " Document(id='6ccab2bb-2f2c-4861-aba2-6ed82d64a406', metadata={'source': 'langchain_sample.txt'}, page_content='BM25 es un mÃ©todo tradicional de recuperaciÃ³n dispersa que puntÃºa los documentos basÃ¡ndose en la coincidencia de palabras clave. Aunque es rÃ¡pido, a menudo presenta dificultades con sinÃ³nimos y similitud semÃ¡ntica.\\nLa recuperaciÃ³n densa utiliza incrustaciones para vincular la consulta y los documentos en un espacio vectorial. Esto permite capturar el significado semÃ¡ntico, lo que lo hace Ãºtil para consultas difusas o en lenguaje natural.')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paso 11: Reordenar los documentos según el ranking del LLM\n",
    "# Usamos los índices proporcionados por el LLM para reorganizar los documentos\n",
    "\n",
    "# List comprehension que:\n",
    "# 1. Toma cada índice de la lista de indices\n",
    "# 2. Verifica que el índice sea válido (0 <= i < len(retrieved_docs))\n",
    "# 3. Obtiene el documento correspondiente de retrieved_docs\n",
    "reranked_docs = [retrieved_docs[i] for i in indices if 0 <= i < len(retrieved_docs)]\n",
    "\n",
    "# Mostrar los documentos reordenados\n",
    "# Ahora están ordenados de más a menos relevante según el análisis del LLM\n",
    "reranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e076f87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Resultados Finales Reordenados:\n",
      "\n",
      "Rango 1:\n",
      "LangChain es un framework flexible diseÃ±ado para desarrollar aplicaciones basadas en grandes modelos de lenguaje (LLM). Proporciona herramientas y abstracciones para trabajar con LLM de forma mÃ¡s eficaz e incluye componentes para la gestiÃ³n de indicaciones, cadenas, memoria y agentes.\n",
      "\n",
      "Rango 2:\n",
      "Los agentes en LangChain son cadenas que utilizan LLM para decidir quÃ© herramientas usar y en quÃ© orden. Esto los hace adecuados para tareas de varios pasos, como la respuesta a preguntas con bÃºsqueda y ejecuciÃ³n de cÃ³digo.\n",
      "LangChain admite la integraciÃ³n de herramientas, como bÃºsqueda web, calculadoras y API, lo que permite que los LLM interactÃºen con sistemas externos y respondan con mayor precisiÃ³n a consultas dinÃ¡micas.\n",
      "\n",
      "Rango 3:\n",
      "LangChain se integra con numerosos servicios de terceros, como OpenAI, Hugging Face y Cohere. Esto permite a los desarrolladores experimentar con diferentes modelos y optimizar el rendimiento para casos de uso especÃ­ficos, como la sÃ­ntesis, la respuesta a preguntas o la traducciÃ³n.\n",
      "\n",
      "Rango 4:\n",
      "LangChain admite la recuperaciÃ³n hÃ­brida combinando BM25 y puntuaciones de similitud densas. Este enfoque mejora tanto la precisiÃ³n como la recuperaciÃ³n en la bÃºsqueda de documentos.\n",
      "FAISS es una biblioteca popular utilizada para la bÃºsqueda rÃ¡pida y aproximada del vecino mÃ¡s cercano en espacios de alta dimensiÃ³n. Admite Ã­ndices planos y comprimidos, lo que la hace escalable para grandes almacenes de documentos.\n",
      "\n",
      "Rango 5:\n",
      "La GeneraciÃ³n Aumentada por RecuperaciÃ³n (RAG) es una potente tÃ©cnica que recupera conocimiento externo y lo transmite a la indicaciÃ³n para fundamentar las respuestas de los LLM. LangChain facilita la implementaciÃ³n de RAG utilizando bases de datos vectoriales como FAISS, Chroma y Pinecone.\n",
      "\n",
      "Rango 6:\n",
      "BM25 es un mÃ©todo tradicional de recuperaciÃ³n dispersa que puntÃºa los documentos basÃ¡ndose en la coincidencia de palabras clave. Aunque es rÃ¡pido, a menudo presenta dificultades con sinÃ³nimos y similitud semÃ¡ntica.\n",
      "La recuperaciÃ³n densa utiliza incrustaciones para vincular la consulta y los documentos en un espacio vectorial. Esto permite capturar el significado semÃ¡ntico, lo que lo hace Ãºtil para consultas difusas o en lenguaje natural.\n",
      "\n",
      "Rango 7:\n",
      "LangChain admite la recuperaciÃ³n hÃ­brida combinando BM25 y puntuaciones de similitud densas. Este enfoque mejora tanto la precisiÃ³n como la recuperaciÃ³n en la bÃºsqueda de documentos.\n",
      "FAISS es una biblioteca popular utilizada para la bÃºsqueda rÃ¡pida y aproximada del vecino mÃ¡s cercano en espacios de alta dimensiÃ³n. Admite Ã­ndices planos y comprimidos, lo que la hace escalable para grandes almacenes de documentos.\n",
      "\n",
      "Rango 8:\n",
      "LangChain es un framework flexible diseÃ±ado para desarrollar aplicaciones basadas en grandes modelos de lenguaje (LLM). Proporciona herramientas y abstracciones para trabajar con LLM de forma mÃ¡s eficaz e incluye componentes para la gestiÃ³n de indicaciones, cadenas, memoria y agentes.\n",
      "\n",
      "Rango 9:\n",
      "Los agentes en LangChain son cadenas que utilizan LLM para decidir quÃ© herramientas usar y en quÃ© orden. Esto los hace adecuados para tareas de varios pasos, como la respuesta a preguntas con bÃºsqueda y ejecuciÃ³n de cÃ³digo.\n",
      "LangChain admite la integraciÃ³n de herramientas, como bÃºsqueda web, calculadoras y API, lo que permite que los LLM interactÃºen con sistemas externos y respondan con mayor precisiÃ³n a consultas dinÃ¡micas.\n",
      "\n",
      "Rango 10:\n",
      "LangChain se integra con numerosos servicios de terceros, como OpenAI, Hugging Face y Cohere. Esto permite a los desarrolladores experimentar con diferentes modelos y optimizar el rendimiento para casos de uso especÃ­ficos, como la sÃ­ntesis, la respuesta a preguntas o la traducciÃ³n.\n",
      "\n",
      "Rango 11:\n",
      "LangChain admite la recuperaciÃ³n hÃ­brida combinando BM25 y puntuaciones de similitud densas. Este enfoque mejora tanto la precisiÃ³n como la recuperaciÃ³n en la bÃºsqueda de documentos.\n",
      "FAISS es una biblioteca popular utilizada para la bÃºsqueda rÃ¡pida y aproximada del vecino mÃ¡s cercano en espacios de alta dimensiÃ³n. Admite Ã­ndices planos y comprimidos, lo que la hace escalable para grandes almacenes de documentos.\n",
      "\n",
      "Rango 12:\n",
      "La GeneraciÃ³n Aumentada por RecuperaciÃ³n (RAG) es una potente tÃ©cnica que recupera conocimiento externo y lo transmite a la indicaciÃ³n para fundamentar las respuestas de los LLM. LangChain facilita la implementaciÃ³n de RAG utilizando bases de datos vectoriales como FAISS, Chroma y Pinecone.\n",
      "\n",
      "Rango 13:\n",
      "BM25 es un mÃ©todo tradicional de recuperaciÃ³n dispersa que puntÃºa los documentos basÃ¡ndose en la coincidencia de palabras clave. Aunque es rÃ¡pido, a menudo presenta dificultades con sinÃ³nimos y similitud semÃ¡ntica.\n",
      "La recuperaciÃ³n densa utiliza incrustaciones para vincular la consulta y los documentos en un espacio vectorial. Esto permite capturar el significado semÃ¡ntico, lo que lo hace Ãºtil para consultas difusas o en lenguaje natural.\n"
     ]
    }
   ],
   "source": [
    "# Paso 12: Mostrar los resultados finales reordenados\n",
    "# Presentamos los documentos en su nuevo orden de relevancia\n",
    "\n",
    "print(\"\\n📊 Resultados Finales Reordenados:\")\n",
    "# enumerate(reranked_docs, 1) comienza la numeración en 1 para mejor legibilidad\n",
    "for i, doc in enumerate(reranked_docs, 1):\n",
    "    # Mostramos cada documento con su nueva posición en el ranking\n",
    "    print(f\"\\nRango {i}:\\n{doc.page_content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
