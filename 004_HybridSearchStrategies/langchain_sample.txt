LangChain es un framework flexible diseñado para desarrollar aplicaciones basadas en grandes modelos de lenguaje (LLM). Proporciona herramientas y abstracciones para trabajar con LLM de forma más eficaz e incluye componentes para la gestión de indicaciones, cadenas, memoria y agentes.
LangChain se integra con numerosos servicios de terceros, como OpenAI, Hugging Face y Cohere. Esto permite a los desarrolladores experimentar con diferentes modelos y optimizar el rendimiento para casos de uso específicos, como la síntesis, la respuesta a preguntas o la traducción.
La Generación Aumentada por Recuperación (RAG) es una potente técnica que recupera conocimiento externo y lo transmite a la indicación para fundamentar las respuestas de los LLM. LangChain facilita la implementación de RAG utilizando bases de datos vectoriales como FAISS, Chroma y Pinecone.
BM25 es un método tradicional de recuperación dispersa que puntúa los documentos basándose en la coincidencia de palabras clave. Aunque es rápido, a menudo presenta dificultades con sinónimos y similitud semántica.
La recuperación densa utiliza incrustaciones para vincular la consulta y los documentos en un espacio vectorial. Esto permite capturar el significado semántico, lo que lo hace útil para consultas difusas o en lenguaje natural.
LangChain admite la recuperación híbrida combinando BM25 y puntuaciones de similitud densas. Este enfoque mejora tanto la precisión como la recuperación en la búsqueda de documentos.
FAISS es una biblioteca popular utilizada para la búsqueda rápida y aproximada del vecino más cercano en espacios de alta dimensión. Admite índices planos y comprimidos, lo que la hace escalable para grandes almacenes de documentos.
Los agentes en LangChain son cadenas que utilizan LLM para decidir qué herramientas usar y en qué orden. Esto los hace adecuados para tareas de varios pasos, como la respuesta a preguntas con búsqueda y ejecución de código.
LangChain admite la integración de herramientas, como búsqueda web, calculadoras y API, lo que permite que los LLM interactúen con sistemas externos y respondan con mayor precisión a consultas dinámicas.
La memoria en LangChain permite la retención de contexto en varios pasos de una conversación o tarea, lo que hace que la aplicación sea más coherente y con estado.