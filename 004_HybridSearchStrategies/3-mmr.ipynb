{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebcf6a13",
   "metadata": {},
   "source": [
    "### Relevancia Marginal Máxima (MMR - Maximal Marginal Relevance)\n",
    "\n",
    "**MMR (Maximal Marginal Relevance)** es una técnica poderosa de recuperación consciente de la diversidad utilizada en recuperación de información y pipelines RAG para equilibrar relevancia y novedad al seleccionar documentos.\n",
    "\n",
    "**¿Qué problema resuelve MMR?**\n",
    "\n",
    "Los métodos tradicionales de búsqueda por similitud pueden devolver documentos muy similares entre sí (redundantes). MMR aborda este problema seleccionando documentos que son:\n",
    "1. **Relevantes** a la consulta del usuario\n",
    "2. **Diversos** entre sí (minimizan la redundancia)\n",
    "\n",
    "**¿Cómo funciona?**\n",
    "\n",
    "MMR utiliza una fórmula que balancea dos objetivos:\n",
    "- **Similitud con la consulta**: Qué tan relevante es el documento para la pregunta\n",
    "- **Disimilitud con documentos ya seleccionados**: Qué tan diferente es de los documentos ya recuperados\n",
    "\n",
    "**Ventajas:**\n",
    "- ✅ Evita información repetitiva\n",
    "- ✅ Proporciona una perspectiva más amplia del tema\n",
    "- ✅ Mejora la calidad de las respuestas al incluir información complementaria\n",
    "- ✅ Especialmente útil cuando hay múltiples aspectos de una pregunta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87294dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Udemy\\RAGBootcamp\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importación de librerías necesarias para implementar MMR\n",
    "\n",
    "# FAISS: Librería de Facebook para búsqueda de similitud vectorial eficiente\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# HuggingFaceEmbeddings: Para generar embeddings usando modelos de HuggingFace\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# TextLoader: Para cargar archivos de texto plano\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# RecursiveCharacterTextSplitter: Para dividir texto en fragmentos manejables\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# init_chat_model: Para inicializar modelos de chat de diferentes proveedores\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# PromptTemplate: Para crear plantillas de prompts con variables dinámicas\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# create_stuff_documents_chain: Crea una cadena que inserta documentos en un prompt\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# create_retrieval_chain: Combina recuperación de documentos con generación de respuestas\n",
    "from langchain.chains.retrieval import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0340285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de variables de entorno para las API keys\n",
    "\n",
    "# Importar módulos para manejo de variables de entorno\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Cargar las variables de entorno desde el archivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Establecer la API key de OpenAI desde las variables de entorno\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Establecer la API key de Groq desde las variables de entorno\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54cb5be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'langchain_rag_dataset.txt'}, page_content='LangChain es un framework de cÃ³digo abierto diseÃ±ado para simplificar el desarrollo de aplicaciones que utilizan grandes modelos de lenguaje (LLM).'),\n",
       " Document(metadata={'source': 'langchain_rag_dataset.txt'}, page_content='LangChain proporciona abstracciones para trabajar con indicaciones, cadenas, memoria y agentes, lo que facilita la creaciÃ³n de sistemas complejos basados â€‹â€‹en LLM.\\nEl framework admite la integraciÃ³n con diversas bases de datos vectoriales como FAISS y Chroma para la recuperaciÃ³n semÃ¡ntica.'),\n",
       " Document(metadata={'source': 'langchain_rag_dataset.txt'}, page_content='LangChain habilita la GeneraciÃ³n Aumentada por RecuperaciÃ³n (RAG), permitiendo a los desarrolladores obtener el contexto relevante antes de generar respuestas.'),\n",
       " Document(metadata={'source': 'langchain_rag_dataset.txt'}, page_content='La memoria en LangChain ayuda a los modelos a retener interacciones previas, lo que aumenta la coherencia de las conversaciones multi-turno.\\nLos agentes en LangChain pueden usar herramientas como calculadoras, API de bÃºsqueda o funciones personalizadas segÃºn las instrucciones que reciben.'),\n",
       " Document(metadata={'source': 'langchain_rag_dataset.txt'}, page_content='BM25 y la recuperaciÃ³n basada en vectores se pueden combinar en LangChain para admitir estrategias de recuperaciÃ³n hÃ\\xadbridas.'),\n",
       " Document(metadata={'source': 'langchain_rag_dataset.txt'}, page_content='FAISS es una biblioteca de alto rendimiento para la bÃºsqueda por similitud que LangChain aprovecha para una recuperaciÃ³n eficiente en pipelines RAG. Chroma es un almacÃ©n vectorial ligero que se utiliza a menudo en LangChain para el almacenamiento y la recuperaciÃ³n de documentos mediante'),\n",
       " Document(metadata={'source': 'langchain_rag_dataset.txt'}, page_content='y la recuperaciÃ³n de documentos mediante incrustaciÃ³n.'),\n",
       " Document(metadata={'source': 'langchain_rag_dataset.txt'}, page_content='Las plantillas de indicaciones de LangChain admiten el formato estilo Jinja y la inyecciÃ³n de variables para personalizar las entradas del modelo.\\nLa cadena \"stuff\" envÃ\\xada todo el contexto a la vez al LLM, lo que resulta Ãºtil para documentos cortos en RAG.'),\n",
       " Document(metadata={'source': 'langchain_rag_dataset.txt'}, page_content='La cadena \"map-reduce\" fragmenta documentos grandes, los procesa por separado y luego agrega los resultados.\\nLa cadena \"refine\" actualiza iterativamente una respuesta incorporando cada nuevo fragmento de informaciÃ³n.'),\n",
       " Document(metadata={'source': 'langchain_rag_dataset.txt'}, page_content='LangChain permite que los LLM actÃºen como agentes que deciden a quÃ© herramienta llamar y en quÃ© orden durante una tarea.\\nLangChain admite memoria conversacional mediante ConversationBufferMemory y memoria de resumen mediante ConversationSummaryMemory.'),\n",
       " Document(metadata={'source': 'langchain_rag_dataset.txt'}, page_content='Los agentes de LangChain pueden interactuar con API y bases de datos externas, lo que mejora las capacidades de las aplicaciones basadas en LLM. Las canalizaciones RAG en LangChain implican la carga, divisiÃ³n, incrustaciÃ³n, recuperaciÃ³n y generaciÃ³n de respuestas basadas en LLM (relevancia'),\n",
       " Document(metadata={'source': 'langchain_rag_dataset.txt'}, page_content='de respuestas basadas en LLM (relevancia marginal mÃ¡xima) de documentos.'),\n",
       " Document(metadata={'source': 'langchain_rag_dataset.txt'}, page_content='La recuperaciÃ³n de MMR (relevancia marginal mÃ¡xima) en LangChain mejora la diversidad al equilibrar la relevancia y la redundancia.\\nEl uso de herramientas en LangChain permite a los agentes ejecutar funciones predefinidas de Python con la informaciÃ³n contextual del usuario.'),\n",
       " Document(metadata={'source': 'langchain_rag_dataset.txt'}, page_content='LangChain permite la reclasificaciÃ³n de los resultados recuperados mediante LLM o codificadores cruzados neuronales para mejorar la calidad del contexto.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paso 1: Cargar y fragmentar el documento\n",
    "\n",
    "# Cargar el archivo de texto que contiene información sobre LangChain y RAG\n",
    "loader = TextLoader(\"langchain_rag_dataset.txt\")\n",
    "# load() devuelve una lista de documentos con todo el contenido del archivo\n",
    "raw_docs = loader.load()\n",
    "\n",
    "# Crear un splitter para dividir el texto en fragmentos más pequeños\n",
    "# chunk_size=300: Cada fragmento tendrá máximo 300 caracteres\n",
    "# chunk_overlap=50: Habrá una superposición de 50 caracteres entre fragmentos consecutivos\n",
    "# La superposición ayuda a mantener el contexto entre fragmentos\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "\n",
    "# Dividir los documentos crudos en fragmentos (chunks) manejables\n",
    "chunks = splitter.split_documents(raw_docs)\n",
    "\n",
    "# Mostrar los fragmentos generados\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64b95ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 2: Crear el almacén vectorial FAISS con embeddings de HuggingFace\n",
    "\n",
    "# Inicializar el modelo de embeddings de HuggingFace\n",
    "# all-MiniLM-L6-v2 es un modelo compacto y eficiente que genera vectores de 384 dimensiones\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Crear el almacén vectorial FAISS a partir de los fragmentos de documentos\n",
    "# FAISS convierte cada fragmento en un vector numérico para búsqueda de similitud eficiente\n",
    "# Este almacén soportará búsqueda MMR para diversidad en los resultados\n",
    "vectorstore = FAISS.from_documents(chunks, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "836eb965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3: Crear el recuperador con MMR (Maximal Marginal Relevance)\n",
    "\n",
    "# Convertir el almacén vectorial en un recuperador configurado para usar MMR\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",  # Especifica que usaremos MMR en lugar de similitud simple\n",
    "                        # MMR balancea relevancia y diversidad en los resultados\n",
    "    search_kwargs={\"k\": 3}  # k=3: Recuperar los 3 documentos más relevantes y diversos\n",
    "                            # MMR seleccionará documentos que sean relevantes pero también\n",
    "                            # diferentes entre sí para evitar redundancia\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ded4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda vacía - puede ser usada para pruebas o código adicional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7a54fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 4: Configurar el prompt y el modelo de lenguaje (LLM)\n",
    "\n",
    "# Crear la plantilla de prompt que define cómo el LLM usará el contexto recuperado\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Responde la pregunta basándote en el contexto proporcionado.\n",
    "\n",
    "Contexto:\n",
    "{context}\n",
    "\n",
    "Pregunta: {input}\n",
    "\"\"\")\n",
    "\n",
    "# Inicializar el LLM usando Groq con el modelo Gemma2-9B\n",
    "# Groq ofrece inferencia rápida y Gemma2 es eficiente para generación de texto\n",
    "llm = init_chat_model(\"groq:llama-3.1-8b-instant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cff3768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 5: Construir el pipeline RAG completo\n",
    "\n",
    "# Crear la cadena de documentos que combina el LLM con el prompt\n",
    "# Esta cadena toma documentos recuperados y los inserta en el prompt\n",
    "document_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Crear la cadena RAG completa que integra:\n",
    "# 1. El recuperador MMR (para obtener documentos relevantes y diversos)\n",
    "# 2. La cadena de documentos (para generar la respuesta con el LLM)\n",
    "rag_chain = create_retrieval_chain(retriever=retriever, combine_docs_chain=document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28ecccc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Respuesta:\n",
      " Según el contexto proporcionado, LangChain soporta a los agentes y la memoria de las siguientes maneras:\n",
      "\n",
      "- La memoria en LangChain ayuda a los modelos a retener interacciones previas, lo que aumenta la coherencia de las conversaciones multi-turno.\n",
      "- Los agentes en LangChain pueden usar herramientas como calculadoras, API de búsqueda o funciones personalizadas según las instrucciones que reciben.\n",
      "- LangChain permite la reclasificación de los resultados recuperados mediante LLM o codificadores cruzados neuronales para mejorar la calidad del contexto.\n"
     ]
    }
   ],
   "source": [
    "# Paso 6: Realizar una consulta al sistema RAG con MMR\n",
    "\n",
    "# Definir la pregunta que queremos responder\n",
    "# Esta pregunta toca dos temas diferentes: agentes y memoria\n",
    "# MMR ayudará a recuperar documentos diversos que cubran ambos aspectos\n",
    "query = {\"input\": \"¿Cómo soporta LangChain los agentes y la memoria?\"}\n",
    "\n",
    "# Invocar la cadena RAG completa con nuestra consulta\n",
    "# El flujo será: consulta → recuperación MMR → inserción en prompt → generación LLM\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "# Mostrar solo la respuesta generada por el LLM\n",
    "print(\"✅ Respuesta:\\n\", response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75c6aefd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': '¿Cómo soporta LangChain los agentes y la memoria?',\n",
       " 'context': [Document(id='c5deca15-ebf4-4933-92b2-a4584cb1e0d3', metadata={'source': 'langchain_rag_dataset.txt'}, page_content='La memoria en LangChain ayuda a los modelos a retener interacciones previas, lo que aumenta la coherencia de las conversaciones multi-turno.\\nLos agentes en LangChain pueden usar herramientas como calculadoras, API de bÃºsqueda o funciones personalizadas segÃºn las instrucciones que reciben.'),\n",
       "  Document(id='38c71fd4-e659-4086-a7a8-cbc01ff03262', metadata={'source': 'langchain_rag_dataset.txt'}, page_content='de respuestas basadas en LLM (relevancia marginal mÃ¡xima) de documentos.'),\n",
       "  Document(id='ad40c814-be7a-4065-8874-dc10b6ef2575', metadata={'source': 'langchain_rag_dataset.txt'}, page_content='LangChain permite la reclasificaciÃ³n de los resultados recuperados mediante LLM o codificadores cruzados neuronales para mejorar la calidad del contexto.')],\n",
       " 'answer': 'Según el contexto proporcionado, LangChain soporta a los agentes y la memoria de las siguientes maneras:\\n\\n- La memoria en LangChain ayuda a los modelos a retener interacciones previas, lo que aumenta la coherencia de las conversaciones multi-turno.\\n- Los agentes en LangChain pueden usar herramientas como calculadoras, API de búsqueda o funciones personalizadas según las instrucciones que reciben.\\n- LangChain permite la reclasificación de los resultados recuperados mediante LLM o codificadores cruzados neuronales para mejorar la calidad del contexto.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostrar la respuesta completa incluyendo contexto y metadatos\n",
    "# Esta celda muestra:\n",
    "# - 'input': La pregunta original\n",
    "# - 'context': Los documentos recuperados por MMR (3 documentos diversos)\n",
    "# - 'answer': La respuesta generada por el LLM basada en el contexto\n",
    "\n",
    "# Nota: Observa que MMR seleccionó 3 documentos diferentes que cubren aspectos\n",
    "# complementarios: memoria en conversaciones, agentes con APIs, y tipos de memoria\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174e3a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda vacía - puede ser usada para experimentos adicionales\n",
    "# Por ejemplo, podrías:\n",
    "# - Probar diferentes valores de k en el recuperador MMR\n",
    "# - Comparar resultados de MMR vs búsqueda por similitud simple\n",
    "# - Ajustar el tamaño de los chunks para ver cómo afecta los resultados"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
