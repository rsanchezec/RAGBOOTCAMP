{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a129598",
   "metadata": {},
   "source": [
    "## Recuperador H√≠brido - Combinando Recuperadores Densos y Dispersos\n",
    "\n",
    "Un recuperador h√≠brido combina dos enfoques complementarios:\n",
    "- **Recuperaci√≥n Densa**: Utiliza embeddings vectoriales para b√∫squeda sem√°ntica (captura el significado)\n",
    "- **Recuperaci√≥n Dispersa**: Utiliza t√©cnicas de coincidencia de palabras clave como BM25 (captura t√©rminos exactos)\n",
    "\n",
    "Esta combinaci√≥n mejora la calidad de recuperaci√≥n al aprovechar las fortalezas de ambos m√©todos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f518da96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Udemy\\RAGBootcamp\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importaci√≥n de librer√≠as necesarias para la b√∫squeda h√≠brida\n",
    "\n",
    "# FAISS: Librer√≠a de Facebook para b√∫squeda de similitud vectorial eficiente\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# HuggingFaceEmbeddings: Para generar embeddings usando modelos de HuggingFace\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# BM25Retriever: Implementaci√≥n del algoritmo BM25 para recuperaci√≥n dispersa basada en palabras clave\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "# EnsembleRetriever: Combina m√∫ltiples recuperadores con pesos espec√≠ficos\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "# Document: Clase para representar documentos con contenido y metadatos\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c86da13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Crear documentos de ejemplo\n",
    "# Creamos una lista de documentos con diferentes tem√°ticas para probar el sistema de recuperaci√≥n\n",
    "docs = [\n",
    "    # Documento sobre LangChain y su prop√≥sito\n",
    "    Document(page_content=\"LangChain ayuda a construir aplicaciones LLM.\"),\n",
    "    # Documento sobre bases de datos vectoriales\n",
    "    Document(page_content=\"Pinecone es una base de datos vectorial para b√∫squeda sem√°ntica.\"),\n",
    "    # Documento con informaci√≥n geogr√°fica\n",
    "    Document(page_content=\"La Torre Eiffel est√° ubicada en Par√≠s.\"),\n",
    "    # Documento sobre aplicaciones de IA ag√©ntica con Langchain\n",
    "    Document(page_content=\"Langchain puede ser usado para desarrollar aplicaciones de IA ag√©ntica.\"),\n",
    "    # Documento sobre las capacidades de Langchain\n",
    "    Document(page_content=\"Langchain tiene muchos tipos de recuperadores.\")\n",
    "]\n",
    "\n",
    "# Paso 2: Configurar el Recuperador Denso (FAISS + HuggingFace)\n",
    "# Inicializamos el modelo de embeddings de HuggingFace\n",
    "# all-MiniLM-L6-v2 es un modelo ligero y eficiente que genera vectores de 384 dimensiones\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Creamos una base de datos vectorial FAISS a partir de los documentos\n",
    "# FAISS convierte cada documento en un vector usando el modelo de embeddings\n",
    "dense_vectorstore = FAISS.from_documents(docs, embedding_model)\n",
    "\n",
    "# Convertimos el almac√©n vectorial en un recuperador\n",
    "# Este recuperador realizar√° b√∫squedas por similitud sem√°ntica\n",
    "dense_retriever = dense_vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76569a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3: Configurar el Recuperador Disperso (BM25)\n",
    "# BM25 es un algoritmo de ranking que usa frecuencia de t√©rminos y longitud de documentos\n",
    "# Es excelente para coincidencias exactas de palabras clave\n",
    "sparse_retriever = BM25Retriever.from_documents(docs)\n",
    "\n",
    "# Configuramos el n√∫mero de documentos m√°s relevantes a recuperar (top-k)\n",
    "# k=3 significa que devolver√° los 3 documentos m√°s relevantes\n",
    "sparse_retriever.k = 3\n",
    "\n",
    "# Paso 4: Combinar recuperadores con EnsembleRetriever\n",
    "# El recuperador h√≠brido combina los resultados de ambos recuperadores\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    # Lista de recuperadores a combinar: denso (sem√°ntico) y disperso (palabras clave)\n",
    "    retrievers=[dense_retriever, sparse_retriever],\n",
    "    # Pesos para cada recuperador: 70% denso, 30% disperso\n",
    "    # Los pesos determinan la importancia relativa de cada m√©todo en los resultados finales\n",
    "    weight=[0.7, 0.3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57d59933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnsembleRetriever(retrievers=[VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002E773A33980>, search_kwargs={}), BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x000002E770DCA660>, k=3)], weights=[0.5, 0.5])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostrar la configuraci√≥n del recuperador h√≠brido\n",
    "# Esto nos permite verificar c√≥mo est√° configurado el sistema\n",
    "hybrid_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dec3b869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Documento 1:\n",
      "LangChain ayuda a construir aplicaciones LLM.\n",
      "\n",
      "üîπ Documento 2:\n",
      "Pinecone es una base de datos vectorial para b√∫squeda sem√°ntica.\n",
      "\n",
      "üîπ Documento 3:\n",
      "Langchain tiene muchos tipos de recuperadores.\n",
      "\n",
      "üîπ Documento 4:\n",
      "Langchain puede ser usado para desarrollar aplicaciones de IA ag√©ntica.\n"
     ]
    }
   ],
   "source": [
    "# Paso 5: Realizar una consulta y obtener resultados\n",
    "# Definimos una pregunta que queremos responder usando nuestro sistema de recuperaci√≥n\n",
    "query = \"¬øC√≥mo puedo construir una aplicaci√≥n usando LLMs?\"\n",
    "\n",
    "# Invocamos el recuperador h√≠brido con nuestra consulta\n",
    "# El sistema combinar√° los resultados de b√∫squeda sem√°ntica (FAISS) y de palabras clave (BM25)\n",
    "results = hybrid_retriever.invoke(query)\n",
    "\n",
    "# Paso 6: Mostrar los resultados recuperados\n",
    "# Iteramos sobre cada documento recuperado y lo mostramos\n",
    "for i, doc in enumerate(results):\n",
    "    # Mostramos el n√∫mero del documento y su contenido\n",
    "    print(f\"\\nüîπ Documento {i+1}:\\n{doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11c57cb",
   "metadata": {},
   "source": [
    "### Pipeline RAG con Recuperador H√≠brido\n",
    "\n",
    "Ahora construiremos un pipeline RAG (Retrieval-Augmented Generation) completo que:\n",
    "1. Recupera documentos relevantes usando el recuperador h√≠brido\n",
    "2. Utiliza esos documentos como contexto para un LLM\n",
    "3. Genera una respuesta fundamentada en la informaci√≥n recuperada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bf22afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaci√≥n de componentes para construir el pipeline RAG\n",
    "\n",
    "# init_chat_model: Funci√≥n para inicializar modelos de chat de diferentes proveedores\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# PromptTemplate: Para crear plantillas de prompts con variables din√°micas\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# create_stuff_documents_chain: Crea una cadena que \"rellena\" documentos en un prompt\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# create_retrieval_chain: Combina recuperaci√≥n de documentos con generaci√≥n de respuestas\n",
    "from langchain.chains.retrieval import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99e17a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000002E7759D3CB0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002E775F7F230>, root_client=<openai.OpenAI object at 0x000002E773C7FE60>, root_async_client=<openai.AsyncOpenAI object at 0x000002E775ACA570>, temperature=0.2, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paso 5: Crear la plantilla de prompt\n",
    "# Definimos c√≥mo queremos que el LLM use el contexto recuperado para responder\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Responde la pregunta bas√°ndote en el contexto a continuaci√≥n.\n",
    "\n",
    "Contexto:\n",
    "{context}\n",
    "\n",
    "Pregunta: {input}\n",
    "\"\"\")\n",
    "\n",
    "# Paso 6: Inicializar el modelo de lenguaje (LLM)\n",
    "# Usamos GPT-3.5-turbo de OpenAI con temperatura 0.2\n",
    "# temperature=0.2 significa respuestas m√°s determin√≠sticas y menos creativas\n",
    "llm = init_chat_model(\"openai:gpt-3.5-turbo\", temperature=0.2)\n",
    "\n",
    "# Mostrar la configuraci√≥n del LLM\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9eb55e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | EnsembleRetriever(retrievers=[VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002E773A33980>, search_kwargs={}), BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x000002E770DCA660>, k=3)], weights=[0.5, 0.5]), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\nResponde la pregunta bas√°ndote en el contexto a continuaci√≥n.\\n\\nContexto:\\n{context}\\n\\nPregunta: {input}\\n')\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000002E7759D3CB0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002E775F7F230>, root_client=<openai.OpenAI object at 0x000002E773C7FE60>, root_async_client=<openai.AsyncOpenAI object at 0x000002E775ACA570>, temperature=0.2, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paso 7: Crear la cadena de documentos\n",
    "# Esta cadena toma los documentos recuperados y los combina con el prompt\n",
    "# \"stuff\" significa que todos los documentos se insertan directamente en el prompt\n",
    "document_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Paso 8: Crear la cadena RAG completa\n",
    "# Esta cadena combina:\n",
    "# 1. El recuperador h√≠brido (para obtener documentos relevantes)\n",
    "# 2. La cadena de documentos (para generar la respuesta con el LLM)\n",
    "rag_chain = create_retrieval_chain(retriever=hybrid_retriever, combine_docs_chain=document_chain)\n",
    "\n",
    "# Mostrar la estructura de la cadena RAG\n",
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bb2441c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Respuesta:\n",
      " Para construir una aplicaci√≥n utilizando LLMs, puedes utilizar LangChain, ya que esta plataforma ayuda a construir aplicaciones LLM. Adem√°s, puedes aprovechar Pinecone como una base de datos vectorial para b√∫squeda sem√°ntica, y utilizar los diferentes tipos de recuperadores que ofrece LangChain. De esta manera, podr√°s desarrollar aplicaciones de IA ag√©ntica utilizando LLMs.\n",
      "\n",
      "üìÑ Documentos Fuente:\n",
      "\n",
      "Doc 1: LangChain ayuda a construir aplicaciones LLM.\n",
      "\n",
      "Doc 2: Pinecone es una base de datos vectorial para b√∫squeda sem√°ntica.\n",
      "\n",
      "Doc 3: Langchain tiene muchos tipos de recuperadores.\n",
      "\n",
      "Doc 4: Langchain puede ser usado para desarrollar aplicaciones de IA ag√©ntica.\n"
     ]
    }
   ],
   "source": [
    "# Paso 9: Realizar una pregunta al sistema RAG\n",
    "# Creamos un diccionario con nuestra consulta\n",
    "query = {\"input\": \"¬øC√≥mo puedo construir una aplicaci√≥n usando LLMs?\"}\n",
    "\n",
    "# Invocamos la cadena RAG completa\n",
    "# Esto ejecutar√°: recuperaci√≥n de documentos ‚Üí inserci√≥n en prompt ‚Üí generaci√≥n de respuesta\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "# Paso 10: Mostrar los resultados\n",
    "# Imprimimos la respuesta generada por el LLM\n",
    "print(\"‚úÖ Respuesta:\\n\", response[\"answer\"])\n",
    "\n",
    "# Mostramos los documentos fuente que se usaron para generar la respuesta\n",
    "print(\"\\nüìÑ Documentos Fuente:\")\n",
    "for i, doc in enumerate(response[\"context\"]):\n",
    "    # Para cada documento, mostramos su n√∫mero y contenido\n",
    "    print(f\"\\nDoc {i+1}: {doc.page_content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
